{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "#### Summary:\n",
    "- 3 baselines, copy the value from last period, copy the value from the same period the year before, take the max between the last 2 periods\n",
    "- Models on geo features alone can at best match the performance of the baseline\n",
    "- Need to find a way to bring some improvement by adding conflict features!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "# GKG data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper_functions.gdelt_data_mapping_optimized import load_gadm_data\n",
    "from helper_functions.download_gdelt_gkg import (\n",
    "    download_gkg_range,\n",
    "    consolidate_gkg,\n",
    "    process_gkg_data,\n",
    "    consolidate_and_merge_fews_gkg,\n",
    "    aggregate_files_by_day,\n",
    ")\n",
    "import multiprocessing\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', 50)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Define paths\n",
    "# ---------------------------------------------------------------------\n",
    "gadm_path = \"../data/gadm/gadm_410_filtered_v2.gpkg\"\n",
    "fews_path = \"../data/fews/fews_with_conflicts_admin2_v2.parquet\"\n",
    "gkg_raw_dir = \"../data/gdelt/gkg/1_raw\"\n",
    "gkg_daily_dir = \"../data/gdelt/gkg/1_raw_daily\"\n",
    "gkg_consolidated_dir = \"../data/gdelt/gkg/2_consolidated\"\n",
    "gkg_mapped_dir = \"../data/gdelt/gkg/3_mapped\"\n",
    "gkg_output_path = \"../data/gdelt/gkg/4_aggregated/gkg_final.parquet\"\n",
    "\n",
    "os.makedirs(gkg_daily_dir, exist_ok=True)\n",
    "os.makedirs(gkg_consolidated_dir, exist_ok=True)\n",
    "os.makedirs(gkg_mapped_dir, exist_ok=True)\n",
    "os.makedirs(os.path.dirname(gkg_output_path), exist_ok=True)\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Step 1: Download & aggregate GKG\n",
    "# ---------------------------------------------------------------------\n",
    "print(\"Downloading GKG data...\")\n",
    "#download_gkg_range(\n",
    "#     start=\"2024-03-01\",\n",
    "#     end=\"2024-07-30\",\n",
    "#     raw_dir=gkg_raw_dir,\n",
    "#     max_workers=8\n",
    "# )\n",
    "\n",
    "#aggregate_files_by_day(\n",
    "#     input_folder=gkg_raw_dir,\n",
    "#     output_folder=gkg_daily_dir,\n",
    "#     start_date=\"20160101\",\n",
    "#     end_date=\"20240731\"\n",
    "# )\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Step 2: Consolidate daily ‚Üí monthly parquet\n",
    "# ---------------------------------------------------------------------\n",
    "#print(\"Consolidating GKG data...\")\n",
    "#consolidate_gkg(\n",
    "#     raw_dir=gkg_daily_dir,\n",
    "#     out_file=os.path.join(gkg_consolidated_dir, \"files.parquet\")\n",
    "# )\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Step 3: Load FEWS NET & GADM data\n",
    "# ---------------------------------------------------------------------\n",
    "fews_df = pd.read_parquet(fews_path)\n",
    "fews_df = fews_df[['ADMIN0', 'ADMIN1', 'ADMIN2', 'period', 'CS_score']]\n",
    "\n",
    "print(\"Loading GADM data...\")\n",
    "gadm_gdf, fews_df = load_gadm_data(gadm_path, fews_df)\n",
    "\n",
    "# # ---------------------------------------------------------------------\n",
    "# # Step 4: Combine consolidated monthly files\n",
    "# # ---------------------------------------------------------------------\n",
    "month_files = glob.glob(os.path.join(gkg_consolidated_dir, \"*.parquet\"))\n",
    "if not month_files:\n",
    "    raise FileNotFoundError(f\"No consolidated parquet files found in {gkg_consolidated_dir}\")\n",
    "\n",
    "gkg_df = pd.concat((pd.read_parquet(fp) for fp in month_files), ignore_index=True)\n",
    "print(f\"Loaded {len(gkg_df):,} GKG records\")\n",
    "\n",
    "# # ---------------------------------------------------------------------\n",
    "# # Step 5: Map to administrative regions\n",
    "# # ---------------------------------------------------------------------\n",
    "print(\"Mapping administrative regions...\")\n",
    "process_gkg_data(gkg_df, output_dir=gkg_mapped_dir)\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Step 6: Merge with FEWS data\n",
    "# ---------------------------------------------------------------------\n",
    "print(\"Consolidating mapped GKG data with FEWS data...\")\n",
    "df_final = consolidate_and_merge_fews_gkg(mapped_gkg_dir=gkg_mapped_dir, fews_df=fews_df, period=None)\n",
    "print(\"Unique URLs:\", len({str(u).strip() for x in df_final[\"DocumentIdentifier\"].dropna() for u in np.atleast_1d(x) if pd.notna(u) and str(u).strip().lower() not in (\"none\",\"\")}))\n",
    "print(f\"Final dataset rows: {len(df_final):,}\")\n",
    "\n",
    "for col in df_final.columns:\n",
    "    if col.startswith(\"n_\") or col in [\"usd_aid\"]:\n",
    "        df_final[col] = pd.to_numeric(df_final[col], errors=\"coerce\").fillna(0).astype(int)\n",
    "\n",
    "df_final.to_parquet(gkg_output_path, index=False)\n",
    "print(f\"Saved final GKG dataset to {gkg_output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "consolidated = pd.read_parquet(\"../data/gdelt/gkg/2_consolidated/gkg_2022_07.parquet\")\n",
    "consolidated = consolidated[consolidated['V2Counts'] != 'None']\n",
    "consolidated.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import re\n",
    "import os\n",
    "from helper_functions.url_scraping_base import process_urls_in_chunks\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Load GKG data\n",
    "# ---------------------------------------------------------------------\n",
    "gkg_path = \"../data/gdelt/gkg/4_aggregated/gkg_final.parquet\"\n",
    "if not os.path.exists(gkg_path):\n",
    "    raise FileNotFoundError(f\"Missing file: {gkg_path}\")\n",
    "\n",
    "df = pd.read_parquet(gkg_path)\n",
    "print(f\"Loaded {len(df):,} GKG records from {gkg_path}\")\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Extract unique URLs from DocumentIdentifier\n",
    "# ---------------------------------------------------------------------\n",
    "unique_urls = sorted({\n",
    "    str(u).strip()\n",
    "    for arr in df[\"DocumentIdentifier\"].dropna()\n",
    "    for u in (arr if isinstance(arr, (list, tuple, np.ndarray)) else [arr])\n",
    "    if isinstance(u, str) and u.startswith(\"http\") and str(u).strip().lower() not in (\"none\", \"\")\n",
    "})\n",
    "print(f\"Total unique URLs found: {len(unique_urls):,}\")\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Identify already processed URLs\n",
    "# ---------------------------------------------------------------------\n",
    "chunk_dir = \"../data/gdelt/gkg/scraped_urls\"\n",
    "os.makedirs(chunk_dir, exist_ok=True)\n",
    "\n",
    "parquet_files = glob.glob(os.path.join(chunk_dir, \"chunk_*.parquet\"))\n",
    "\n",
    "chunk_numbers = (\n",
    "    [int(re.search(r'chunk_(\\d+)\\.parquet', f).group(1)) for f in parquet_files]\n",
    "    if parquet_files else []\n",
    ")\n",
    "highest_chunk = max(chunk_numbers) + 1 if chunk_numbers else 0\n",
    "print(f\"Highest existing chunk: {highest_chunk - 1 if highest_chunk > 0 else 'None'}\")\n",
    "\n",
    "if parquet_files:\n",
    "    processed = pd.concat([pd.read_parquet(f) for f in parquet_files], ignore_index=True)\n",
    "    processed_urls = set(processed['url'].dropna().values)\n",
    "else:\n",
    "    processed_urls = set()\n",
    "\n",
    "unique_urls = [u for u in unique_urls if u not in processed_urls]\n",
    "print(f\"Remaining URLs to process: {len(unique_urls):,}\")\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Run async URL scraping\n",
    "# ---------------------------------------------------------------------\n",
    "import nest_asyncio, asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "if unique_urls:\n",
    "    process_urls_in_chunks(\n",
    "        urls=unique_urls,\n",
    "        chunk_size=10000,\n",
    "        concurrency=150,\n",
    "        chunk_id=highest_chunk,\n",
    "        timeout=4,\n",
    "        max_retries=3,\n",
    "        max_selenium_workers=4,\n",
    "        fallback_mode=\"async_only\",  # no Selenium fallback\n",
    "        output_dir=chunk_dir\n",
    "    )\n",
    "else:\n",
    "    print(\"‚úÖ All URLs already processed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "### Filter and clean urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import math\n",
    "import logging\n",
    "import multiprocessing as mp\n",
    "from collections import Counter\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "import glob\n",
    "import spacy\n",
    "\n",
    "from helper_functions.topic_modelling.text_processing_parallel import preprocess_text_parallel\n",
    "from helper_functions.topic_modelling.flatten_articles import filter_articles_by_lexicon\n",
    "from helper_functions.topic_modelling.deduplication import deduplicate_minhash\n",
    "from helper_functions.run_ner_parallel import run_ner_parallel, inject_countries_from_demonyms\n",
    "\n",
    "# ============================================================\n",
    "# 0Ô∏è‚É£ LOAD GKG SCRAPED ARTICLES\n",
    "# ============================================================\n",
    "\n",
    "parquet_dir = \"../data/gdelt/gkg/scraped_urls\"\n",
    "parquet_files = glob.glob(os.path.join(parquet_dir, \"chunk_*.parquet\"))\n",
    "if not parquet_files:\n",
    "    raise FileNotFoundError(f\"No parquet files found in {parquet_dir}\")\n",
    "\n",
    "df_scraped = pd.concat([pd.read_parquet(f) for f in parquet_files], ignore_index=True)\n",
    "print(f\"Loaded {len(df_scraped):,} scraped GKG articles\")\n",
    "\n",
    "# ============================================================\n",
    "# 1Ô∏è‚É£ PREPARE TEXT\n",
    "# ============================================================\n",
    "\n",
    "df_scraped[\"header\"] = df_scraped[\"header\"].fillna(\"\").astype(str)\n",
    "df_scraped[\"body\"] = df_scraped[\"body\"].fillna(\"\").astype(str)\n",
    "df_scraped[\"text\"] = df_scraped[\"header\"] + \" \" + df_scraped[\"body\"]\n",
    "\n",
    "# ============================================================\n",
    "# 2Ô∏è‚É£ LEXICON FILTER\n",
    "# ============================================================\n",
    "\n",
    "print(\"Applying LEAP4FNSSA lexicon filter...\")\n",
    "df_scraped = filter_articles_by_lexicon(\n",
    "    df_scraped,\n",
    "    clean_text_col=\"text\",\n",
    "    lexicon_path=\"../data/LEAP4FNSSA_LEXICON_long.csv\"\n",
    ")\n",
    "print(\"Remaining after lexicon filter:\", df_scraped.shape)\n",
    "\n",
    "# ============================================================\n",
    "# 3Ô∏è‚É£ HEAVY NLP PREPROCESSING\n",
    "# ============================================================\n",
    "\n",
    "df_scraped = preprocess_text_parallel(df_scraped, text_col=\"text\")\n",
    "print(\"Text preprocessing done.\")\n",
    "\n",
    "# ============================================================\n",
    "# 4Ô∏è‚É£ TRUNCATE CLEAN TEXT TO 500 WORDS\n",
    "# ============================================================\n",
    "\n",
    "df_scraped[\"clean_text\"] = df_scraped[\"clean_text\"].apply(\n",
    "    lambda t: \" \".join(str(t).split()[:500])\n",
    ")\n",
    "\n",
    "# ============================================================\n",
    "# 5Ô∏è‚É£ DEDUPLICATION\n",
    "# ============================================================\n",
    "\n",
    "print(\"Running MinHash deduplication...\")\n",
    "df_scraped = deduplicate_minhash(df_scraped, text_col=\"clean_text\", threshold=0.90)\n",
    "print(\"Remaining after dedup:\", df_scraped.shape)\n",
    "\n",
    "# ============================================================\n",
    "# 6Ô∏è‚É£ NER EXTRACTION + DEMONYM INJECTION\n",
    "# ============================================================\n",
    "\n",
    "print(\"üîç Running NER...\")\n",
    "\n",
    "df_scraped[\"clean_text\"] = df_scraped[\"clean_text\"].apply(inject_countries_from_demonyms)\n",
    "\n",
    "n_process = 1 if \"ipykernel\" in mp.current_process().name.lower() else mp.cpu_count()\n",
    "df_scraped = run_ner_parallel(df_scraped, text_col=\"clean_text\", n_process=n_process)\n",
    "\n",
    "print(\"NER done.\")\n",
    "\n",
    "# Keep raw copies\n",
    "df_scraped[\"NER_admin0_raw\"] = df_scraped[\"NER_admin0\"]\n",
    "df_scraped[\"NER_admin1_raw\"] = df_scraped[\"NER_admin1\"]\n",
    "df_scraped[\"NER_admin2_raw\"] = df_scraped[\"NER_admin2\"]\n",
    "\n",
    "# ============================================================\n",
    "# 7Ô∏è‚É£ LOAD FEWS COUNTRIES\n",
    "# ============================================================\n",
    "\n",
    "fews_path = \"../data/fews/fews_with_conflicts_admin2.parquet\"\n",
    "fews_df = pd.read_parquet(fews_path)\n",
    "fews_countries = [c.lower() for c in fews_df[\"ADMIN0\"].unique()]\n",
    "\n",
    "# ============================================================\n",
    "# 8Ô∏è‚É£ REFINEMENT: PICK MOST FREQUENT FEWS COUNTRY PER ARTICLE\n",
    "# ============================================================\n",
    "\n",
    "def pick_main_country_from_text(text: str) -> str:\n",
    "    if not isinstance(text, str):\n",
    "        return None\n",
    "    txt = text.lower()\n",
    "    counts = Counter()\n",
    "    for country in fews_countries:\n",
    "        c = txt.count(country)\n",
    "        if c > 0:\n",
    "            counts[country] += c\n",
    "    return counts.most_common(1)[0][0] if counts else None\n",
    "\n",
    "def refine_ner_country(row):\n",
    "    main = pick_main_country_from_text(row[\"clean_text\"])\n",
    "    if main:\n",
    "        row[\"NER_admin0\"] = main\n",
    "    return row\n",
    "\n",
    "df_scraped = df_scraped.apply(refine_ner_country, axis=1)\n",
    "print(\"NER country refinement complete.\")\n",
    "\n",
    "# ============================================================\n",
    "# 9Ô∏è‚É£ FILTER BY FEWS COUNTRIES\n",
    "# ============================================================\n",
    "\n",
    "df_scraped = df_scraped[\n",
    "    df_scraped[\"NER_admin0\"].isin(fews_countries) |\n",
    "    df_scraped[\"NER_admin1\"].isin(fews_countries) |\n",
    "    df_scraped[\"NER_admin2\"].isin(fews_countries)\n",
    "]\n",
    "\n",
    "print(\"Remaining after FEWS filter:\", len(df_scraped))\n",
    "\n",
    "# ============================================================\n",
    "# üîü CRISIS FILTER (SAFE FOR GKG)\n",
    "# ============================================================\n",
    "\n",
    "# Removed \"inflation/market/price/commodity\" to avoid noise.\n",
    "crisis_terms = re.compile(\n",
    "    r\"(famine|hunger|malnutrition|undernourish|food\\s+crisis|food\\s+security|\"\n",
    "    r\"crop|harvest|yield|farmer|agricultur|seed|irrigat|fertiliz|pest|livestock|\"\n",
    "    r\"drought|flood|rainfall|storm|cyclone|heatwave|disaster|\"\n",
    "    r\"aid|relief|humanitarian|\"\n",
    "    r\"refugee|displaced|idp|\"\n",
    "    r\"conflict|violence|clash|attack|insurgent|militia|war|unrest|protest)\",\n",
    "    re.I\n",
    ")\n",
    "\n",
    "df_scraped = df_scraped[\n",
    "    df_scraped[\"clean_text\"].str.contains(crisis_terms, na=False)\n",
    "]\n",
    "\n",
    "print(\"Remaining after crisis filter:\", len(df_scraped))\n",
    "\n",
    "# ============================================================\n",
    "# 1Ô∏è‚É£1Ô∏è‚É£ SAVE OUTPUT\n",
    "# ============================================================\n",
    "\n",
    "out_path = \"../data/gdelt/gkg/scraped_urls/cleaned_filtered_urls.parquet\"\n",
    "os.makedirs(os.path.dirname(out_path), exist_ok=True)\n",
    "\n",
    "df_scraped.to_parquet(out_path, index=False)\n",
    "print(f\"Saved cleaned GKG dataset to: {out_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "19e238ca122764f831b3091dd8ff35b1b7683cbf7688be73b57ef77bc8403891"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
