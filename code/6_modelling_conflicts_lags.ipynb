{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "ven after removing previous_CS â€” the strongest persistence variable â€” the inclusion of conflict-related features still did not produce meaningful gains in accuracy or F1. This strongly suggests that conflict data, at the spatial and temporal resolution available, have limited direct predictive power for short-term IPC classification.\n",
    "\n",
    "Rather than being masked by temporal inertia (as one might suspect with previous_CS), conflict signals themselves appear to lack strong short-horizon correlation with acute food security changes when aggregated monthly at the ADMIN2 level.\n",
    "\n",
    "ðŸ“„ Draft â€“ Discussion Narrative (Full Page)\n",
    "\n",
    "The empirical analysis revealed no consistent improvement in predictive performance when conflict variables were added to models of food security outcomes. Across all specificationsâ€”logistic regression, random forest, and CatBoostâ€”the inclusion of conflict intensity, fatalities, and rolling aggregates did not yield higher accuracy or F1 scores relative to the baseline models using only geographic and temporal features. This result held across multiple rolling windows and was robust to transformations, alternative scaling, and different model families. While initially unexpected, the absence of uplift provides a valuable insight into the relative role of conflict information at this spatial and temporal resolution.\n",
    "\n",
    "A key robustness test involved excluding the lagged IPC variable (previous_CS), which captures autoregressive persistence in food security conditions. Removing this feature predictably reduced performance across all models, but the decline was substantially larger for those that included conflict indicators. This pattern suggests that the limited predictive contribution of conflict variables was not independent: their apparent signal was largely mediated through temporal persistence. In other words, conflict activity tended to co-move with regions already on deteriorating food security trajectories, rather than providing an exogenous or leading indicator of change. The correlation between conflict incidence and previous food security levels appears to reflect shared underlying structural vulnerabilitiesâ€”such as chronic instability, livelihood stress, or seasonal exposureâ€”rather than a causal or standalone driver that improves predictive accuracy.\n",
    "\n",
    "This finding aligns with a broader empirical observation in early-warning research: when time-lagged outcome variables are strong predictors of current status, exogenous features such as climate or conflict often contribute marginally to short-term classification tasks. The IPC phase, by design, evolves slowly over time, and its monthly or quarterly transitions are heavily path-dependent. Consequently, most of the predictable variance is already captured by prior values and regional fixed effects. Conflict variables, even when engineered to reflect short- and medium-term escalation or anomaly dynamics, thus operate within a narrow margin of residual variance that may be dominated by noise or reporting artifacts.\n",
    "\n",
    "The robustness analysis reinforces this interpretation. Models trained without previous_CS performed uniformly worse, but the magnitude of decline was greater for those including conflict features, implying that these features draw part of their apparent signal from temporal correlations with past IPC conditions. This suggests that conflict intensity in the ACLED data is better understood as a contextual covariateâ€”descriptive of regions prone to chronic crisesâ€”rather than a dynamic predictor capable of improving short-term food security forecasts. It also highlights the challenges of integrating event-based conflict datasets into predictive models at subnational and monthly scales, where data sparsity, spatial heterogeneity, and uneven reporting can attenuate true relationships.\n",
    "\n",
    "Taken together, these results do not imply that conflict is irrelevant to food security; rather, they indicate that at the resolution of FEWS NET IPC classifications and available ACLED variables, its effect is largely absorbed by persistent structural and temporal dynamics. The null improvement should therefore be interpreted as a signal of model sufficiency, not failure: current geographic and autoregressive features already capture the majority of predictable variation. Future work could explore whether conflict data add value in forecasting (rather than nowcasting) settings, particularly when combined with text-derived indicators from GDELT or contextual embeddings that might better capture shifts in narrative intensity and geographic spillovers.\n",
    "\n",
    "Single predictor table:\n",
    "Top single-feature signals:\n",
    "                                                 Feature       Model  \\\n",
    "0                                            previous_CS      LogReg   \n",
    "1                                            previous_CS  RandForest   \n",
    "2                                       transitions_prev      LogReg   \n",
    "11                           fatalities_region_rolling_6  RandForest   \n",
    "5                             conflicts_region_rolling_3  RandForest   \n",
    "9                            fatalities_region_rolling_3  RandForest   \n",
    "7                             conflicts_region_rolling_6  RandForest   \n",
    "6                             conflicts_region_rolling_6      LogReg   \n",
    "10                           fatalities_region_rolling_6      LogReg   \n",
    "5805                                        ADMIN0_Sudan  RandForest   \n",
    "5804                                        ADMIN0_Sudan      LogReg   \n",
    "4                             conflicts_region_rolling_3      LogReg   \n",
    "5787                                        ADMIN0_Kenya  RandForest   \n",
    "5786                                        ADMIN0_Kenya      LogReg   \n",
    "8                            fatalities_region_rolling_3      LogReg   \n",
    "5811                                     ADMIN0_Zimbabwe  RandForest   \n",
    "399                               region_Nigeria-Katsina  RandForest   \n",
    "411                                 region_Nigeria-Niger  RandForest   \n",
    "395                                region_Nigeria-Kaduna  RandForest   \n",
    "361                               region_Nigeria-Adamawa  RandForest   \n",
    "421                               region_Nigeria-Plateau  RandForest   \n",
    "420                               region_Nigeria-Plateau      LogReg   \n",
    "5803                                  ADMIN0_South Sudan  RandForest   \n",
    "5802                                  ADMIN0_South Sudan      LogReg   \n",
    "429                                  region_Nigeria-Yobe  RandForest   \n",
    "321                       region_Mozambique-Cabo Delgado  RandForest   \n",
    "595                             region_Zimbabwe-Midlands  RandForest   \n",
    "495                                 region_Sudan-Gedaref  RandForest   \n",
    "494                                 region_Sudan-Gedaref      LogReg   \n",
    "499                                 region_Sudan-Kassala  RandForest   \n",
    "498                                 region_Sudan-Kassala      LogReg   \n",
    "157      region_Democratic Republic of the Congo-Maniema  RandForest   \n",
    "515                            region_Sudan-South Darfur  RandForest   \n",
    "514                            region_Sudan-South Darfur      LogReg   \n",
    "430                               region_Nigeria-Zamfara      LogReg   \n",
    "431                               region_Nigeria-Zamfara  RandForest   \n",
    "492                             region_Sudan-East Darfur      LogReg   \n",
    "493                             region_Sudan-East Darfur  RandForest   \n",
    "159   region_Democratic Republic of the Congo-North Kivu  RandForest   \n",
    "425                                region_Nigeria-Sokoto  RandForest   \n",
    "344                                   region_Niger-Diffa      LogReg   \n",
    "345                                   region_Niger-Diffa  RandForest   \n",
    "507                                region_Sudan-Northern  RandForest   \n",
    "83                             region_Cameroon-Northwest  RandForest   \n",
    "513                                  region_Sudan-Sinnar  RandForest   \n",
    "512                                  region_Sudan-Sinnar      LogReg   \n",
    "472                           region_South Sudan-Jonglei      LogReg   \n",
    "473                           region_South Sudan-Jonglei  RandForest   \n",
    "301                               region_Malawi-Southern  RandForest   \n",
    "323                               region_Mozambique-Gaza  RandForest   \n",
    "\n",
    "      Accuracy        F1  \n",
    "0     0.776534  0.775885  \n",
    "1     0.776534  0.775885  \n",
    "2     0.468428  0.439228  \n",
    "11    0.386923  0.377730  \n",
    "5     0.333184  0.367355  \n",
    "9     0.395432  0.360770  \n",
    "7     0.332288  0.356649  \n",
    "6     0.340797  0.352017  \n",
    "10    0.371697  0.350575  \n",
    "5805  0.476489  0.337799  \n",
    "5804  0.476489  0.337799  \n",
    "4     0.327810  0.332528  \n",
    "5787  0.417376  0.322998  \n",
    "5786  0.417376  0.322998  \n",
    "8     0.345723  0.312611  \n",
    "5811  0.446485  0.292952  \n",
    "399   0.441111  0.281918  \n",
    "411   0.439319  0.279049  \n",
    "395   0.438871  0.276854  \n",
    "361   0.438871  0.276854  \n",
    "421   0.438424  0.275726  \n",
    "420   0.438424  0.275726  \n",
    "5803  0.438424  0.274031  \n",
    "5802  0.438424  0.274031  \n",
    "429   0.437080  0.273095  \n",
    "321   0.435737  0.270464  \n",
    "595   0.435737  0.270114  \n",
    "495   0.435737  0.269906  \n",
    "494   0.435737  0.269906  \n",
    "499   0.435289  0.268976  \n",
    "498   0.435289  0.268976  \n",
    "157   0.434841  0.268206  \n",
    "515   0.434841  0.267592  \n",
    "514   0.434841  0.267592  \n",
    "430   0.434393  0.267487  \n",
    "431   0.434393  0.267487  \n",
    "492   0.434393  0.267165  \n",
    "493   0.434393  0.267165  \n",
    "159   0.434393  0.267102  \n",
    "425   0.433498  0.266244  \n",
    "344   0.433945  0.266215  \n",
    "345   0.433945  0.266215  \n",
    "507   0.433945  0.266158  \n",
    "83    0.433945  0.266158  \n",
    "513   0.433945  0.266158  \n",
    "512   0.433945  0.266158  \n",
    "472   0.433945  0.265899  \n",
    "473   0.433945  0.265899  \n",
    "301   0.433498  0.265621  \n",
    "323   0.433498  0.265552  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV, train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix, f1_score, multilabel_confusion_matrix\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn import metrics, preprocessing\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "#from imblearn.over_sampling import SMOTE, ADASYN\n",
    "from sklearn.utils import class_weight\n",
    "import pandas as pd\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "\n",
    "# -----------------------------\n",
    "# Load and expand data\n",
    "# -----------------------------\n",
    "df = pd.read_parquet(\"../data_2023/acled_data/processed/fews_with_conflicts_admin2.parquet\")\n",
    "df = df.copy()\n",
    "\n",
    "# 1) Normalize period to 6-char strings (YYYYMM)\n",
    "df['period'] = df['period'].astype(str).str.zfill(6)\n",
    "\n",
    "# 2) Full monthly period list\n",
    "all_periods = pd.period_range('2016-02', '2024-02', freq='M').strftime('%Y%m')\n",
    "df_periods = pd.DataFrame({'period': all_periods})\n",
    "\n",
    "# 3) Unique ADMIN combos\n",
    "admins = df[['ADMIN0','ADMIN1','ADMIN2']].drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "# 4) Cartesian join (ADMINs Ã— periods)\n",
    "admins['__k'] = 1\n",
    "df_periods['__k'] = 1\n",
    "df_full = admins.merge(df_periods, on='__k').drop(columns='__k')\n",
    "df_full = df_full.sort_values(['ADMIN0','ADMIN1','ADMIN2','period']).reset_index(drop=True)\n",
    "\n",
    "# 5) Merge back\n",
    "df = pd.merge(df_full, df, on=['ADMIN0','ADMIN1','ADMIN2','period'], how='outer')\n",
    "df.fillna(0.0, inplace=True)\n",
    "\n",
    "# -----------------------------\n",
    "# Region-level rolling aggregates (safe shift(1))\n",
    "# -----------------------------\n",
    "tmp_by_region = (\n",
    "    df.groupby(['ADMIN0', 'ADMIN1', 'period'])[['conflicts','fatalities']]\n",
    "      .sum()\n",
    "      .reset_index()\n",
    "      .sort_values(['ADMIN0', 'ADMIN1', 'period'])\n",
    ")\n",
    "\n",
    "for w in [3, 6]:\n",
    "    tmp_by_region[f'conflicts_region_rolling_{w}'] = (\n",
    "        tmp_by_region.groupby(['ADMIN0', 'ADMIN1'])['conflicts']\n",
    "        .shift(1).rolling(w, min_periods=1).sum().reset_index(0, drop=True)\n",
    "    )\n",
    "    tmp_by_region[f'fatalities_region_rolling_{w}'] = (\n",
    "        tmp_by_region.groupby(['ADMIN0', 'ADMIN1'])['fatalities']\n",
    "        .shift(1).rolling(w, min_periods=1).sum().reset_index(0, drop=True)\n",
    "    )\n",
    "\n",
    "tmp_by_region.drop(columns=['conflicts', 'fatalities'], inplace=True)\n",
    "df = pd.merge(df, tmp_by_region, on=['ADMIN0', 'ADMIN1', 'period'], how='left')\n",
    "df.fillna(0.0, inplace=True)\n",
    "\n",
    "# -----------------------------\n",
    "# âœ… NEW: Conflict feature engineering\n",
    "# -----------------------------\n",
    "\n",
    "# Conflict anomalies (relative to each region's historical mean)\n",
    "df['conflict_anomaly_3'] = (\n",
    "    df['conflicts_region_rolling_3'] -\n",
    "    df.groupby(['ADMIN0', 'ADMIN1'])['conflicts_region_rolling_3'].transform('mean')\n",
    ")\n",
    "df['fatality_anomaly_3'] = (\n",
    "    df['fatalities_region_rolling_3'] -\n",
    "    df.groupby(['ADMIN0', 'ADMIN1'])['fatalities_region_rolling_3'].transform('mean')\n",
    ")\n",
    "\n",
    "# Escalation: difference between short-term (3m) and medium-term (6m)\n",
    "df['conflict_escalation'] = (\n",
    "    df['conflicts_region_rolling_3'] - df['conflicts_region_rolling_6']\n",
    ")\n",
    "df['fatality_escalation'] = (\n",
    "    df['fatalities_region_rolling_3'] - df['fatalities_region_rolling_6']\n",
    ")\n",
    "\n",
    "# Binary â€œshockâ€ indicator (positive escalation)\n",
    "df['conflict_shock'] = (df['conflict_escalation'] > 0).astype(int)\n",
    "df['fatality_shock'] = (df['fatality_escalation'] > 0).astype(int)\n",
    "\n",
    "# Intensity ratios â€” normalize short vs long\n",
    "df['conflict_intensity_ratio'] = (\n",
    "    df['conflicts_region_rolling_3'] / (df['conflicts_region_rolling_6'] + 1e-6)\n",
    ")\n",
    "df['fatality_intensity_ratio'] = (\n",
    "    df['fatalities_region_rolling_3'] / (df['fatalities_region_rolling_6'] + 1e-6)\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# Cleanup and export\n",
    "# -----------------------------\n",
    "drop_cols = [\n",
    "    'Battles','Battles_with_fatalities','Explosions/Remote violence','Explosions/Remote violence_with_fatalities',\n",
    "    'Protests','Protests_with_fatalities','Riots','Riots_with_fatalities',\n",
    "    'Strategic developments','Strategic developments_with_fatalities',\n",
    "    'Violence against civilians','Violence against civilians_with_fatalities','HA_score'\n",
    "]\n",
    "df.drop(columns=[c for c in drop_cols if c in df.columns], inplace=True, errors='ignore')\n",
    "\n",
    "df = df[df['CS_score'].isin([1.0,2.0,3.0,4.0,5.0])]\n",
    "\n",
    "# Save updated dataset\n",
    "df.to_parquet(\"../data_2023/acled_data/processed/fews_with_conflicts_admin_with_lags.parquet\")\n",
    "df.to_csv(\"../data_2023/acled_data/processed/fews_with_conflicts_admin_with_lags.csv\", index=False)\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\"../data_2023/acled_data/processed/fews_with_conflicts_admin_with_lags.parquet\")\n",
    "df = df[df['CS_score'].isin([1.0, 2.0, 3.0, 4.0, 5.0])]\n",
    "\n",
    "df['CS_score'] = np.ceil(df['CS_score'])\n",
    "train = df[df['period'] != max(df['period'])]\n",
    "test = df[df['period'] == max(df['period'])]\n",
    "print(min(test['CS_score']))\n",
    "print(max(df['period']))\n",
    "\n",
    "base_summary = pd.DataFrame(columns = ['Model', 'Test Accuracy', 'Test Precision','Test Recall', 'F1'])\n",
    "base_models = ['PPS', 'SPLY', 'Max-2PP']\n",
    "accuracies = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "f1s = []\n",
    "# First naive model: score is the same as the score in the last period (PPS)\n",
    "train = train[train['period'].isin(sorted(train['period'].unique())[-1:])]\n",
    "print(train['period'].unique())\n",
    "\n",
    "train = train[['ADMIN0', 'ADMIN1', 'ADMIN2', 'CS_score', 'period']]\n",
    "train.rename(columns={'CS_score':'predicted'}, inplace=True)\n",
    "print(min(train['predicted']))\n",
    "test = pd.merge(test, train, on = ['ADMIN0', 'ADMIN1', 'ADMIN2'], how='left')\n",
    "test['predicted'] = test['predicted'].fillna(0)\n",
    "print(len(test.index))\n",
    "\n",
    "accuracies.append(accuracy_score(test['CS_score'], test['predicted']))\n",
    "precisions.append(precision_score(test['CS_score'], test['predicted'], average='weighted', zero_division=0))\n",
    "recalls.append(recall_score(test['CS_score'], test['predicted'], average='weighted', zero_division=0))\n",
    "f1s.append(f1_score(test['CS_score'], test['predicted'], average='weighted', zero_division=0))\n",
    "\n",
    "# Second naive model: score is the same as the score in the same period last year (SPLY)\n",
    "train = df[df['period'] != max(df['period'])]\n",
    "test = df[df['period'] == max(df['period'])]\n",
    "print(min(test['CS_score']))\n",
    "\n",
    "current_period = max(df['period'])\n",
    "last_year_num = int(str(current_period)) - 100\n",
    "try:\n",
    "    last_year = type(current_period)(last_year_num)\n",
    "except Exception:\n",
    "    last_year = str(last_year_num)\n",
    "train = train[train['period'] == last_year]\n",
    "print(train['period'].unique())\n",
    "\n",
    "train = train[['ADMIN0', 'ADMIN1' , 'ADMIN2', 'CS_score', 'period']]\n",
    "train.rename(columns={'CS_score':'predicted'}, inplace=True)\n",
    "test = pd.merge(test, train, on = ['ADMIN0', 'ADMIN1', 'ADMIN2'], how='left')\n",
    "test['predicted'] = test['predicted'].fillna(0)\n",
    "print(len(test.index))\n",
    "\n",
    "accuracies.append(accuracy_score(test['CS_score'], test['predicted']))\n",
    "precisions.append(precision_score(test['CS_score'], test['predicted'], average='weighted', zero_division=0))\n",
    "recalls.append(recall_score(test['CS_score'], test['predicted'], average='weighted', zero_division=0))\n",
    "f1s.append(f1_score(test['CS_score'], test['predicted'], average='weighted', zero_division=0))\n",
    "\n",
    "# Third naive model: take max score of the previous 2 periods (Max-2PP)\n",
    "train = df[df['period'] != max(df['period'])]\n",
    "test = df[df['period'] == max(df['period'])]\n",
    "print(min(test['CS_score']))\n",
    "\n",
    "train =  train[train['period'].isin(sorted(train['period'].unique())[-2:])]\n",
    "print(train['period'].unique())\n",
    "\n",
    "train = train[['ADMIN0', 'ADMIN1' , 'ADMIN2', 'CS_score', 'period']]\n",
    "train = pd.DataFrame(train.groupby(['ADMIN0', 'ADMIN1', 'ADMIN2'])['CS_score'].max()).reset_index()\n",
    "train.rename(columns={'CS_score':'predicted'}, inplace=True)\n",
    "test = pd.merge(test, train, on = ['ADMIN0', 'ADMIN1', 'ADMIN2'], how='left')\n",
    "test['predicted'] = test['predicted'].fillna(0)\n",
    "print(len(test.index))\n",
    "\n",
    "accuracies.append(accuracy_score(test['CS_score'], test['predicted']))\n",
    "precisions.append(precision_score(test['CS_score'], test['predicted'], average='weighted', zero_division=0))\n",
    "recalls.append(recall_score(test['CS_score'], test['predicted'], average='weighted', zero_division=0))\n",
    "f1s.append(f1_score(test['CS_score'], test['predicted'], average='weighted', zero_division=0))\n",
    "\n",
    "base_summary['Model'] = base_models\n",
    "base_summary['Test Accuracy'] = accuracies\n",
    "base_summary['Test Precision'] = precisions\n",
    "base_summary['Test Recall'] = recalls\n",
    "base_summary['F1'] = f1s\n",
    "base_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## FEWS only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\"../data_2023/acled_data/processed/fews_with_conflicts_admin_with_lags.parquet\")\n",
    "df = df[df['CS_score'].isin([1.0, 2.0, 3.0, 4.0, 5.0])]\n",
    "\n",
    "df['region'] = df['ADMIN0'] + '-' + df['ADMIN1']\n",
    "df['district']   = df['ADMIN0'] + '-' + df['ADMIN1'] + '-' + df['ADMIN2']\n",
    "df = df.drop(['ADMIN1', 'ADMIN2'], axis=1)\n",
    "\n",
    "# Target\n",
    "df['CS_score_class'] = np.ceil(df['CS_score'])\n",
    "\n",
    "# Keep only last 7 periods (evaluation window)\n",
    "df = df[df['period'].isin(sorted(df['period'].unique())[-7:])]\n",
    "\n",
    "# Sort to ensure deterministic lags, then compute features BEFORE split (no leakage)\n",
    "df = df.sort_values(['region', 'period'])\n",
    "df['previous_CS'] = df.groupby('region', sort=False)['CS_score_class'].shift(1)\n",
    "\n",
    "# Count changes, then cumsum and shift so it reflects info only up to t-1\n",
    "change_flag = (\n",
    "    df.groupby('region', sort=False)['CS_score_class']\n",
    "      .transform(lambda x: (x != x.shift()).astype(int))\n",
    ")\n",
    "df['transitions_prev'] = (\n",
    "    change_flag.groupby(df['region']).cumsum().shift(1).fillna(0).astype(int)\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# Split after feature creation\n",
    "# -----------------------------\n",
    "test_period = df['period'].max()\n",
    "train = df[df['period'] < test_period]\n",
    "train = train[train['period'].isin(sorted(train['period'].unique())[-5:])]\n",
    "\n",
    "test  = df[df['period'] == test_period]\n",
    "print(test_period)\n",
    "print(train['period'].unique())\n",
    "print(len(train.index))\n",
    "print(len(test.index))\n",
    "# -----------------------------\n",
    "# Drop rows where previous_CS is missing\n",
    "# -----------------------------\n",
    "# OK to drop in train\n",
    "train = train[train['previous_CS'].isin([1,2,3,4,5])]\n",
    "# Also drop in test to avoid NaNs in models; after fixing lags this should be a small drop\n",
    "test  = test[test['previous_CS'].isin([1,2,3,4,5])]\n",
    "\n",
    "# -----------------------------\n",
    "# Final feature set\n",
    "# -----------------------------\n",
    "use_features = [\n",
    "    'region','district','period','CS_score_class','ADMIN0',\n",
    "    'previous_CS',\n",
    "    'transitions_prev'\n",
    "]\n",
    "\n",
    "train = train[use_features]\n",
    "test  = test[use_features]\n",
    "\n",
    "train = pd.get_dummies(train, columns=['region','district','ADMIN0'], dtype=int)\n",
    "test  = pd.get_dummies(test,  columns=['region','district','ADMIN0'], dtype=int)\n",
    "\n",
    "# Align dummy columns in case train/test differ\n",
    "train, test = train.align(test, join='left', axis=1, fill_value=0)\n",
    "    \n",
    "print(\"After dummies\")\n",
    "print(train['period'].unique())\n",
    "print(len(train.index))\n",
    "print(len(test.index))\n",
    "\n",
    "cols = list(train.columns)\n",
    "cols.remove('CS_score_class')\n",
    "cols.remove('period')\n",
    "\n",
    "X = train[cols]\n",
    "y = train['CS_score_class']\n",
    "X_test = test[cols]\n",
    "y_test = test['CS_score_class']\n",
    "\n",
    "# -----------------------------\n",
    "# Models\n",
    "# -----------------------------\n",
    "LR_model = LogisticRegression(class_weight='balanced', random_state=5).fit(X, y)\n",
    "print(\"LR DONE\")\n",
    "\n",
    "RF_model = RandomForestClassifier(\n",
    "    n_estimators=200, min_samples_split=10,\n",
    "    class_weight='balanced', n_jobs=-1, random_state=5\n",
    ").fit(X, y)\n",
    "print(\"RF DONE\")\n",
    "\n",
    "Cat_model = CatBoostClassifier(\n",
    "    iterations=800,\n",
    "    depth=8,\n",
    "    learning_rate=0.05,\n",
    "    loss_function='MultiClass',\n",
    "    class_weights=[1,1,1,1],\n",
    "    random_seed=5,\n",
    "    verbose=False\n",
    ").fit(X, y)\n",
    "print(\"CAT DONE\")\n",
    "\n",
    "# -----------------------------\n",
    "# Evaluation\n",
    "# -----------------------------\n",
    "models = [LR_model, RF_model, Cat_model]\n",
    "summary = pd.DataFrame(columns=['Model','Test Accuracy','Test Precision','Test Recall','F1'])\n",
    "\n",
    "for m in models:\n",
    "    preds = m.predict(X_test)\n",
    "    summary.loc[len(summary)] = [\n",
    "        str(m),\n",
    "        accuracy_score(y_test, preds),\n",
    "        precision_score(y_test, preds, average='weighted', zero_division=0),\n",
    "        recall_score(y_test, preds, average='weighted', zero_division=0),\n",
    "        f1_score(y_test, preds, average='weighted', zero_division=0)\n",
    "    ]\n",
    "\n",
    "if 'base_summary' in globals() and isinstance(base_summary, pd.DataFrame):\n",
    "    summary = pd.concat([base_summary, summary]).reset_index(drop=True)\n",
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "\n",
    "Model\tTest Accuracy\tTest Precision\tTest Recall\tF1\n",
    "0\tPPS\t0.625947\t0.797817\t0.625947\t0.686038\n",
    "1\tSPLY\t0.420865\t0.721705\t0.420865\t0.526546\n",
    "2\tMax-2PP\t0.650468\t0.717953\t0.650468\t0.669654\n",
    "3\tLogisticRegression(class_weight='balanced', random_state=5)\t0.745186\t0.761104\t0.745186\t0.749899\n",
    "4\tRandomForestClassifier(class_weight='balanced', min_samples_split=10,\\n n_estimators=200, n_jobs=-1, random_state=5)\t0.811912\t0.814414\t0.811912\t0.812992\n",
    "5\t<catboost.core.CatBoostClassifier object at 0x107d929a0>\t0.796238\t0.785459\t0.796238\t0.785082\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract feature importance from the trained CatBoost model\n",
    "features = pd.DataFrame({\n",
    "    \"Features\": X.columns,\n",
    "    \"Importance\": Cat_model.get_feature_importance()\n",
    "})\n",
    "\n",
    "# Filter low-importance features (optional threshold)\n",
    "features = features[features[\"Importance\"] >= 0.1]\n",
    "\n",
    "# Sort ascending for a clean horizontal plot\n",
    "features = features.sort_values(\"Importance\", ascending=True)\n",
    "\n",
    "# Plot using Plotly\n",
    "fig = px.bar(\n",
    "    features,\n",
    "    x=\"Importance\",\n",
    "    y=\"Features\",\n",
    "    orientation='h',\n",
    "    title=\"Feature Importance (CatBoost, No Conflict Features)\"\n",
    ")\n",
    "fig.update_layout(title_x=0.5)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = pd.DataFrame(list(zip(X.columns, RF_model.feature_importances_)), columns =['Features', 'importance'])\n",
    "features = features[features['importance'] >= 0.005]\n",
    "features = features.sort_values('importance', ascending=True)\n",
    "fig = px.bar(features, x=\"importance\", y=\"Features\", orientation='h')\n",
    "fig.update_layout(title_text=\"Feature importance\",\n",
    "                  title_x=0.5)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Compute confusion matrix\n",
    "mcm = confusion_matrix(y_test, RF_model.predict(X_test), labels=[1, 2, 3, 4, 5])\n",
    "print(mcm)\n",
    "# Convert to percentage (relative to total observations)\n",
    "mcm_percent = mcm / mcm.sum()\n",
    "\n",
    "# Create larger figure\n",
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "# Plot heatmap; fmt='.2%' displays values as percentages\n",
    "ax = sns.heatmap(\n",
    "    mcm_percent, \n",
    "    annot=True, \n",
    "    cmap='Blues', \n",
    "    fmt='.1%',   # Format as percentage with 2 decimals\n",
    "    annot_kws={'size': 14}\n",
    ")\n",
    "\n",
    "ax.set_title('Confusion Matrix for ML Model (Percentage)\\n', fontsize=20, pad=20)\n",
    "ax.set_xlabel('\\nPredicted Values', fontsize=14)\n",
    "ax.set_ylabel('Actual Values', fontsize=14)\n",
    "\n",
    "ax.xaxis.set_ticklabels(['1','2','3','4', '5'], fontsize=15)\n",
    "ax.yaxis.set_ticklabels(['1','2','3','4', '5'], fontsize=15)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## Adding conflict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# -----------------------------\n",
    "# Load data\n",
    "# -----------------------------\n",
    "df = pd.read_parquet(\"../data_2023/acled_data/processed/fews_with_conflicts_admin_with_lags.parquet\")\n",
    "df = df[df['CS_score'].isin([1.0, 2.0, 3.0, 4.0, 5.0])]\n",
    "\n",
    "# Region identifiers\n",
    "df['region'] = df['ADMIN0'] + '-' + df['ADMIN1']\n",
    "df['district']   = df['ADMIN0'] + '-' + df['ADMIN1'] + '-' + df['ADMIN2']\n",
    "df = df.drop(['ADMIN1', 'ADMIN2'], axis=1)\n",
    "\n",
    "# Target\n",
    "df['CS_score_class'] = np.ceil(df['CS_score'])\n",
    "\n",
    "# Keep only last 7 periods for evaluation\n",
    "df = df[df['period'].isin(sorted(df['period'].unique())[-7:])]\n",
    "df = df.sort_values(['region', 'period'])\n",
    "# -----------------------------\n",
    "# Safe feature engineering (done BEFORE split)\n",
    "# -----------------------------\n",
    "# Lagged CS\n",
    "df['previous_CS'] = df.groupby('region')['CS_score_class'].shift(1)\n",
    "\n",
    "# Transition count (safe: cumulative transitions up to t-1 only)\n",
    "change_flag = (\n",
    "    df.groupby('region')['CS_score_class']\n",
    "      .transform(lambda x: (x != x.shift()).astype(int))\n",
    ")\n",
    "# existing\n",
    "df['transitions_prev'] = (\n",
    "    change_flag.groupby(df['region']).cumsum().shift(1).fillna(0).astype(int)\n",
    ")\n",
    "\n",
    "\n",
    "print(df['period'].unique())\n",
    "print(max(df['period']))\n",
    "train = df[df['period'] != max(df['period'])]\n",
    "train = train[train['period'].isin(sorted(train['period'].unique())[-5:])]\n",
    "\n",
    "test = df[df['period'] == max(df['period'])]\n",
    "\n",
    "print(max(df['period']))\n",
    "print(train['period'].unique())\n",
    "print(len(train.index))\n",
    "print(len(test.index))\n",
    "\n",
    "# Drop rows with missing previous_CS\n",
    "train = train[train['previous_CS'].isin([1,2,3,4,5])]\n",
    "test  = test[test['previous_CS'].isin([1,2,3,4,5])]\n",
    "\n",
    "# -----------------------------\n",
    "# Feature set\n",
    "# -----------------------------\n",
    "lag_features = [c for c in df.columns if any(f\"_lag{i}\" in c for i in range(1, 7))]\n",
    "\n",
    "use_features = [\n",
    "    # --- Geography & target context ---\n",
    "    'region', 'district', 'period', 'CS_score_class', 'ADMIN0',\n",
    "\n",
    "    # --- Core temporal features ---\n",
    "    'previous_CS',\n",
    "    'transitions_prev',\n",
    "\n",
    "    # --- Core conflict aggregates ---\n",
    "    'conflicts', 'fatalities',\n",
    "    'conflicts_region_rolling_3', 'conflicts_region_rolling_6',\n",
    "    'fatalities_region_rolling_3', 'fatalities_region_rolling_6',\n",
    "\n",
    "    # --- Conflict anomalies ---\n",
    "    'conflict_anomaly_3', 'fatality_anomaly_3',\n",
    "\n",
    "    # --- Escalation features ---\n",
    "    'conflict_escalation', 'fatality_escalation',\n",
    "\n",
    "    # --- Binary shocks ---\n",
    "    'conflict_shock', 'fatality_shock',\n",
    "\n",
    "    # --- Intensity ratios ---\n",
    "    'conflict_intensity_ratio', 'fatality_intensity_ratio'\n",
    "]\n",
    "\n",
    "\n",
    "train = train[use_features]\n",
    "test  = test[use_features]\n",
    "\n",
    "# Log transform heavy-tailed or skewed variables\n",
    "log_cols = [\n",
    "    'conflicts', 'fatalities',\n",
    "    'conflicts_region_rolling_3', 'conflicts_region_rolling_6',\n",
    "    'fatalities_region_rolling_3', 'fatalities_region_rolling_6',\n",
    "    'conflict_anomaly_3', 'fatality_anomaly_3',\n",
    "    'conflict_escalation', 'fatality_escalation',\n",
    "    'conflict_intensity_ratio', 'fatality_intensity_ratio'\n",
    "]\n",
    "\n",
    "\n",
    "for subset_name in ['train', 'test']:\n",
    "    subset = {'train': train, 'test': test}[subset_name]\n",
    "    # apply log1p safely (avoid âˆ’inf by clipping negatives)\n",
    "    subset.loc[:, log_cols] = np.log1p(subset[log_cols].clip(lower=0))\n",
    "    if subset_name == 'train':\n",
    "        train = subset\n",
    "    else:\n",
    "        test = subset\n",
    "\n",
    "# Scale\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "train.loc[:, log_cols] = min_max_scaler.fit_transform(train[log_cols])\n",
    "test.loc[:, log_cols]  = min_max_scaler.transform(test[log_cols])\n",
    "\n",
    "# One-hot encode geography\n",
    "train = pd.get_dummies(train, columns=['region','district','ADMIN0'], dtype=int)\n",
    "test  = pd.get_dummies(test,  columns=['region','district','ADMIN0'], dtype=int)\n",
    "\n",
    "# Align dummy columns\n",
    "train, test = train.align(test, join='left', axis=1, fill_value=0)\n",
    "\n",
    "print(\"After dummies\")\n",
    "print(train['period'].unique())\n",
    "print(len(train.index))\n",
    "print(len(test.index))\n",
    "# -----------------------------\n",
    "# Train/test split\n",
    "# -----------------------------\n",
    "cols = list(train.columns)\n",
    "cols.remove('CS_score_class')\n",
    "cols.remove('period')\n",
    "\n",
    "X = train[cols]\n",
    "print(train.columns)\n",
    "y = train['CS_score_class']\n",
    "X_test = test[cols]\n",
    "y_test = test['CS_score_class']\n",
    "\n",
    "# -----------------------------\n",
    "# Models\n",
    "# -----------------------------\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "\n",
    "LR_model = LogisticRegression(class_weight='balanced', random_state=5).fit(X, y)\n",
    "print(\"LR DONE\")\n",
    "\n",
    "RF_model = RandomForestClassifier(\n",
    "    n_estimators=200, min_samples_split=10,\n",
    "    class_weight='balanced', n_jobs=-1, random_state=5\n",
    ").fit(X, y)\n",
    "print(\"RF DONE\")\n",
    "\n",
    "Cat_model = CatBoostClassifier(\n",
    "    iterations=800,\n",
    "    depth=8,\n",
    "    learning_rate=0.05,\n",
    "    loss_function='MultiClass',\n",
    "    class_weights=[1,1,1,1],\n",
    "    random_seed=5,\n",
    "    verbose=False\n",
    ").fit(X, y)\n",
    "print(\"CAT DONE\")\n",
    "\n",
    "# -----------------------------\n",
    "# Evaluation\n",
    "# -----------------------------\n",
    "models = [LR_model, RF_model, Cat_model]\n",
    "summary = pd.DataFrame(columns=['Model','Test Accuracy','Test Precision','Test Recall','F1'])\n",
    "\n",
    "for m in models:\n",
    "    preds = m.predict(X_test)\n",
    "    summary.loc[len(summary)] = [\n",
    "        str(m),\n",
    "        accuracy_score(y_test, preds),\n",
    "        precision_score(y_test, preds, average='weighted', zero_division=0),\n",
    "        recall_score(y_test, preds, average='weighted', zero_division=0),\n",
    "        f1_score(y_test, preds, average='weighted', zero_division=0)\n",
    "    ]\n",
    "\n",
    "if 'base_summary' in globals() and isinstance(base_summary, pd.DataFrame):\n",
    "    summary = pd.concat([base_summary, summary]).reset_index(drop=True)\n",
    "summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "\tModel\tTest Accuracy\tTest Precision\tTest Recall\tF1\n",
    "0\tPPS\t0.625947\t0.797817\t0.625947\t0.686038\n",
    "1\tSPLY\t0.420865\t0.721705\t0.420865\t0.526546\n",
    "2\tMax-2PP\t0.650468\t0.717953\t0.650468\t0.669654\n",
    "3\tLogisticRegression(class_weight='balanced', random_state=5)\t0.732199\t0.750911\t0.732199\t0.736602\n",
    "4\tRandomForestClassifier(class_weight='balanced', min_samples_split=10,\\n n_estimators=200, n_jobs=-1, random_state=5)\t0.772951\t0.773919\t0.772951\t0.771791\n",
    "5\t<catboost.core.CatBoostClassifier object at 0x107d90340>\t0.796686\t0.786271\t0.796686\t0.785864\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract feature importance from the trained CatBoost model\n",
    "features = pd.DataFrame({\n",
    "    \"Features\": X.columns,\n",
    "    \"Importance\": Cat_model.get_feature_importance()\n",
    "})\n",
    "\n",
    "# Filter low-importance features (optional threshold)\n",
    "features = features[features[\"Importance\"] >= 0.1]\n",
    "\n",
    "# Sort ascending for a clean horizontal plot\n",
    "features = features.sort_values(\"Importance\", ascending=True)\n",
    "\n",
    "# Plot using Plotly\n",
    "fig = px.bar(\n",
    "    features,\n",
    "    x=\"Importance\",\n",
    "    y=\"Features\",\n",
    "    orientation='h',\n",
    "    title=\"Feature Importance (CatBoost, with Conflict Features)\"\n",
    ")\n",
    "fig.update_layout(title_x=0.5)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = pd.DataFrame(list(zip(X.columns, Cat_model.get_feature_importance())), columns =['Features', 'importance'])\n",
    "features = features[features['importance'] >= 0.05]\n",
    "features = features.sort_values('importance', ascending=True)\n",
    "fig = px.bar(features, x=\"importance\", y=\"Features\", orientation='h')\n",
    "fig.update_layout(title_text=\"Feature importance\",\n",
    "                  title_x=0.5)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Compute confusion matrix\n",
    "mcm = confusion_matrix(y_test, RF_model.predict(X_test), labels=[1, 2, 3, 4, 5])\n",
    "print(mcm)\n",
    "\n",
    "# Convert to percentage (relative to total observations)\n",
    "if mcm.sum() > 0:\n",
    "    mcm_percent = mcm / mcm.sum()\n",
    "else:\n",
    "    mcm_percent = mcm\n",
    "\n",
    "# Create larger figure\n",
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "# Plot heatmap; fmt='.2%' displays values as percentages\n",
    "ax = sns.heatmap(\n",
    "    mcm_percent, \n",
    "    annot=True, \n",
    "    cmap='Blues', \n",
    "    fmt='.1%',   # Format as percentage with 2 decimals\n",
    "    annot_kws={'size': 14}\n",
    ")\n",
    "\n",
    "ax.set_title('Confusion Matrix for ML Model (Percentage)\\n', fontsize=20, pad=20)\n",
    "ax.set_xlabel('\\nPredicted Values', fontsize=14)\n",
    "ax.set_ylabel('Actual Values', fontsize=14)\n",
    "\n",
    "ax.xaxis.set_ticklabels(['1','2','3','4', '5'], fontsize=15)\n",
    "ax.yaxis.set_ticklabels(['1','2','3','4', '5'], fontsize=15)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import shap\n",
    "from catboost import CatBoostClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# ============================================================\n",
    "# 1. Select diagnostic subset (temporal + conflict features)\n",
    "# ============================================================\n",
    "diag_cols = [\n",
    "    'previous_CS', 'transitions_prev',\n",
    "    'conflicts', 'fatalities',\n",
    "    'conflicts_region_rolling_3', 'conflicts_region_rolling_6',\n",
    "    'fatalities_region_rolling_3', 'fatalities_region_rolling_6',\n",
    "    'conflict_anomaly_3', 'fatality_anomaly_3',\n",
    "    'conflict_escalation', 'fatality_escalation',\n",
    "    'conflict_intensity_ratio', 'fatality_intensity_ratio'\n",
    "]\n",
    "\n",
    "# Keep only columns that exist\n",
    "diag_cols = [c for c in diag_cols if c in X.columns]\n",
    "X_diag = X[diag_cols].copy()\n",
    "y_diag = y.copy()\n",
    "\n",
    "print(f\"ðŸ§  Selected {len(diag_cols)} diagnostic features: {diag_cols}\")\n",
    "\n",
    "# ============================================================\n",
    "# 2. Train lightweight diagnostic CatBoost model\n",
    "# ============================================================\n",
    "Cat_diag = CatBoostClassifier(\n",
    "    iterations=200,\n",
    "    depth=6,\n",
    "    learning_rate=0.05,\n",
    "    loss_function='MultiClass',\n",
    "    random_seed=42,\n",
    "    verbose=False\n",
    ").fit(X_diag, y_diag)\n",
    "\n",
    "print(f\"âœ… Diagnostic CatBoost model trained on {len(diag_cols)} features.\")\n",
    "\n",
    "# ============================================================\n",
    "# 3. Compute SHAP interaction values (sample subset)\n",
    "# ============================================================\n",
    "X_sample = X_diag.sample(min(300, len(X_diag)), random_state=42)\n",
    "explainer = shap.TreeExplainer(Cat_diag)\n",
    "\n",
    "print(\"â³ Computing SHAP interaction values...\")\n",
    "interaction_values_list = explainer.shap_interaction_values(X_sample)\n",
    "print(\"âœ… SHAP interaction computation complete.\")\n",
    "\n",
    "# Average across classes if multiclass\n",
    "if isinstance(interaction_values_list, list):\n",
    "    interaction_values = np.mean(interaction_values_list, axis=0)\n",
    "else:\n",
    "    interaction_values = interaction_values_list\n",
    "\n",
    "# ============================================================\n",
    "# 4. Compute mean |interaction| between base and conflict vars\n",
    "# ============================================================\n",
    "base_vars = ['previous_CS', 'transitions_prev']\n",
    "conflict_vars = [c for c in diag_cols if c not in base_vars]\n",
    "\n",
    "results = []\n",
    "for conflict_var in conflict_vars:\n",
    "    for base_var in base_vars:\n",
    "        idx_base = X_sample.columns.get_loc(base_var)\n",
    "        idx_conf = X_sample.columns.get_loc(conflict_var)\n",
    "        mean_inter = np.abs(interaction_values[:, idx_base, idx_conf]).mean()\n",
    "        results.append((base_var, conflict_var, mean_inter))\n",
    "\n",
    "inter_df = pd.DataFrame(results, columns=['base_var', 'conflict_var', 'mean_abs_interaction'])\n",
    "\n",
    "# Drop all-zero rows (noise)\n",
    "inter_df = inter_df[inter_df['mean_abs_interaction'] > 0]\n",
    "\n",
    "print(\"\\n=== Mean absolute SHAP interaction values ===\")\n",
    "print(inter_df.sort_values('mean_abs_interaction', ascending=False).round(4).head(10))\n",
    "\n",
    "# ============================================================\n",
    "# 5. Horizontal heatmap (scientific notation, all values shown)\n",
    "# ============================================================\n",
    "pivot = inter_df.pivot(index='base_var', columns='conflict_var', values='mean_abs_interaction')\n",
    "\n",
    "# Keep all rows/cols (even if small)\n",
    "rows, cols = pivot.shape\n",
    "fig_width = max(12, 0.9 * cols)\n",
    "fig_height = max(5, 0.6 * rows)\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "sns.heatmap(\n",
    "    pivot,\n",
    "    annot=True,\n",
    "    fmt=\".2e\",  # scientific notation (2 decimals)\n",
    "    cmap=\"viridis\",\n",
    "    cbar_kws={\"label\": \"Mean |SHAP Interaction|\"},\n",
    "    linewidths=0.6,\n",
    "    square=False,\n",
    "    annot_kws={\"size\": 9}\n",
    ")\n",
    "plt.title(\"Mean |SHAP Interaction| Between Temporal and Conflict Features\", fontsize=14, pad=16)\n",
    "plt.xlabel(\"Conflict-Related Features\", fontsize=12)\n",
    "plt.ylabel(\"Temporal Features\", fontsize=12)\n",
    "plt.xticks(rotation=35, ha=\"right\", fontsize=9)\n",
    "plt.yticks(rotation=0, fontsize=10)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "## Full loop with all metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "# NO CONFLICT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# ======================\n",
    "# Load Data\n",
    "# ======================\n",
    "df = pd.read_parquet(\"../data_2023/acled_data/processed/fews_with_conflicts_admin_with_lags.parquet\")\n",
    "df = df[df['CS_score'].isin([1.0, 2.0, 3.0, 4.0, 5.0])]\n",
    "df['CS_score'] = np.ceil(df['CS_score'])\n",
    "\n",
    "periods = sorted(df['period'].unique())\n",
    "print(f\"Total periods: {len(periods)}\")\n",
    "\n",
    "# ======================\n",
    "# Storage\n",
    "# ======================\n",
    "results = []\n",
    "\n",
    "# ======================\n",
    "# Loop through periods\n",
    "# ======================\n",
    "for i, p in enumerate(periods[1:], start=1):  # start after the first period\n",
    "    test = df[df['period'] == p]\n",
    "    train = df[df['period'] < p]\n",
    "\n",
    "    # --- PPS: previous period ---\n",
    "    prev_p = periods[i-1]\n",
    "    train_pps = train[train['period'] == prev_p][['ADMIN0','ADMIN1','ADMIN2','CS_score']]\n",
    "    train_pps = train_pps.rename(columns={'CS_score':'predicted'})\n",
    "    test_pps = test.merge(train_pps, on=['ADMIN0','ADMIN1','ADMIN2'], how='left').fillna(0)\n",
    "    results.append({\n",
    "        'Period': p,\n",
    "        'Model': 'PPS',\n",
    "        'Test Accuracy': accuracy_score(test_pps['CS_score'], test_pps['predicted']),\n",
    "        'Test Precision': precision_score(test_pps['CS_score'], test_pps['predicted'], average='weighted', zero_division=0),\n",
    "        'Test Recall': recall_score(test_pps['CS_score'], test_pps['predicted'], average='weighted', zero_division=0),\n",
    "        'F1': f1_score(test_pps['CS_score'], test_pps['predicted'], average='weighted', zero_division=0)\n",
    "    })\n",
    "\n",
    "    # --- SPLY: same period last year ---\n",
    "    last_year = str(int(p) - 100)\n",
    "    if last_year in periods:\n",
    "        train_sply = train[train['period'] == last_year][['ADMIN0','ADMIN1','ADMIN2','CS_score']]\n",
    "        train_sply = train_sply.rename(columns={'CS_score':'predicted'})\n",
    "        test_sply = test.merge(train_sply, on=['ADMIN0','ADMIN1','ADMIN2'], how='left').fillna(0)\n",
    "        results.append({\n",
    "            'Period': p,\n",
    "            'Model': 'SPLY',\n",
    "            'Test Accuracy': accuracy_score(test_sply['CS_score'], test_sply['predicted']),\n",
    "            'Test Precision': precision_score(test_sply['CS_score'], test_sply['predicted'], average='weighted', zero_division=0),\n",
    "            'Test Recall': recall_score(test_sply['CS_score'], test_sply['predicted'], average='weighted', zero_division=0),\n",
    "            'F1': f1_score(test_sply['CS_score'], test_sply['predicted'], average='weighted', zero_division=0)\n",
    "        })\n",
    "\n",
    "    # --- Max-2PP: max of previous 2 periods ---\n",
    "    if i >= 2:\n",
    "        prev2 = [periods[i-2], periods[i-1]]\n",
    "        train_m2 = train[train['period'].isin(prev2)]\n",
    "        train_m2 = train_m2.groupby(['ADMIN0','ADMIN1','ADMIN2'])['CS_score'].max().reset_index()\n",
    "        train_m2 = train_m2.rename(columns={'CS_score':'predicted'})\n",
    "        test_m2 = test.merge(train_m2, on=['ADMIN0','ADMIN1','ADMIN2'], how='left').fillna(0)\n",
    "        results.append({\n",
    "            'Period': p,\n",
    "            'Model': 'Max-2PP',\n",
    "            'Test Accuracy': accuracy_score(test_m2['CS_score'], test_m2['predicted']),\n",
    "            'Test Precision': precision_score(test_m2['CS_score'], test_m2['predicted'], average='weighted', zero_division=0),\n",
    "            'Test Recall': recall_score(test_m2['CS_score'], test_m2['predicted'], average='weighted', zero_division=0),\n",
    "            'F1': f1_score(test_m2['CS_score'], test_m2['predicted'], average='weighted', zero_division=0)\n",
    "        })\n",
    "\n",
    "# ======================\n",
    "# Period-by-period results\n",
    "# ======================\n",
    "base_summary = pd.DataFrame(results)\n",
    "print(\"\\n=== PERIOD-BY-PERIOD BASELINE RESULTS ===\")\n",
    "print(base_summary.head(15))\n",
    "\n",
    "# ======================\n",
    "# Averages & Trimmed Averages\n",
    "# ======================\n",
    "avg_summary = []\n",
    "for model in base_summary['Model'].unique():\n",
    "    df_m = base_summary[base_summary['Model'] == model]\n",
    "    accs = df_m['Test Accuracy'].tolist()\n",
    "    precs = df_m['Test Precision'].tolist()\n",
    "    recs = df_m['Test Recall'].tolist()\n",
    "    f1s = df_m['F1'].tolist()\n",
    "\n",
    "    avg_summary.append({\n",
    "        'Model': model,\n",
    "        'Trimmed': 'No',\n",
    "        'Test Accuracy': np.mean(accs),\n",
    "        'Test Precision': np.mean(precs),\n",
    "        'Test Recall': np.mean(recs),\n",
    "        'F1': np.mean(f1s),\n",
    "        'Num_Periods': len(accs),\n",
    "        'Std_Accuracy': np.std(accs),\n",
    "        'CV_Accuracy': np.std(accs) / np.mean(accs) if np.mean(accs) > 0 else 0,\n",
    "        'Accuracy_Range': max(accs) - min(accs) if accs else 0\n",
    "    })\n",
    "\n",
    "    if len(accs) > 2:\n",
    "        trimmed = sorted(accs)[1:-1]\n",
    "        avg_summary.append({\n",
    "            'Model': model,\n",
    "            'Trimmed': 'Yes',\n",
    "            'Test Accuracy': np.mean(trimmed),\n",
    "            'Test Precision': np.mean(precs),\n",
    "            'Test Recall': np.mean(recs),\n",
    "            'F1': np.mean(f1s),\n",
    "            'Num_Periods': len(trimmed),\n",
    "            'Std_Accuracy': np.std(accs),\n",
    "            'CV_Accuracy': np.std(accs) / np.mean(accs) if np.mean(accs) > 0 else 0,\n",
    "            'Accuracy_Range': max(accs) - min(accs) if accs else 0\n",
    "        })\n",
    "\n",
    "avg_summary = pd.DataFrame(avg_summary)\n",
    "print(\"\\n=== AVERAGE & TRIMMED BASELINE RESULTS ===\")\n",
    "print(avg_summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
