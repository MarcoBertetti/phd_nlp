{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tune a Bert model to identify specific topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizer\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import pandas as pd\n",
    "LABEL_SCHEMA = {\n",
    "    \"impact_type\": {\n",
    "        \"type\": \"single-label\",\n",
    "        \"classes\": [\n",
    "            \"death\", \"injury\", \"displacement\", \"missing\", \"disease\",\n",
    "            \"economic_loss\", \"conflict\", \"weather_shock\", \"policy_change\", \"none\"\n",
    "        ]\n",
    "    },\n",
    "    \"resource\": {\n",
    "        \"type\": \"multi-label\",\n",
    "        \"classes\": [\n",
    "            \"food\", \"water\", \"cash_aid\", \"healthcare\", \"shelter\",\n",
    "            \"livelihoods\", \"education\", \"infrastructure\", \"none\"\n",
    "        ]\n",
    "    },\n",
    "    \"urgency\": {\n",
    "        \"type\": \"single-label\",\n",
    "        \"classes\": [\n",
    "            \"low\", \"moderate\", \"high\", \"unclear\"\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Load the full labeled dataset\n",
    "df = pd.read_csv(\"./for_bert/labeled_chunk_all.csv\")\n",
    "\n",
    "# Parse resource column from string to list\n",
    "df[\"resource\"] = df[\"resource\"].apply(ast.literal_eval)\n",
    "\n",
    "# Binarize the resource field\n",
    "mlb = MultiLabelBinarizer()\n",
    "resource_binary = mlb.fit_transform(df[\"resource\"])\n",
    "resource_classes = mlb.classes_\n",
    "\n",
    "# Add one binary column per resource class\n",
    "for i, label in enumerate(resource_classes):\n",
    "    df[f\"resource_{label}\"] = resource_binary[:, i]\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "df[\"token_length\"] = df[\"clean_text\"].apply(lambda x: len(tokenizer.tokenize(str(x))))\n",
    "\n",
    "# Combine fields for stratification\n",
    "df[\"stratify_label\"] = df[\"impact_type\"] + \"_\" + df[\"urgency\"]\n",
    "\n",
    "# Split into train and validation sets\n",
    "train_df, val_df = train_test_split(\n",
    "    df,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=df[\"stratify_label\"]\n",
    ")\n",
    "\n",
    "# Final feature columns\n",
    "resource_cols = [f\"resource_{label}\" for label in resource_classes]\n",
    "final_columns = [\"clean_text\", \"impact_type\", \"urgency\"] + resource_cols\n",
    "\n",
    "train_df = train_df[final_columns].reset_index(drop=True)\n",
    "val_df = val_df[final_columns].reset_index(drop=True)\n",
    "\n",
    "# Token length stats\n",
    "print(\"Token length stats (train):\")\n",
    "print(train_df[\"clean_text\"].apply(lambda x: len(tokenizer.tokenize(str(x)))).describe())\n",
    "print(\"\\nTrain/Val shape:\", train_df.shape, val_df.shape)\n",
    "\n",
    "# -------------------------------\n",
    "# Optimized chunking (no decoding)\n",
    "# -------------------------------\n",
    "\n",
    "def chunk_input_ids(text, tokenizer, max_length=512, stride=256):\n",
    "    \"\"\"\n",
    "    Splits text into overlapping input_id chunks (no decoding).\n",
    "    \"\"\"\n",
    "    tokens = tokenizer.encode(text, add_special_tokens=False)\n",
    "    return [tokens[i:i+max_length] for i in range(0, len(tokens), stride) if tokens[i:i+max_length]]\n",
    "\n",
    "def expand_dataset_fast(df, text_col=\"clean_text\", label_cols=None):\n",
    "    \"\"\"\n",
    "    Applies chunking to every row and replicates labels per chunk.\n",
    "    Stores input_ids (not raw text).\n",
    "    \"\"\"\n",
    "    label_cols = label_cols or [\"impact_type\", \"urgency\"] + [c for c in df.columns if c.startswith(\"resource_\")]\n",
    "    records = []\n",
    "\n",
    "    for row in tqdm(df.itertuples(), total=len(df)):\n",
    "        chunks = chunk_input_ids(getattr(row, text_col), tokenizer)\n",
    "        for chunk in chunks:\n",
    "            record = { \"input_ids\": chunk }\n",
    "            for col in label_cols:\n",
    "                record[col] = getattr(row, col)\n",
    "            records.append(record)\n",
    "\n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "# Apply to both train and validation sets\n",
    "expanded_train_df = expand_dataset_fast(train_df)\n",
    "expanded_val_df = expand_dataset_fast(val_df)\n",
    "\n",
    "# Final shapes\n",
    "print(\"\\nExpanded train shape:\", expanded_train_df.shape)\n",
    "print(\"Expanded val shape:\", expanded_val_df.shape)\n",
    "\n",
    "# Optional: save to CSV or pickle if needed\n",
    "\n",
    "# Encode single-label classification targets\n",
    "impact_enc = LabelEncoder()\n",
    "urgency_enc = LabelEncoder()\n",
    "\n",
    "expanded_train_df[\"impact_type_id\"] = impact_enc.fit_transform(expanded_train_df[\"impact_type\"])\n",
    "expanded_val_df[\"impact_type_id\"] = impact_enc.transform(expanded_val_df[\"impact_type\"])\n",
    "\n",
    "expanded_train_df[\"urgency_id\"] = urgency_enc.fit_transform(expanded_train_df[\"urgency\"])\n",
    "expanded_val_df[\"urgency_id\"] = urgency_enc.transform(expanded_val_df[\"urgency\"])\n",
    "\n",
    "expanded_train_df.to_pickle(\"./for_bert/expanded_train.pkl\")\n",
    "expanded_val_df.to_pickle(\"./for_bert/expanded_val.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================ 0. Imports & Device ============================\n",
    "import os, torch, torch.nn as nn, pandas as pd, numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from torch.utils.data import Dataset as TorchDataset\n",
    "from transformers import (\n",
    "    BertModel, BertTokenizer, BertPreTrainedModel,\n",
    "    DataCollatorWithPadding, TrainingArguments, Trainer\n",
    ")\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# ============================ 1. Model =======================================\n",
    "class MultiTaskBERT(BertPreTrainedModel):\n",
    "    def __init__(self, config, num_impact, num_urgency, num_resource):\n",
    "        super().__init__(config)\n",
    "        self.bert = BertModel(config)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.impact_head   = nn.Linear(config.hidden_size, num_impact)\n",
    "        self.urgency_head  = nn.Linear(config.hidden_size, num_urgency)\n",
    "        self.resource_head = nn.Linear(config.hidden_size, num_resource)\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        impact_type_id=None,\n",
    "        urgency_id=None,\n",
    "        **kwargs      # each resource_* label arrives here\n",
    "    ):\n",
    "        pooled = self.dropout(\n",
    "            self.bert(input_ids=input_ids, attention_mask=attention_mask).pooler_output\n",
    "        )\n",
    "        impact_logits   = self.impact_head(pooled)\n",
    "        urgency_logits  = self.urgency_head(pooled)\n",
    "        resource_logits = self.resource_head(pooled)\n",
    "\n",
    "        loss = None\n",
    "        if impact_type_id is not None and urgency_id is not None:\n",
    "            # stack resource labels in consistent column order\n",
    "            resource_labels = torch.stack(\n",
    "                [kwargs[col] for col in sorted(k for k in kwargs if k.startswith(\"resource_\"))],\n",
    "                dim=1\n",
    "            ).float()\n",
    "            loss = (\n",
    "                nn.CrossEntropyLoss()(impact_logits, impact_type_id) +\n",
    "                nn.CrossEntropyLoss()(urgency_logits, urgency_id)   +\n",
    "                0.5 * nn.BCEWithLogitsLoss()(resource_logits, resource_labels)\n",
    "            )\n",
    "\n",
    "        return SequenceClassifierOutput(\n",
    "            loss   = loss,\n",
    "            logits = (impact_logits, urgency_logits, resource_logits),\n",
    "        )\n",
    "\n",
    "# ============================ 2. Data Loading ================================\n",
    "train_path = \"/content/drive/MyDrive/phd/expanded_train.pkl\"\n",
    "val_path   = \"/content/drive/MyDrive/phd/expanded_val.pkl\"\n",
    "\n",
    "df_train = pd.read_pickle(train_path).reset_index(drop=True)\n",
    "df_val   = pd.read_pickle(val_path).reset_index(drop=True)\n",
    "\n",
    "impact_enc  = LabelEncoder()\n",
    "urgency_enc = LabelEncoder()\n",
    "df_train[\"impact_type_id\"] = impact_enc.fit_transform(df_train[\"impact_type\"])\n",
    "df_val[\"impact_type_id\"]   = impact_enc.transform(df_val[\"impact_type\"])\n",
    "df_train[\"urgency_id\"] = urgency_enc.fit_transform(df_train[\"urgency\"])\n",
    "df_val[\"urgency_id\"]   = urgency_enc.transform(df_val[\"urgency\"])\n",
    "\n",
    "resource_cols = [c for c in df_train.columns if c.startswith(\"resource_\")]\n",
    "\n",
    "class CustomTorchDataset(TorchDataset):\n",
    "    def __init__(self, df, resource_cols):\n",
    "        self.input_ids = list(df[\"input_ids\"])\n",
    "        self.impact_type_id = list(df[\"impact_type_id\"])\n",
    "        self.urgency_id = list(df[\"urgency_id\"])\n",
    "        self.resource_matrix = df[resource_cols].values.astype(\"float32\")\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        example = {\n",
    "            \"input_ids\": torch.tensor(self.input_ids[idx]),\n",
    "            \"impact_type_id\": torch.tensor(self.impact_type_id[idx]),\n",
    "            \"urgency_id\": torch.tensor(self.urgency_id[idx]),\n",
    "        }\n",
    "        # add each resource_* label separately\n",
    "        for i, col in enumerate(resource_cols):\n",
    "            example[col] = torch.tensor(self.resource_matrix[idx, i])\n",
    "        return example\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "train_ds = CustomTorchDataset(df_train, resource_cols)\n",
    "val_ds   = CustomTorchDataset(df_val,   resource_cols)\n",
    "\n",
    "# ============================ 3. Tokenizer & Collator ========================\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "collator  = DataCollatorWithPadding(tokenizer, pad_to_multiple_of=8, return_tensors=\"pt\")\n",
    "\n",
    "# ============================ 4. Build / Compile Model =======================\n",
    "model = MultiTaskBERT.from_pretrained(\n",
    "    \"bert-base-uncased\",\n",
    "    num_impact=len(impact_enc.classes_),\n",
    "    num_urgency=len(urgency_enc.classes_),\n",
    "    num_resource=len(resource_cols)\n",
    ").to(device)\n",
    "model = torch.compile(model)\n",
    "\n",
    "# ============================ 5. Training Args ===============================\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"/content/drive/MyDrive/phd/bert_multitask_model\",\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=5,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=5,\n",
    "    logging_dir=\"/content/drive/MyDrive/phd/logs\",\n",
    "    report_to=\"none\",\n",
    "    remove_unused_columns=False,\n",
    "    fp16=True,\n",
    "    label_names=[\"impact_type_id\", \"urgency_id\"] + resource_cols  # â† add this line\n",
    "\n",
    ")\n",
    "\n",
    "# ============================ 6. Metrics =====================================\n",
    "LABEL_ORDER = [\"impact_type_id\", \"urgency_id\"] + resource_cols   # same order!\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    # label_ids is a tuple in that order\n",
    "    impact_labels, urgency_labels, *resource_cols_labels = pred.label_ids\n",
    "\n",
    "    impact_logits, urgency_logits, resource_logits = pred.predictions\n",
    "\n",
    "    impact_preds  = impact_logits.argmax(axis=1)\n",
    "    urgency_preds = urgency_logits.argmax(axis=1)\n",
    "\n",
    "    # stack resource labels / preds\n",
    "    resource_labels = np.stack(resource_cols_labels, axis=1)\n",
    "    resource_preds  = (torch.sigmoid(torch.tensor(resource_logits)) > 0.5).int().numpy()\n",
    "\n",
    "    return dict(\n",
    "        impact_acc       = accuracy_score(impact_labels, impact_preds),\n",
    "        urgency_acc      = accuracy_score(urgency_labels, urgency_preds),\n",
    "        resource_f1_micro= f1_score(resource_labels, resource_preds, average=\"micro\"),\n",
    "        resource_f1_macro= f1_score(resource_labels, resource_preds, average=\"macro\"),\n",
    "    )\n",
    "\n",
    "# ============================ 7. Trainer =====================================\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# ---------- Resume if checkpoint exists ----------\n",
    "ckpt_dir   = args.output_dir\n",
    "ckpts = [os.path.join(ckpt_dir, d) for d in os.listdir(ckpt_dir) if d.startswith(\"checkpoint-\")]\n",
    "if ckpts:\n",
    "    latest = max(ckpts, key=lambda p: int(p.split(\"-\")[-1]))\n",
    "    print(\"Resuming from:\", latest)\n",
    "    trainer.train(resume_from_checkpoint=latest)\n",
    "else:\n",
    "    trainer.train()\n",
    "\n",
    "# ============================ 8. Save Final Model ============================\n",
    "final_path = \"/content/drive/MyDrive/phd/bert_multitask_model/final\"\n",
    "trainer.save_model(final_path)\n",
    "tokenizer.save_pretrained(final_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
