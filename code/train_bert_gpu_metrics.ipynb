{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tune a Bert model to identify specific topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torch.nn as nn\n",
    "from transformers import (\n",
    "    BertModel, BertTokenizer, BertPreTrainedModel,\n",
    "    DataCollatorWithPadding, TrainingArguments, Trainer\n",
    ")\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from datasets import Dataset\n",
    "import pandas as pd, numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    logits = pred.predictions\n",
    "    labels = pred.label_ids\n",
    "\n",
    "    # Split predictions\n",
    "    impact_logits, urgency_logits, resource_logits = logits\n",
    "    impact_labels = labels[\"impact_type_id\"]\n",
    "    urgency_labels = labels[\"urgency_id\"]\n",
    "    resource_labels = torch.stack([labels[col] for col in resource_cols], dim=1).numpy()\n",
    "\n",
    "    impact_preds = impact_logits.argmax(axis=1)\n",
    "    urgency_preds = urgency_logits.argmax(axis=1)\n",
    "    resource_preds = (torch.sigmoid(torch.tensor(resource_logits)) > 0.5).int().numpy()\n",
    "\n",
    "    return {\n",
    "        \"impact_acc\": accuracy_score(impact_labels, impact_preds),\n",
    "        \"urgency_acc\": accuracy_score(urgency_labels, urgency_preds),\n",
    "        \"resource_f1_micro\": f1_score(resource_labels, resource_preds, average=\"micro\"),\n",
    "        \"resource_f1_macro\": f1_score(resource_labels, resource_preds, average=\"macro\"),\n",
    "    }\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "# 0.  DEVICE\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "\n",
    "device = torch.device(\"mps\")  # Apple‑silicon GPU\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "# 1.  MULTI‑TASK MODEL\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "\n",
    "class MultiTaskBERT(BertPreTrainedModel):\n",
    "    \"\"\"BERT backbone + 3 classification heads (impact, urgency, resources).\"\"\"\n",
    "\n",
    "    def __init__(self, config, num_impact: int, num_urgency: int, num_resource: int):\n",
    "        super().__init__(config)\n",
    "        self.bert = BertModel(config)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.impact_head = nn.Linear(config.hidden_size, num_impact)\n",
    "        self.urgency_head = nn.Linear(config.hidden_size, num_urgency)\n",
    "        self.resource_head = nn.Linear(config.hidden_size, num_resource)\n",
    "        self.init_weights()\n",
    "\n",
    "    # **kwargs swallows any extra keys Trainer might pass (impact_type_id …)\n",
    "    def forward(self, input_ids=None, attention_mask=None, **kwargs):\n",
    "        pooled = self.dropout(self.bert(input_ids, attention_mask).pooler_output)\n",
    "        return {\n",
    "            \"impact\":   self.impact_head(pooled),\n",
    "            \"urgency\":  self.urgency_head(pooled),\n",
    "            \"resource\": self.resource_head(pooled),\n",
    "        }\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "# 2.  LOAD DATA & ENCODE LABELS\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "\n",
    "df_train = pd.read_pickle(\"./for_bert/expanded_train.pkl\")\n",
    "df_val   = pd.read_pickle(\"./for_bert/expanded_val.pkl\")\n",
    "\n",
    "impact_enc  = LabelEncoder()\n",
    "urgency_enc = LabelEncoder()\n",
    "\n",
    "df_train[\"impact_type_id\"] = impact_enc.fit_transform(df_train[\"impact_type\"])\n",
    "df_val[\"impact_type_id\"]   = impact_enc.transform(df_val[\"impact_type\"])\n",
    "\n",
    "df_train[\"urgency_id\"] = urgency_enc.fit_transform(df_train[\"urgency\"])\n",
    "df_val[\"urgency_id\"]   = urgency_enc.transform(df_val[\"urgency\"])\n",
    "\n",
    "resource_cols = [c for c in df_train.columns if c.startswith(\"resource_\")]\n",
    "\n",
    "cols = [\"input_ids\", \"impact_type_id\", \"urgency_id\"] + resource_cols\n",
    "\n",
    "# build HF datasets\n",
    "train_ds = Dataset.from_pandas(df_train[cols]).with_format(\"torch\", columns=cols)\n",
    "val_ds   = Dataset.from_pandas(df_val[cols]).with_format(\"torch\", columns=cols)\n",
    "\n",
    "print(train_ds.column_names)\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "# 3.  TOKENIZER & COLLATOR (dynamic padding)\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "collator  = DataCollatorWithPadding(tokenizer, pad_to_multiple_of=8, return_tensors=\"pt\")\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "# 4.  BUILD MODEL (frozen backbone for quick test)\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "\n",
    "model = MultiTaskBERT.from_pretrained(\n",
    "    \"bert-base-uncased\",\n",
    "    num_impact=len(impact_enc.classes_),\n",
    "    num_urgency=len(urgency_enc.classes_),\n",
    "    num_resource=len(resource_cols),\n",
    ").to(device)\n",
    "\n",
    "#for p in model.bert.parameters():  # comment out to fine‑tune full model\n",
    "#    p.requires_grad = False\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "# 5.  CUSTOM TRAINER\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "\n",
    "class MultiTaskTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        # move tensors to correct device\n",
    "        inputs = {k: (v.to(device) if isinstance(v, torch.Tensor) else v) for k, v in inputs.items()}\n",
    "\n",
    "        y_impact   = inputs.pop(\"impact_type_id\")\n",
    "        y_urgency  = inputs.pop(\"urgency_id\")\n",
    "        y_resource = torch.stack([inputs.pop(col) for col in resource_cols], dim=1).float()\n",
    "\n",
    "        outputs = model(\n",
    "            input_ids=inputs[\"input_ids\"],\n",
    "            attention_mask=inputs[\"attention_mask\"],\n",
    "        )\n",
    "\n",
    "        # Weighted loss components\n",
    "        loss_impact   = nn.CrossEntropyLoss()(outputs[\"impact\"], y_impact)\n",
    "        loss_urgency  = nn.CrossEntropyLoss()(outputs[\"urgency\"], y_urgency)\n",
    "        loss_resource = nn.BCEWithLogitsLoss()(outputs[\"resource\"], y_resource)\n",
    "\n",
    "        # You can adjust these weights based on validation later\n",
    "        total_loss = 1.0 * loss_impact + 1.0 * loss_urgency + 0.5 * loss_resource\n",
    "\n",
    "        return (total_loss, (outputs[\"impact\"].detach().cpu().numpy(),\n",
    "                     outputs[\"urgency\"].detach().cpu().numpy(),\n",
    "                     outputs[\"resource\"].detach().cpu().numpy())) if return_outputs else total_loss\n",
    "\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "# 6.  TRAINING ARGS\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"./bert_multitask_model\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=5,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs\",\n",
    "    report_to=\"none\",\n",
    "    remove_unused_columns=False,   #<- keep label columns\n",
    ")\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "# 7.  TRAIN\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "\n",
    "trainer = MultiTaskTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "# 8.  SAVE MODEL\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "\n",
    "trainer.save_model(\"./for_bert/bert_multitask_model/final\")\n",
    "tokenizer.save_pretrained(\"./for_bert/bert_multitask_model/final\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install scikit-learn\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
