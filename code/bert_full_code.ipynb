{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JdAcdgUPveYd",
    "outputId": "2ac481fc-5aff-4119-b371-47d749dbc5f4"
   },
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Version checks\n",
    "import sys\n",
    "import torch\n",
    "import numpy\n",
    "import pandas as pd\n",
    "import transformers\n",
    "import sklearn\n",
    "import datasets\n",
    "\n",
    "print(f\"Python         : {sys.version}\")\n",
    "print(f\"PyTorch        : {torch.__version__}\")\n",
    "print(f\"Numpy          : {numpy.__version__}\")\n",
    "print(f\"Pandas         : {pd.__version__}\")\n",
    "print(f\"Transformers   : {transformers.__version__}\")\n",
    "print(f\"Scikit-learn   : {sklearn.__version__}\")\n",
    "print(f\"Datasets (HF)  : {datasets.__version__}\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device   :\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m9gq6V3IIYzc",
    "outputId": "c9224645-34ad-4a92-a80d-13dcd92189db"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizer\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import pandas as pd\n",
    "LABEL_SCHEMA = {\n",
    "    \"impact_type\": {\n",
    "        \"type\": \"single-label\",\n",
    "        \"classes\": [\n",
    "            \"death\", \"injury\", \"displacement\", \"missing\", \"disease\",\n",
    "            \"economic_loss\", \"conflict\", \"weather_shock\", \"policy_change\", \"none\"\n",
    "        ]\n",
    "    },\n",
    "    \"resource\": {\n",
    "        \"type\": \"multi-label\",\n",
    "        \"classes\": [\n",
    "            \"food\", \"water\", \"cash_aid\", \"healthcare\", \"shelter\",\n",
    "            \"livelihoods\", \"education\", \"infrastructure\", \"none\"\n",
    "        ]\n",
    "    },\n",
    "    \"urgency\": {\n",
    "        \"type\": \"single-label\",\n",
    "        \"classes\": [\n",
    "            \"low\", \"moderate\", \"high\", \"unclear\"\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Load the full labeled dataset\n",
    "df = pd.read_csv(\"/content/drive/MyDrive/phd/labeled_chunk_all.csv\")\n",
    "\n",
    "# Parse resource column from string to list\n",
    "df[\"resource\"] = df[\"resource\"].apply(ast.literal_eval)\n",
    "\n",
    "# Binarize the resource field\n",
    "mlb = MultiLabelBinarizer()\n",
    "resource_binary = mlb.fit_transform(df[\"resource\"])\n",
    "resource_classes = mlb.classes_\n",
    "\n",
    "# Add one binary column per resource class\n",
    "for i, label in enumerate(resource_classes):\n",
    "    df[f\"resource_{label}\"] = resource_binary[:, i]\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "df[\"token_length\"] = df[\"clean_text\"].apply(lambda x: len(tokenizer.tokenize(str(x))))\n",
    "\n",
    "# Combine fields for stratification\n",
    "df[\"stratify_label\"] = df[\"impact_type\"] + \"_\" + df[\"urgency\"]\n",
    "\n",
    "# Split into train and validation sets\n",
    "train_df, val_df = train_test_split(\n",
    "    df,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=df[\"stratify_label\"]\n",
    ")\n",
    "\n",
    "# Final feature columns\n",
    "resource_cols = [f\"resource_{label}\" for label in resource_classes]\n",
    "final_columns = [\"clean_text\", \"impact_type\", \"urgency\"] + resource_cols\n",
    "\n",
    "train_df = train_df[final_columns].reset_index(drop=True)\n",
    "val_df = val_df[final_columns].reset_index(drop=True)\n",
    "\n",
    "# Token length stats\n",
    "print(\"Token length stats (train):\")\n",
    "print(train_df[\"clean_text\"].apply(lambda x: len(tokenizer.tokenize(str(x)))).describe())\n",
    "print(\"\\nTrain/Val shape:\", train_df.shape, val_df.shape)\n",
    "\n",
    "# -------------------------------\n",
    "# Optimized chunking (no decoding)\n",
    "# -------------------------------\n",
    "\n",
    "def chunk_input_ids(text, tokenizer, max_length=512, stride=256):\n",
    "    \"\"\"\n",
    "    Splits text into overlapping input_id chunks (no decoding).\n",
    "    \"\"\"\n",
    "    tokens = tokenizer.encode(text, add_special_tokens=False)\n",
    "    return [tokens[i:i+max_length] for i in range(0, len(tokens), stride) if tokens[i:i+max_length]]\n",
    "\n",
    "def expand_dataset_fast(df, text_col=\"clean_text\", label_cols=None):\n",
    "    \"\"\"\n",
    "    Applies chunking to every row and replicates labels per chunk.\n",
    "    Stores input_ids (not raw text).\n",
    "    \"\"\"\n",
    "    label_cols = label_cols or [\"impact_type\", \"urgency\"] + [c for c in df.columns if c.startswith(\"resource_\")]\n",
    "    records = []\n",
    "\n",
    "    for row in tqdm(df.itertuples(), total=len(df)):\n",
    "        chunks = chunk_input_ids(getattr(row, text_col), tokenizer)\n",
    "        for chunk in chunks:\n",
    "            record = { \"input_ids\": chunk }\n",
    "            for col in label_cols:\n",
    "                record[col] = getattr(row, col)\n",
    "            records.append(record)\n",
    "\n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "# Apply to both train and validation sets\n",
    "expanded_train_df = expand_dataset_fast(train_df)\n",
    "expanded_val_df = expand_dataset_fast(val_df)\n",
    "\n",
    "# Final shapes\n",
    "print(\"\\nExpanded train shape:\", expanded_train_df.shape)\n",
    "print(\"Expanded val shape:\", expanded_val_df.shape)\n",
    "\n",
    "# Optional: save to CSV or pickle if needed\n",
    "\n",
    "# Encode single-label classification targets\n",
    "impact_enc = LabelEncoder()\n",
    "urgency_enc = LabelEncoder()\n",
    "\n",
    "expanded_train_df[\"impact_type_id\"] = impact_enc.fit_transform(expanded_train_df[\"impact_type\"])\n",
    "expanded_val_df[\"impact_type_id\"] = impact_enc.transform(expanded_val_df[\"impact_type\"])\n",
    "\n",
    "expanded_train_df[\"urgency_id\"] = urgency_enc.fit_transform(expanded_train_df[\"urgency\"])\n",
    "expanded_val_df[\"urgency_id\"] = urgency_enc.transform(expanded_val_df[\"urgency\"])\n",
    "\n",
    "expanded_train_df.to_pickle(\"/content/drive/MyDrive/phd/expanded_train.pkl\")\n",
    "expanded_val_df.to_pickle(\"/content/drive/MyDrive/phd/expanded_val.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "trHIpMKQvHAK",
    "outputId": "f8075987-558e-4e5a-8013-5cf879770992"
   },
   "outputs": [],
   "source": [
    "import torch, torch.nn as nn\n",
    "import pandas as pd, numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    BertModel, BertTokenizer, BertPreTrainedModel,\n",
    "    DataCollatorWithPadding, TrainingArguments, Trainer\n",
    ")\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "# 0. DEVICE\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "# 1. MULTI-TASK MODEL\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "class MultiTaskBERT(BertPreTrainedModel):\n",
    "    def __init__(self, config, num_impact, num_urgency, num_resource):\n",
    "        super().__init__(config)\n",
    "        self.bert = BertModel(config)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.impact_head = nn.Linear(config.hidden_size, num_impact)\n",
    "        self.urgency_head = nn.Linear(config.hidden_size, num_urgency)\n",
    "        self.resource_head = nn.Linear(config.hidden_size, num_resource)\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None, **kwargs):\n",
    "        pooled = self.dropout(self.bert(input_ids=input_ids, attention_mask=attention_mask).pooler_output)\n",
    "        return {\n",
    "            \"impact\": self.impact_head(pooled),\n",
    "            \"urgency\": self.urgency_head(pooled),\n",
    "            \"resource\": self.resource_head(pooled)\n",
    "        }\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "# 2.  LOAD DATA & ENCODE LABELS (for PyTorch Dataset)\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Load data\n",
    "df_train = pd.read_pickle(\"/content/drive/MyDrive/phd/expanded_train.pkl\")\n",
    "df_val   = pd.read_pickle(\"/content/drive/MyDrive/phd/expanded_val.pkl\")\n",
    "\n",
    "# Encode impact and urgency labels\n",
    "impact_enc  = LabelEncoder()\n",
    "urgency_enc = LabelEncoder()\n",
    "\n",
    "df_train[\"impact_type_id\"] = impact_enc.fit_transform(df_train[\"impact_type\"])\n",
    "df_val[\"impact_type_id\"]   = impact_enc.transform(df_val[\"impact_type\"])\n",
    "\n",
    "df_train[\"urgency_id\"] = urgency_enc.fit_transform(df_train[\"urgency\"])\n",
    "df_val[\"urgency_id\"]   = urgency_enc.transform(df_val[\"urgency\"])\n",
    "print(\"val_ds length:\", len(df_val))\n",
    "\n",
    "# Identify resource columns\n",
    "resource_cols = [c for c in df_train.columns if c.startswith(\"resource_\")]\n",
    "\n",
    "# Define custom PyTorch dataset\n",
    "from torch.utils.data import Dataset as TorchDataset\n",
    "import torch\n",
    "\n",
    "class CustomTorchDataset(TorchDataset):\n",
    "    def __init__(self, df, resource_cols):\n",
    "        self.input_ids = list(df[\"input_ids\"])\n",
    "        self.impact_type_id = list(df[\"impact_type_id\"])\n",
    "        self.urgency_id = list(df[\"urgency_id\"])\n",
    "        self.resource_matrix = df[resource_cols].values.astype(\"float32\")\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {\n",
    "            \"input_ids\": self.input_ids[idx],  # list of ints, not tensor!\n",
    "            \"impact_type_id\": self.impact_type_id[idx],\n",
    "            \"urgency_id\": self.urgency_id[idx],\n",
    "        }\n",
    "        for i, col in enumerate(resource_cols):\n",
    "            item[col] = self.resource_matrix[idx, i]\n",
    "        return item\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "\n",
    "# Instantiate datasets\n",
    "train_ds = CustomTorchDataset(df_train, resource_cols)\n",
    "val_ds   = CustomTorchDataset(df_val, resource_cols)\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "# 3. TOKENIZER & COLLATOR\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "collator  = DataCollatorWithPadding(tokenizer, pad_to_multiple_of=8, return_tensors=\"pt\")\n",
    "\n",
    "# (Optional) Debug block: use collator after it's defined!\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "print(\"val_ds length:\", len(val_ds))\n",
    "batch = next(iter(DataLoader(val_ds, batch_size=2, collate_fn=collator)))\n",
    "print(\"Sample batch keys:\", list(batch.keys()))\n",
    "for k in batch:\n",
    "    print(f\"{k}: {batch[k].shape}, dtype={batch[k].dtype}\")\n",
    "for k in batch:\n",
    "    print(f\"{k} (first 2 values): {batch[k][:2]}\")\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "# 4. INIT MODEL\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "model = MultiTaskBERT.from_pretrained(\n",
    "    \"bert-base-uncased\",\n",
    "    num_impact=len(impact_enc.classes_),\n",
    "    num_urgency=len(urgency_enc.classes_),\n",
    "    num_resource=len(resource_cols)\n",
    ").to(device)\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "# 5. CUSTOM TRAINER\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "class MultiTaskTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "        # Always copy to avoid mutating Trainer's batch\n",
    "        inputs = inputs.copy()\n",
    "        # Move everything to device\n",
    "        inputs = {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in inputs.items()}\n",
    "\n",
    "        # Pop the targets from inputs\n",
    "        y_impact   = inputs.pop(\"impact_type_id\")\n",
    "        y_urgency  = inputs.pop(\"urgency_id\")\n",
    "        y_resource = torch.stack([inputs.pop(c) for c in resource_cols], dim=1).float()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"])\n",
    "\n",
    "        # Compute losses\n",
    "        loss_impact   = nn.CrossEntropyLoss()(outputs[\"impact\"], y_impact)\n",
    "        loss_urgency  = nn.CrossEntropyLoss()(outputs[\"urgency\"], y_urgency)\n",
    "        loss_resource = nn.BCEWithLogitsLoss()(outputs[\"resource\"], y_resource)\n",
    "\n",
    "        # Weighted sum of losses (tune these as needed)\n",
    "        total_loss = 1.0 * loss_impact + 1.0 * loss_urgency + 0.5 * loss_resource\n",
    "\n",
    "        # Hugging Face expects just the loss for eval, (loss, outputs) for train\n",
    "        if return_outputs:\n",
    "            return total_loss, outputs\n",
    "        return total_loss\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "# 6. TRAINING ARGS\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "\n",
    "# Identify all label columns\n",
    "all_label_cols = [\"impact_type_id\", \"urgency_id\"] + resource_cols\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"/content/drive/MyDrive/phd/bert_multitask_model\",\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=10,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir=\"/content/drive/MyDrive/phd/logs\",\n",
    "    logging_steps=10,\n",
    "    # ----- FIX HERE -----\n",
    "    load_best_model_at_end=True,      # keep if you want best-model logic\n",
    "    metric_for_best_model=\"eval_impact_acc\",# pick any key you return in compute_metrics\n",
    "    greater_is_better=True,           # set False if you choose a loss\n",
    "    # ---------------------\n",
    "    report_to=\"none\",\n",
    "    remove_unused_columns=True, # Keep True\n",
    "    fp16=True,\n",
    "    label_names=all_label_cols # Add label_names here\n",
    ")\n",
    "\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "# 7. METRICS + TRAIN\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "def compute_metrics(pred):\n",
    "    # unpack predictions\n",
    "    impact_logits, urgency_logits, resource_logits = pred.predictions\n",
    "\n",
    "    # pred.label_ids is a tuple when remove_unused_columns=True and label_names is used\n",
    "    # Unpack the tuple into separate label arrays\n",
    "    impact_labels, urgency_labels = pred.label_ids[:2]\n",
    "    resource_labels = pred.label_ids[2:]\n",
    "\n",
    "    # Convert resource_labels to a numpy array if it's a tuple of arrays\n",
    "    if isinstance(resource_labels, tuple):\n",
    "        resource_labels = np.stack(resource_labels, axis=1)\n",
    "\n",
    "\n",
    "    # derive predictions\n",
    "    impact_preds   = impact_logits.argmax(axis=1)\n",
    "    urgency_preds  = urgency_logits.argmax(axis=1)\n",
    "    resource_preds = (torch.sigmoid(torch.tensor(resource_logits)) > 0.5).int().numpy()\n",
    "\n",
    "    # compute metrics\n",
    "    metrics = {\n",
    "        \"eval_impact_acc\":  accuracy_score(impact_labels, impact_preds),\n",
    "        \"eval_urgency_acc\": accuracy_score(urgency_labels, urgency_preds),\n",
    "        \"eval_resource_f1_micro\": f1_score(resource_labels, resource_preds, average=\"micro\"),\n",
    "        \"eval_resource_f1_macro\": f1_score(resource_labels, resource_preds, average=\"macro\"),\n",
    "    }\n",
    "    return metrics\n",
    "\n",
    "\n",
    "trainer = MultiTaskTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# Resume if checkpoint exists\n",
    "import os\n",
    "\n",
    "checkpoint_dir = \"/content/drive/MyDrive/phd/bert_multitask_model\"\n",
    "checkpoints = [os.path.join(checkpoint_dir, d) for d in os.listdir(checkpoint_dir) if d.startswith(\"checkpoint-\")]\n",
    "\n",
    "if checkpoints:\n",
    "    latest_checkpoint = max(checkpoints, key=lambda x: int(x.split(\"-\")[-1]))\n",
    "    print(f\"Resuming from: {latest_checkpoint}\")\n",
    "    trainer.train(resume_from_checkpoint=latest_checkpoint)\n",
    "else:\n",
    "    trainer.train()\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "# 8. SAVE FINAL MODEL\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "final_path = \"/content/drive/MyDrive/phd/bert_multitask_model/final\"\n",
    "trainer.save_model(final_path)\n",
    "tokenizer.save_pretrained(final_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "AJdrrMd1P7nm",
    "outputId": "894c5b36-8fbc-45c0-f66b-8d68760f9452"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import BertTokenizer, BertPreTrainedModel, BertModel\n",
    "import transformers\n",
    "import torch.nn as nn\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import mode\n",
    "import os\n",
    "import glob\n",
    "import fastparquet\n",
    "\n",
    "# ────────────────\n",
    "# CONFIGURATION\n",
    "# ────────────────\n",
    "MODEL_DIR = \"/content/drive/MyDrive/phd/bert_multitask_model/final\"\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "MAX_LENGTH = 512\n",
    "STRIDE = 256\n",
    "\n",
    "# These must exactly match training\n",
    "impact_classes = [\n",
    "    \"death\", \"injury\", \"displacement\", \"missing\", \"disease\",\n",
    "    \"economic_loss\", \"conflict\", \"weather_shock\", \"policy_change\", \"none\"\n",
    "]\n",
    "urgency_classes = [\"low\", \"moderate\", \"high\", \"unclear\"]\n",
    "resource_cols = [\n",
    "    \"resource_food\", \"resource_water\", \"resource_cash_aid\", \"resource_healthcare\", \"resource_shelter\",\n",
    "    \"resource_livelihoods\", \"resource_education\", \"resource_infrastructure\", \"resource_none\"\n",
    "]\n",
    "\n",
    "# ────────────────\n",
    "# LABEL ENCODERS\n",
    "# ────────────────\n",
    "impact_enc = LabelEncoder()\n",
    "impact_enc.fit(impact_classes)\n",
    "\n",
    "urgency_enc = LabelEncoder()\n",
    "urgency_enc.fit(urgency_classes)\n",
    "\n",
    "\n",
    "# ────────────────\n",
    "# MODEL DEFINITION\n",
    "# ────────────────\n",
    "class MultiTaskBERT(BertPreTrainedModel):\n",
    "    def __init__(self, config, num_impact, num_urgency, num_resource):\n",
    "        super().__init__(config)\n",
    "        self.bert = BertModel(config)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.impact_head = nn.Linear(config.hidden_size, num_impact)\n",
    "        self.urgency_head = nn.Linear(config.hidden_size, num_urgency)\n",
    "        self.resource_head = nn.Linear(config.hidden_size, num_resource)\n",
    "        self.init_weights()\n",
    "    def forward(self, input_ids=None, attention_mask=None, **kwargs):\n",
    "        pooled = self.dropout(self.bert(input_ids=input_ids, attention_mask=attention_mask).pooler_output)\n",
    "        return {\n",
    "            \"impact\": self.impact_head(pooled),\n",
    "            \"urgency\": self.urgency_head(pooled),\n",
    "            \"resource\": self.resource_head(pooled)\n",
    "        }\n",
    "\n",
    "# ────────────────\n",
    "# LOAD MODEL & TOKENIZER\n",
    "# ────────────────\n",
    "tokenizer = BertTokenizer.from_pretrained(MODEL_DIR, local_files_only=True)\n",
    "model = MultiTaskBERT.from_pretrained(\n",
    "    MODEL_DIR,\n",
    "    num_impact=len(impact_classes),\n",
    "    num_urgency=len(urgency_classes),\n",
    "    num_resource=len(resource_cols),\n",
    "    local_files_only=True # Add local_files_only=True here as well\n",
    ")\n",
    "model.to(DEVICE)\n",
    "model.eval()\n",
    "\n",
    "# ────────────────\n",
    "# CHUNKING FUNCTION (from your training)\n",
    "# ────────────────\n",
    "transformers.logging.set_verbosity_error()  # At the top to silence all warnings\n",
    "def chunk_input_ids(text, tokenizer, max_length=512, stride=256):\n",
    "    # Get raw tokens (no special tokens)\n",
    "    tokens = tokenizer.encode(text, add_special_tokens=False)\n",
    "    max_len_no_special = max_length - 2\n",
    "    chunks = []\n",
    "    for i in range(0, len(tokens), stride):\n",
    "        chunk = tokens[i:i + max_len_no_special]\n",
    "        # Now, turn chunk back into string and re-encode with special tokens, pad/truncate to max_length\n",
    "        chunk_text = tokenizer.decode(chunk)\n",
    "        inputs = tokenizer(\n",
    "            chunk_text,\n",
    "            max_length=max_length,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        chunks.append(inputs)\n",
    "    return chunks\n",
    "\n",
    "def predict_for_text(text):\n",
    "    chunks = chunk_input_ids(str(text), tokenizer, max_length=MAX_LENGTH, stride=STRIDE)\n",
    "    if not chunks:\n",
    "        # fallback: [UNK] padded to 512\n",
    "        inputs = tokenizer(\n",
    "            tokenizer.unk_token,\n",
    "            max_length=MAX_LENGTH,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        chunks = [inputs]\n",
    "    input_ids_tensor = torch.cat([chunk['input_ids'] for chunk in chunks]).to(DEVICE)\n",
    "    attention_mask_tensor = torch.cat([chunk['attention_mask'] for chunk in chunks]).to(DEVICE)\n",
    "\n",
    "    # --- DEBUG & CATCH ---\n",
    "    assert input_ids_tensor.shape[1] <= 512, f\"Found input_ids length {input_ids_tensor.shape[1]}\"\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids_tensor, attention_mask=attention_mask_tensor)\n",
    "    # ... (rest of your code unchanged)\n",
    "    impact_preds = outputs[\"impact\"].argmax(dim=1).cpu().numpy()\n",
    "    urgency_preds = outputs[\"urgency\"].argmax(dim=1).cpu().numpy()\n",
    "    resource_probs = torch.sigmoid(outputs[\"resource\"]).cpu().numpy()\n",
    "    resource_preds = (resource_probs > 0.5).astype(int)\n",
    "\n",
    "    impact_final = mode(impact_preds, keepdims=True)[0][0]\n",
    "    urgency_final = mode(urgency_preds, keepdims=True)[0][0]\n",
    "    resource_final = (resource_preds.sum(axis=0) > 0).astype(int)\n",
    "\n",
    "    impact_label = impact_enc.inverse_transform([impact_final])[0]\n",
    "    urgency_label = urgency_enc.inverse_transform([urgency_final])[0]\n",
    "    resource_dict = {col: int(val) for col, val in zip(resource_cols, resource_final)}\n",
    "    return impact_label, urgency_label, resource_dict\n",
    "\n",
    "# ────────────────\n",
    "# APPLY TO DATAFRAME\n",
    "# ────────────────\n",
    "\n",
    "# Example: df should have a column 'clean_text'\n",
    "tqdm.pandas()  # progress bar\n",
    "\n",
    "# 1) Read data\n",
    "parquet_files = glob.glob(os.path.join(\"/content/drive/MyDrive/phd/data/exploded_partial_chunk_*.parquet\"))\n",
    "print(parquet_files)\n",
    "# Read and concatenate all parquet files\n",
    "exploded_df = pd.concat([fastparquet.ParquetFile(file).to_pandas() for file in parquet_files], ignore_index=True)\n",
    "exploded_df = exploded_df.reset_index(drop=True)\n",
    "\n",
    "results = exploded_df['clean_text'].progress_apply(predict_for_text)\n",
    "\n",
    "# Add predictions to dataframe\n",
    "exploded_df['pred_impact_type'] = results.apply(lambda x: x[0])\n",
    "exploded_df['pred_urgency'] = results.apply(lambda x: x[1])\n",
    "for i, col in enumerate(resource_cols):\n",
    "    exploded_df[f'pred_{col}'] = results.apply(lambda x: x[2][col])\n",
    "\n",
    "exploded_df.to_parquet(\"/content/drive/MyDrive/phd/data/exploded_full.parquet\")\n",
    "exploded_df.head(20)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
