{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO \n",
    "- Train a specific BERT before using it\n",
    "- Add a step to use an LLM (probably LLnan3 locally) to add features, labels etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "from helper_functions.topic_modelling.flatten_articles import flatten_articles\n",
    "from helper_functions.topic_modelling.sentiment_analysis import perform_sentiment_analysis\n",
    "from helper_functions.topic_modelling.add_counts_columns_parallel import (\n",
    "    add_synonym_frequency_columns,\n",
    "    add_category_count_columns\n",
    ")\n",
    "from helper_functions.topic_modelling.aggregate_articles import aggregate_articles, FS_CATEGORIES\n",
    "\n",
    "# 1ï¸âƒ£ Load enriched event dataset with:\n",
    "#    - articles (list of cleaned + truncated text strings)\n",
    "#    - NER_admin0_list / admin1 / admin2\n",
    "#    - event metadata (ADMIN0/1/2, CS_score, period, etc.)\n",
    "# ================================================================\n",
    "\n",
    "exploded_df = pd.read_parquet(\"../data/gdelt/gkg/scraped_urls/cleaned_filtered_urls.parquet\")\n",
    "\n",
    "# ================================================================\n",
    "# 3ï¸âƒ£ (OPTIONAL SAFETY) Ensure clean_text is string\n",
    "# ================================================================\n",
    "exploded_df[\"clean_text\"] = exploded_df[\"clean_text\"].astype(str)\n",
    "\n",
    "# ================================================================\n",
    "# 4ï¸âƒ£ Sentiment Analysis\n",
    "# ================================================================\n",
    "exploded_df = perform_sentiment_analysis(exploded_df, text_col=\"clean_text\")\n",
    "print(\"Sentiment analysis done.\")\n",
    "\n",
    "# ================================================================\n",
    "# 5ï¸âƒ£ Keyword frequency & category count features\n",
    "# ================================================================\n",
    "exploded_df = add_synonym_frequency_columns(exploded_df, text_col=\"clean_text\")\n",
    "print(\"Frequency counts done\")\n",
    "exploded_df = add_category_count_columns(exploded_df, text_col=\"clean_text\")\n",
    "print(\"Keyword & category counts added.\")\n",
    "\n",
    "# ================================================================\n",
    "# 6ï¸âƒ£ Save in chunks\n",
    "# ================================================================\n",
    "out_dir = \"../data/gdelt/gkg/5_modelled\"\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "exploded_df.to_parquet(out_dir + \"/gkg_exploded_with_counts.parquet\", index=False)\n",
    "\n",
    "print(\"âœ… Processing complete.\")\n",
    "exploded_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_array_columns(df):\n",
    "    for col in df.columns:\n",
    "        # convert [\"2020-02-13\"] â†’ \"2020-02-13\"\n",
    "        df[col] = df[col].apply(\n",
    "            lambda x: x[0] if isinstance(x, (list, tuple, np.ndarray)) and len(x) == 1 else x\n",
    "        )\n",
    "    return df\n",
    "\n",
    "exploded_df = clean_array_columns(exploded_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import BertTokenizer, BertPreTrainedModel, BertModel\n",
    "import transformers\n",
    "import torch.nn as nn\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import mode\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# CONFIGURATION\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "MODEL_DIR = \"./for_bert/bert_multitask_model\"\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "MAX_LENGTH = 512\n",
    "STRIDE = 256\n",
    "\n",
    "# These must exactly match training\n",
    "impact_classes = [\n",
    "    \"death\", \"injury\", \"displacement\", \"missing\", \"disease\",\n",
    "    \"economic_loss\", \"conflict\", \"weather_shock\", \"policy_change\", \"none\"\n",
    "]\n",
    "urgency_classes = [\"low\", \"moderate\", \"high\", \"unclear\"]\n",
    "resource_cols = [\n",
    "    \"resource_food\", \"resource_water\", \"resource_cash_aid\", \"resource_healthcare\", \"resource_shelter\",\n",
    "    \"resource_livelihoods\", \"resource_education\", \"resource_infrastructure\", \"resource_none\"\n",
    "]\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# LABEL ENCODERS\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "impact_enc = LabelEncoder()\n",
    "impact_enc.fit(impact_classes)\n",
    "\n",
    "urgency_enc = LabelEncoder()\n",
    "urgency_enc.fit(urgency_classes)\n",
    "\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# MODEL DEFINITION\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "class MultiTaskBERT(BertPreTrainedModel):\n",
    "    def __init__(self, config, num_impact, num_urgency, num_resource):\n",
    "        super().__init__(config)\n",
    "        self.bert = BertModel(config)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.impact_head = nn.Linear(config.hidden_size, num_impact)\n",
    "        self.urgency_head = nn.Linear(config.hidden_size, num_urgency)\n",
    "        self.resource_head = nn.Linear(config.hidden_size, num_resource)\n",
    "        self.init_weights()\n",
    "    def forward(self, input_ids=None, attention_mask=None, **kwargs):\n",
    "        pooled = self.dropout(self.bert(input_ids=input_ids, attention_mask=attention_mask).pooler_output)\n",
    "        return {\n",
    "            \"impact\": self.impact_head(pooled),\n",
    "            \"urgency\": self.urgency_head(pooled),\n",
    "            \"resource\": self.resource_head(pooled)\n",
    "        }\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# LOAD MODEL & TOKENIZER\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "tokenizer = BertTokenizer.from_pretrained(MODEL_DIR)\n",
    "model = MultiTaskBERT.from_pretrained(\n",
    "    MODEL_DIR,\n",
    "    num_impact=len(impact_classes),\n",
    "    num_urgency=len(urgency_classes),\n",
    "    num_resource=len(resource_cols)\n",
    ")\n",
    "model.to(DEVICE)\n",
    "model.eval()\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# CHUNKING FUNCTION (from your training)\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "transformers.logging.set_verbosity_error()  # At the top to silence all warnings\n",
    "def chunk_input_ids(text, tokenizer, max_length=512, stride=256):\n",
    "    # Get raw tokens (no special tokens)\n",
    "    tokens = tokenizer.encode(text, add_special_tokens=False)\n",
    "    max_len_no_special = max_length - 2\n",
    "    chunks = []\n",
    "    for i in range(0, len(tokens), stride):\n",
    "        chunk = tokens[i:i + max_len_no_special]\n",
    "        # Now, turn chunk back into string and re-encode with special tokens, pad/truncate to max_length\n",
    "        chunk_text = tokenizer.decode(chunk)\n",
    "        inputs = tokenizer(\n",
    "            chunk_text,\n",
    "            max_length=max_length,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        chunks.append(inputs)\n",
    "    return chunks\n",
    "\n",
    "def predict_for_text(text):\n",
    "    chunks = chunk_input_ids(str(text), tokenizer, max_length=MAX_LENGTH, stride=STRIDE)\n",
    "    if not chunks:\n",
    "        # fallback: [UNK] padded to 512\n",
    "        inputs = tokenizer(\n",
    "            tokenizer.unk_token,\n",
    "            max_length=MAX_LENGTH,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        chunks = [inputs]\n",
    "    input_ids_tensor = torch.cat([chunk['input_ids'] for chunk in chunks]).to(DEVICE)\n",
    "    attention_mask_tensor = torch.cat([chunk['attention_mask'] for chunk in chunks]).to(DEVICE)\n",
    "\n",
    "    # --- DEBUG & CATCH ---\n",
    "    assert input_ids_tensor.shape[1] <= 512, f\"Found input_ids length {input_ids_tensor.shape[1]}\"\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids_tensor, attention_mask=attention_mask_tensor)\n",
    "    # ... (rest of your code unchanged)\n",
    "    impact_preds = outputs[\"impact\"].argmax(dim=1).cpu().numpy()\n",
    "    urgency_preds = outputs[\"urgency\"].argmax(dim=1).cpu().numpy()\n",
    "    resource_probs = torch.sigmoid(outputs[\"resource\"]).cpu().numpy()\n",
    "    resource_preds = (resource_probs > 0.5).astype(int)\n",
    "\n",
    "    impact_final = mode(impact_preds, keepdims=True)[0][0]\n",
    "    urgency_final = mode(urgency_preds, keepdims=True)[0][0]\n",
    "    resource_final = (resource_preds.sum(axis=0) > 0).astype(int)\n",
    "\n",
    "    impact_label = impact_enc.inverse_transform([impact_final])[0]\n",
    "    urgency_label = urgency_enc.inverse_transform([urgency_final])[0]\n",
    "    resource_dict = {col: int(val) for col, val in zip(resource_cols, resource_final)}\n",
    "    return impact_label, urgency_label, resource_dict\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# APPLY TO DATAFRAME\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "# Example: df should have a column 'clean_text'\n",
    "tqdm.pandas()  # progress bar\n",
    "\n",
    "exploded_df = pd.read_parquet(\"../data/gdelt/events/5_modelled/exploded_partial.parquet\")\n",
    "\n",
    "results = exploded_df['clean_text'].progress_apply(predict_for_text)\n",
    "\n",
    "# Add predictions to dataframe\n",
    "exploded_df['pred_impact_type'] = results.apply(lambda x: x[0])\n",
    "exploded_df['pred_urgency'] = results.apply(lambda x: x[1])\n",
    "for i, col in enumerate(resource_cols):\n",
    "    exploded_df[f'pred_{col}'] = results.apply(lambda x: x[2][col])\n",
    "\n",
    "exploded_df.to_parquet(\"../data/gdelt/events/5_modelled/exploded_full.parquet\")\n",
    "exploded_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "events = pd.read_parquet(\"../data/gdelt/events/5_modelled/exploded_full.parquet\", engine=\"fastparquet\")\n",
    "events = events.drop(columns=[c for c in events.columns if c.startswith(\"sentiment.\")])\n",
    "events.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8) Re-aggregate so each original row has arrays of new info\n",
    "group_cols = ['ADMIN0', 'ADMIN1', 'ADMIN2', 'CS_score', 'period']\n",
    "original_list_cols = ['SQLDATE', 'EventCode', 'SOURCEURL', 'NumMentions', 'NumSources', 'NumArticles']\n",
    "\n",
    "final_df = aggregate_articles(\n",
    "    exploded_df,\n",
    "    group_cols=group_cols,\n",
    "    original_list_cols=original_list_cols,\n",
    "    original_df=original_df\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Create a subset for topic modeling\n",
    "bertopic_df = exploded_df[valid_rows].copy()\n",
    "print(f\"BERTopic will run on {bertopic_df.shape[0]} articles (filtered from {exploded_df.shape[0]})\")\n",
    "\n",
    "# Run BERTopic only on valid rows\n",
    "bertopic_df, topic_model = topic_modeling_bertopic(bertopic_df, text_col='clean_text', language='english')\n",
    "\n",
    "# 7) Analyze Topics & Sentiment\n",
    "conflict_keywords = [\n",
    "    'conflict', 'war', 'violence', 'military', 'coup', 'insurgency', 'terrorism',\n",
    "    'battle', 'hostility', 'clash', 'unrest'\n",
    "]\n",
    "climate_keywords = [\n",
    "    'climate', 'weather', 'drought', 'flood', 'rainfall', 'heatwave',\n",
    "    'temperature anomaly', 'extreme weather'\n",
    "]\n",
    "food_security_keywords = [\n",
    "    'food security', 'economic shock', 'inflation', 'poverty', 'supply chain',\n",
    "    'food prices', 'commodity prices', 'subsistence', 'rural development',\n",
    "    'agricultural production', 'sustainable agriculture', 'food access',\n",
    "     'food aid', 'emergency relief', 'food assistance', 'nutrition', 'hunger', 'famine',\n",
    "    'malnutrition', 'crop yield', 'harvest', 'farming', 'livestock'\n",
    "]\n",
    "\n",
    "if topic_model is None:\n",
    "    print(\"Skipping analyze_topics_and_sentiment due to lack of a topic model.\")\n",
    "else:\n",
    "    bertopic_df = analyze_topics_and_sentiment(bertopic_df, topic_model, conflict_keywords, climate_keywords, food_security_keywords)\n",
    "print(\"Analysis of topics and sentiment completed\")\n",
    "\n",
    "exploded_df = exploded_df.merge(bertopic_df[['topic', 'topic_probability', 'topic_label', 'severity']], how='left', left_index=True, right_index=True)\n",
    "print(\"Topic modeling and sentiment analysis done.\")\n",
    "\n",
    "# 8) Re-aggregate so each original row has arrays of new info\n",
    "group_cols = ['ADMIN0', 'ADMIN1', 'ADMIN2', 'CS_score', 'period']\n",
    "original_list_cols = ['SQLDATE', 'EventCode', 'SOURCEURL', 'NumMentions', 'NumSources', 'NumArticles', 'headers', 'bodies']\n",
    "\n",
    "final_df = aggregate_articles(\n",
    "    exploded_df,\n",
    "    group_cols=group_cols,\n",
    "    original_list_cols=original_list_cols,\n",
    "    original_df=original_df\n",
    ")\n",
    "\n",
    "# 9) Save or proceed with analysis\n",
    "keep_cols = [\n",
    "    'ADMIN0', 'ADMIN1', 'ADMIN2', 'CS_score', 'period',\n",
    "    'SQLDATE', 'EventCode', 'SOURCEURL', 'NumMentions',\t'NumSources', 'NumArticles',\n",
    "    'headers', 'sentiment', 'compound_score',\n",
    "    'topic', 'topic_probability', 'topic_label', 'severity',\n",
    "    'named_entities', 'number_context', 'TotalNumMentions', 'TotalNumSources', 'TotalNumArticles'\n",
    "]\n",
    "for cat in FS_CATEGORIES:\n",
    "    keep_cols.append(f\"{cat}_count\")\n",
    "    keep_cols.append(f\"{cat}_freq\")\n",
    "    keep_cols.append(f\"{cat}_sentences\")\n",
    "\n",
    "keep_cols = [c for c in keep_cols if c in final_df.columns]\n",
    "final_df = final_df[keep_cols]\n",
    "\n",
    "\n",
    "final_df.to_parquet('../data/gdelt/events/5_modelled/data.parquet')\n",
    "\n",
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exploded_df.applymap(lambda x: isinstance(x, (list, tuple, np.ndarray))).any()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exploded_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "df = pd.read_parquet(\"../data/gdelt/events/5_modelled/exploded_partial_chunk_1.parquet\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# aggregate everything at the end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ----------------------- Utility Function -----------------------\n",
    "\n",
    "def clean_text_array(arr):\n",
    "    \"\"\"Remove control characters from each string in a list/array.\"\"\"\n",
    "    if isinstance(arr, (list, np.ndarray)):\n",
    "        return [\n",
    "            re.sub(r'[\\x00-\\x1F\\x7F-\\x9F]', '', str(x))\n",
    "            for x in arr if isinstance(x, str) and x.strip()\n",
    "        ]\n",
    "    return arr\n",
    "\n",
    "# ----------------------- Load GDELT Final Output -----------------------\n",
    "\n",
    "mapped_path = \"../data/gdelt/events/3_mapped/gdelt_mapped.parquet\"\n",
    "if not os.path.exists(mapped_path):\n",
    "    raise FileNotFoundError(f\"Missing mapped GDELT file: {mapped_path}\")\n",
    "\n",
    "df_final = pd.read_parquet(mapped_path)\n",
    "print(\"âœ… Loaded GDELT mapped dataset:\", len(df_final))\n",
    "\n",
    "# ----------------------- Build ADMIN lookup from valid rows -----------------------\n",
    "\n",
    "locations = df_final[['ADMIN0', 'ADMIN1', 'ADMIN2']]\n",
    "locations = locations[locations['ADMIN0'].notna()]\n",
    "locations = locations.drop_duplicates()\n",
    "\n",
    "# Dictionary: (ADMIN1, ADMIN2) â†’ ADMIN0\n",
    "admin_lookup = {\n",
    "    (row.ADMIN1, row.ADMIN2): row.ADMIN0\n",
    "    for _, row in locations.iterrows()\n",
    "}\n",
    "\n",
    "def fill_admin0(row):\n",
    "    \"\"\"Fill missing ADMIN0 using (ADMIN1, ADMIN2) lookup.\"\"\"\n",
    "    if pd.isna(row['ADMIN0']):\n",
    "        return admin_lookup.get((row['ADMIN1'], row['ADMIN2']), None)\n",
    "    return row['ADMIN0']\n",
    "\n",
    "# Apply the fill\n",
    "df_final['ADMIN0'] = df_final.apply(fill_admin0, axis=1)\n",
    "\n",
    "print(\"ğŸ§­ Missing ADMIN0 after filling:\", df_final['ADMIN0'].isna().sum())\n",
    "\n",
    "# ----------------------- Prepare Mapping from Scraped URLs -----------------------\n",
    "\n",
    "# Ensure df_scraped has needed columns\n",
    "required_cols = ['url', 'clean_text', 'NER_admin0', 'NER_admin1', 'NER_admin2']\n",
    "assert all(col in df_scraped.columns for col in required_cols), \\\n",
    "    f\"df_scraped must contain {required_cols}, found: {df_scraped.columns.tolist()}\"\n",
    "\n",
    "# Create mappings from URL â†’ clean_text / NER\n",
    "url_to_cleantext = df_scraped.set_index('url')['clean_text'].to_dict()\n",
    "url_to_admin0 = df_scraped.set_index('url')['NER_admin0'].to_dict()\n",
    "url_to_admin1 = df_scraped.set_index('url')['NER_admin1'].to_dict()\n",
    "url_to_admin2 = df_scraped.set_index('url')['NER_admin2'].to_dict()\n",
    "\n",
    "def extract_list(urls, mapping):\n",
    "    \"\"\"Extract a list of mapped values corresponding to a list of SOURCEURL URLs.\"\"\"\n",
    "    if isinstance(urls, (list, np.ndarray)):\n",
    "        return [mapping[u] for u in urls if isinstance(u, str) and u in mapping]\n",
    "    return []\n",
    "\n",
    "# ----------------------- Map Clean Text + NER to df_final -----------------------\n",
    "\n",
    "df_final['articles'] = df_final['SOURCEURL'].apply(\n",
    "    lambda urls: extract_list(urls, url_to_cleantext)\n",
    ")\n",
    "\n",
    "df_final['NER_admin0_list'] = df_final['SOURCEURL'].apply(\n",
    "    lambda urls: extract_list(urls, url_to_admin0)\n",
    ")\n",
    "\n",
    "df_final['NER_admin1_list'] = df_final['SOURCEURL'].apply(\n",
    "    lambda urls: extract_list(urls, url_to_admin1)\n",
    ")\n",
    "\n",
    "df_final['NER_admin2_list'] = df_final['SOURCEURL'].apply(\n",
    "    lambda urls: extract_list(urls, url_to_admin2)\n",
    ")\n",
    "\n",
    "# Clean control characters in articles\n",
    "df_final['articles'] = df_final['articles'].apply(clean_text_array)\n",
    "\n",
    "# ----------------------- Sanity Checks -----------------------\n",
    "\n",
    "df_final['num_articles'] = df_final['articles'].apply(len)\n",
    "print(\"ğŸ§¾ Rows with no matched articles:\", (df_final['num_articles'] == 0).sum())\n",
    "print(\"ğŸ“Š Distribution of article counts per event:\")\n",
    "print(df_final['num_articles'].describe())\n",
    "\n",
    "# ----------------------- Save Full File -----------------------\n",
    "\n",
    "out_dir = \"../data/gdelt/events/4_scraped\"\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "df_final = df_final[\n",
    "    (df_final[\"num_articles\"] > 0) |\n",
    "    (df_final[\"CS_score\"].notna())\n",
    "].reset_index(drop=True)\n",
    "\n",
    "full_out = os.path.join(out_dir, \"data_with_articles_full.parquet\")\n",
    "df_final.to_parquet(full_out, index=False)\n",
    "print(f\"ğŸ’¾ Saved full enriched dataset: {full_out}\")\n",
    "\n",
    "# ----------------------- Save in Chunks -----------------------\n",
    "\n",
    "chunk_size = 20_000\n",
    "num_chunks = (len(df_final) + chunk_size - 1) // chunk_size\n",
    "\n",
    "for i in range(num_chunks):\n",
    "    start_row = i * chunk_size\n",
    "    end_row = min((i + 1) * chunk_size, len(df_final))\n",
    "    chunk_df = df_final.iloc[start_row:end_row].copy()\n",
    "    chunk_path = os.path.join(out_dir, f\"data_with_articles_chunk_{i}.parquet\")\n",
    "    chunk_df.to_parquet(chunk_path, index=False)\n",
    "    print(f\"âœ… Saved chunk {i+1}/{num_chunks}: {chunk_path}\")\n",
    "\n",
    "print(f\"\\nğŸ‰ Done! {num_chunks} Parquet chunk files saved in {out_dir}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
