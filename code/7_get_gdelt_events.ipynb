{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "#### Summary:\n",
    "- 3 baselines, copy the value from last period, copy the value from the same period the year before, take the max between the last 2 periods\n",
    "- Models on geo features alone can at best match the performance of the baseline\n",
    "- Need to find a way to bring some improvement by adding conflict features!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "# Gdelt events data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "import os\n",
    "import requests\n",
    "import zipfile\n",
    "from datetime import datetime, timedelta\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from bs4 import BeautifulSoup\n",
    "import helper_functions.download_gdelt_events as download_gdelt\n",
    "from helper_functions.download_gdelt_events import consolidate_files\n",
    "from helper_functions.gdelt_data_mapping_optimized import load_gadm_data, process_gdelt_data, consolidate_and_merge_fews\n",
    "import multiprocessing\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "from helper_functions.url_scraping_base import process_urls_in_chunks\n",
    "import numpy as np\n",
    "import glob\n",
    "import re\n",
    "import collections.abc\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Download GDELT raw data\n",
    "# ---------------------------------------------------------------------\n",
    "start_date = datetime(2016, 1, 1)\n",
    "end_date = datetime(2024, 2, 28)\n",
    "save_directory = \"../data/gdelt/events/1_raw\"\n",
    "output_file = \"../data/gdelt/events/2_consolidated/combined_data.parquet\"\n",
    "\n",
    "# Paths\n",
    "gadm_path = \"../data/gadm/gadm_410_filtered_v2.gpkg\"\n",
    "fews_path = \"../data/fews/fews_with_conflicts_admin2.parquet\"\n",
    "gdelt_path = \"../data/gdelt/events/2_consolidated/combined_data.parquet\"\n",
    "output_dir = \"../data/gdelt/events/3_mapped/\"\n",
    "final_output_path = os.path.join(output_dir, \"gdelt_mapped.parquet\")\n",
    "\n",
    "# print(\"Downloading GDELT data...\")\n",
    "# download_gdelt.download_gdelt_data(start_date, end_date, save_directory, max_workers=8)\n",
    "\n",
    "# print(\"Consolidating GDELT data...\")\n",
    "# consolidate_files(save_directory, output_file)\n",
    "# 4M articles at this point\n",
    "\n",
    "# recommended_cpus = max(1, multiprocessing.cpu_count() - 1)\n",
    "\n",
    "# # # ---------------------------------------------------------------------\n",
    "# # Load reference data\n",
    "# # ---------------------------------------------------------------------\n",
    "# fews_df = pd.read_parquet(fews_path)\n",
    "# fews_df = fews_df[['ADMIN0', 'ADMIN1', 'ADMIN2', 'period', 'CS_score']]\n",
    "\n",
    "\n",
    "# print(\"Loading GADM\")\n",
    "# gadm_gdf, fews_df = load_gadm_data(gadm_path, fews_df)\n",
    "\n",
    "# # ---------------------------------------------------------------------\n",
    "# # Load and process GDELT\n",
    "# # ---------------------------------------------------------------------\n",
    "# gdelt_df = pd.read_parquet(gdelt_path)\n",
    "# gdelt_df = gdelt_df.dropna(subset=[\"ActionGeo_Lat\", \"ActionGeo_Long\"])\n",
    "\n",
    "# print(\"Processing GDELT data\")\n",
    "#\n",
    "#  process_gdelt_data(gdelt_df, gadm_gdf, output_dir, num_cpus=recommended_cpus)\n",
    "\n",
    "# # ---------------------------------------------------------------------\n",
    "# # Merge with FEWS\n",
    "# # ---------------------------------------------------------------------\n",
    "# print(\"Consolidating mapped GDELT data with FEWS data\")\n",
    "\n",
    "# df_final = consolidate_and_merge_fews(mapped_gdelt_dir=output_dir, fews_df=fews_df)\n",
    "# # 4M articles at this point\n",
    "\n",
    "# df_final.to_parquet(final_output_path, index=False)\n",
    "\n",
    "# print(\"Number of records in final dataset:\", len(df_final))\n",
    "# print(\"\\nFirst few rows of the final dataset:\")\n",
    "# df_final.head()\n",
    "# 4M articles at this point\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(final_output_path)\n",
    "# df = df[df['period'] == '202402']\n",
    "# ---------------------------------------------------------------------\n",
    "# Extract unique URLs\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "def flatten_and_filter(source_col):\n",
    "    urls = set()\n",
    "    for item in source_col.dropna():\n",
    "        if isinstance(item, (list, tuple, set, np.ndarray)):  # ðŸ‘ˆ added np.ndarray\n",
    "            urls.update(\n",
    "                x for x in item\n",
    "                if isinstance(x, str) and x.startswith(\"http\")\n",
    "            )\n",
    "    return urls\n",
    "\n",
    "unique_urls = sorted(flatten_and_filter(df[\"SOURCEURL\"]))\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Handle already processed chunks\n",
    "# ---------------------------------------------------------------------\n",
    "chunk_dir = \"../data/gdelt/events/scraped_urls\"\n",
    "os.makedirs(chunk_dir, exist_ok=True)\n",
    "parquet_files = glob.glob(os.path.join(chunk_dir, \"chunk_*.parquet\"))\n",
    "\n",
    "chunk_numbers = (\n",
    "    [int(re.search(r'chunk_(\\d+)\\.parquet', f).group(1)) for f in parquet_files]\n",
    "    if parquet_files else []\n",
    ")\n",
    "highest_chunk = max(chunk_numbers) + 1 if chunk_numbers else 0\n",
    "print(f\"Highest chunk number: {highest_chunk}\")\n",
    "\n",
    "if parquet_files:\n",
    "    processed = pd.concat([pd.read_parquet(f) for f in parquet_files], ignore_index=True)\n",
    "    processed_urls = set(processed['url'].dropna().values)\n",
    "else:\n",
    "    processed_urls = set()\n",
    "\n",
    "# Remove already processed URLs\n",
    "unique_urls = [u for u in unique_urls if u not in processed_urls]\n",
    "\n",
    "print(f\"Remaining URLs to process: {len(unique_urls)}\")\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Run URL scraping\n",
    "# ---------------------------------------------------------------------\n",
    "process_urls_in_chunks(\n",
    "    urls=unique_urls,\n",
    "    chunk_size=10000,\n",
    "    concurrency=150,\n",
    "    chunk_id=highest_chunk,\n",
    "    timeout=4,\n",
    "    max_retries=3,\n",
    "    max_selenium_workers=4,\n",
    "    fallback_mode=\"async_only\",\n",
    "    output_dir=chunk_dir\n",
    ")\n",
    "# 1.9M articles at this point"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "### Clean the scraped URLS, remove duplicate articles, and articles not related to FS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import math\n",
    "import logging\n",
    "import multiprocessing as mp\n",
    "from collections import Counter\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "import glob\n",
    "import spacy\n",
    "\n",
    "from helper_functions.topic_modelling.text_processing_parallel import preprocess_text_parallel\n",
    "from helper_functions.topic_modelling.flatten_articles import filter_articles_by_lexicon\n",
    "from helper_functions.topic_modelling.deduplication import deduplicate_minhash\n",
    "from helper_functions.run_ner_parallel import run_ner_parallel, inject_countries_from_demonyms  # âœ… imported correctly\n",
    "\n",
    "# ----------------- Load scraped files -----------------\n",
    "parquet_dir = \"../data/gdelt/events/scraped_urls\"\n",
    "parquet_files = glob.glob(os.path.join(parquet_dir, \"chunk_*.parquet\"))\n",
    "if not parquet_files:\n",
    "    raise FileNotFoundError(f\"No parquet files found in {parquet_dir}\")\n",
    "\n",
    "df_scraped = pd.concat([pd.read_parquet(f) for f in parquet_files], ignore_index=True)\n",
    "print(\"Total scraped articles:\", len(df_scraped))\n",
    "# ~1.9M articles at this point\n",
    "\n",
    "# ----------------- Remove noisy events -----------------\n",
    "mapped_path = \"../data/gdelt/events/2_consolidated/combined_data.parquet\"\n",
    "if not os.path.exists(mapped_path):\n",
    "    raise FileNotFoundError(f\"Missing mapped GDELT file: {mapped_path}\")\n",
    "\n",
    "codes_mapping = pd.read_parquet(mapped_path)\n",
    "codes_mapping = (\n",
    "    codes_mapping\n",
    "    .groupby(\"SOURCEURL\", as_index=False)\n",
    "    .agg({\"EventCode\": \"max\"})\n",
    ")\n",
    "\n",
    "df_scraped = pd.merge(df_scraped, codes_mapping, left_on='url', right_on='SOURCEURL', how='inner')\n",
    "df_scraped = df_scraped[~df_scraped[\"EventCode\"].isin([10, 20, 30, 31, 32, 33, 34, 42, 43, 46])].reset_index(drop=True)\n",
    "print(\"Remaining after removing 10/20/30/31/32/33/34/42/43/46:\", len(df_scraped))\n",
    "# ~920K here\n",
    "\n",
    "# ----------------- Pre-processing pipeline -----------------\n",
    "df_scraped['header'] = df_scraped['header'].fillna(\"\").astype(str)\n",
    "df_scraped['body'] = df_scraped['body'].fillna(\"\").astype(str)\n",
    "df_scraped['text'] = df_scraped['header'] + \" \" + df_scraped['body']\n",
    "\n",
    "print(\"Applying LEAP4FNSSA lexicon filter (raw text)...\")\n",
    "\n",
    "df_scraped = filter_articles_by_lexicon(\n",
    "    df_scraped,\n",
    "    clean_text_col=\"text\",\n",
    "    lexicon_path=\"../data/LEAP4FNSSA_LEXICON_long.csv\"\n",
    ")\n",
    "print(\"Lexicon filtering complete. Remaining:\", df_scraped.shape)\n",
    "# ~381k articles\n",
    "\n",
    "# 2ï¸âƒ£ Heavy preprocessing\n",
    "df_scraped = preprocess_text_parallel(df_scraped, text_col='text')\n",
    "print(\"Text preprocessing done.\")\n",
    "\n",
    "# 3ï¸âƒ£ Truncate to 500 words\n",
    "def truncate_to_500_words(text):\n",
    "    words = str(text).split()\n",
    "    return ' '.join(words[:500])\n",
    "\n",
    "df_scraped['clean_text'] = df_scraped['clean_text'].apply(truncate_to_500_words)\n",
    "\n",
    "# 4ï¸âƒ£ Deduplicate by near-duplicate text\n",
    "df_scraped = deduplicate_minhash(df_scraped, text_col='clean_text', threshold=0.85)\n",
    "print(\"After deduplication:\", df_scraped.shape)\n",
    "# ~354k articles\n",
    "\n",
    "# ----------------- NER Extraction -----------------\n",
    "print(\"ðŸ” Running NER location extraction...\")\n",
    "\n",
    "# Inject country names for demonyms before NER\n",
    "df_scraped[\"clean_text\"] = df_scraped[\"clean_text\"].apply(inject_countries_from_demonyms)\n",
    "\n",
    "# Automatically disable multiprocessing in Jupyter (for stability)\n",
    "n_process = 1 if \"ipykernel\" in mp.current_process().name.lower() else mp.cpu_count()\n",
    "\n",
    "df_scraped = run_ner_parallel(df_scraped, text_col=\"clean_text\", n_process=n_process)\n",
    "print(\"âœ… NER extraction complete.\")\n",
    "\n",
    "# ----------------- Load FEWS countries (for both refinement & filtering) -----------------\n",
    "fews_path = \"../data/fews/fews_with_conflicts_admin2.parquet\"\n",
    "fews_df = pd.read_parquet(fews_path)\n",
    "fews_countries = [country.lower() for country in fews_df['ADMIN0'].unique()]\n",
    "\n",
    "# ----------------- Refine main country using FEWS mentions -----------------\n",
    "# Keep raw spaCy outputs for debugging if needed\n",
    "df_scraped[\"NER_admin0_raw\"] = df_scraped[\"NER_admin0\"]\n",
    "df_scraped[\"NER_admin1_raw\"] = df_scraped[\"NER_admin1\"]\n",
    "df_scraped[\"NER_admin2_raw\"] = df_scraped[\"NER_admin2\"]\n",
    "\n",
    "def pick_main_country_from_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Count mentions of each FEWS country in the article text (clean_text)\n",
    "    and return the most frequently mentioned one (or None if no FEWS country appears).\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return None\n",
    "\n",
    "    txt = text.lower()\n",
    "    counts = Counter()\n",
    "\n",
    "    # naive substring counting; demonyms already injected as country names\n",
    "    for country in fews_countries:\n",
    "        c = txt.count(country)\n",
    "        if c > 0:\n",
    "            counts[country] += c\n",
    "\n",
    "    if counts:\n",
    "        return counts.most_common(1)[0][0]\n",
    "    return None\n",
    "\n",
    "def refine_ner_country(row):\n",
    "    main = pick_main_country_from_text(row[\"clean_text\"])\n",
    "    if main:\n",
    "        # override top-level country with the most-mentioned FEWS country\n",
    "        row[\"NER_admin0\"] = main\n",
    "    return row\n",
    "\n",
    "df_scraped = df_scraped.apply(refine_ner_country, axis=1)\n",
    "print(\"âœ… NER country refinement based on FEWS mentions complete.\")\n",
    "\n",
    "# ----------------- Filter by FEWS countries -----------------\n",
    "df_scraped = df_scraped[\n",
    "    df_scraped['NER_admin0'].isin(fews_countries) |\n",
    "    df_scraped['NER_admin1'].isin(fews_countries) |\n",
    "    df_scraped['NER_admin2'].isin(fews_countries)\n",
    "]\n",
    "print(\"Remaining after FEWS NER filter:\", len(df_scraped))\n",
    "\n",
    "# ----------------- Filter by crisis terms -----------------\n",
    "crisis_terms = re.compile(\n",
    "    r\"(famine|hunger|malnutrition|food|nutrition|crop|harvest|yield|farmer|\"\n",
    "    r\"agricultur|drought|flood|rainfall|storm|cyclone|heatwave|disaster|aid|relief|\"\n",
    "    r\"refugee|displaced|idp|conflict|war|violence|attack|protest|unrest|\"\n",
    "    r\"livelihood|poverty|shortage|inflation|market|commodity|price|\"\n",
    "    r\"food security|early warning|ipc|wfp|unicef|ocha|ngo)\",\n",
    "    re.I\n",
    ")\n",
    "\n",
    "df_scraped = df_scraped[\n",
    "    df_scraped[\"clean_text\"].str.contains(crisis_terms, na=False)\n",
    "]\n",
    "\n",
    "print(\"After crisis-context filter:\", len(df_scraped))\n",
    "\n",
    "# ----------------- Save output -----------------\n",
    "out_path = \"../data/gdelt/events/scraped_urls/cleaned_filtered_urls.parquet\"\n",
    "df_scraped.to_parquet(out_path, index=False)\n",
    "print(f\"ðŸ’¾ Saved to {out_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scraped[df_scraped['url'] == 'https://www.thenews.com.pk/print/666155-30-killed-in-eastern-burkina-faso-attack']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "19e238ca122764f831b3091dd8ff35b1b7683cbf7688be73b57ef77bc8403891"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
