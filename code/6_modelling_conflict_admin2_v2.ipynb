{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "#### Summary:\n",
    "- 3 baselines, copy the value from last period, copy the value from the same period the year before, take the max between the last 2 periods\n",
    "- Models on geo features alone can at best match the performance of the baseline\n",
    "- Need to find a way to bring some improvement by adding conflict features!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "### 1286 rows in test, with models i predict all of them\n",
    "\n",
    "It is high because many areas are missing. options are:\n",
    "- inner join and only count what is there (PPS has highest accuracy but predicts 50% less areas)\n",
    "- left join and fillna(0) which keeps all rows from test but lowers accuracy ad all missing areas are considered wrong\n",
    "- left join and fillna(witht the correct score) but that artificially inflates the accuracy\n",
    "- Use a previous period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV, train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix, f1_score, multilabel_confusion_matrix\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn import metrics, preprocessing\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "#from imblearn.over_sampling import SMOTE, ADASYN\n",
    "from sklearn.utils import class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\"../data_2023/acled_data/processed/fews_with_conflicts_admin2.parquet\")\n",
    "#df = df[(df['ADMIN0'] == 'Burkina Faso') & (df['ADMIN2'] == 'Bale')]\n",
    "df.head(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "### Fit a classifier to predict the exact IPC score\n",
    "- We start by creating a naive rule based classifier to be used as baseline\n",
    "- We then train and test actual models and compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\"../data_2023/acled_data/processed/fews_with_conflicts_admin2.parquet\")\n",
    "df = df[df['CS_score'].isin([1.0, 2.0, 3.0, 4.0, 5.0])]\n",
    "\n",
    "df['CS_score'] = np.ceil(df['CS_score'])\n",
    "train = df[df['period'] == '202310']\n",
    "test = df[df['period'] == '202402']\n",
    "\n",
    "train.reset_index(drop=True, inplace=True)\n",
    "test.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print(len(test.index))\n",
    "print(len(train.index))\n",
    "\n",
    "train = train[['ADMIN0', 'ADMIN1', 'ADMIN2', 'CS_score', 'period']]\n",
    "train.rename(columns={'CS_score':'predicted'}, inplace=True)\n",
    "\n",
    "test = pd.merge(test, train, on = ['ADMIN0', 'ADMIN1', 'ADMIN2'], how='left')\n",
    "print(len(test.index))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\"../data_2023/acled_data/processed/fews_with_conflicts_admin2.parquet\")\n",
    "df = df[df['CS_score'].isin([1.0, 2.0, 3.0, 4.0, 5.0])]\n",
    "\n",
    "df['CS_score'] = np.ceil(df['CS_score'])\n",
    "train = df[df['period'] != max(df['period'])]\n",
    "test = df[df['period'] == max(df['period'])]\n",
    "print(max(df['period']))\n",
    "\n",
    "base_summary = pd.DataFrame(columns = ['Model', 'Test Accuracy', 'Test Precision','Test Recall', 'F1'])\n",
    "base_models = ['PPS', 'SPLY', 'Max-2PP']\n",
    "accuracies = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "f1s = []\n",
    "# First naive model: score is the same as the score in the last period (PPS)\n",
    "train = train[train['period'].isin(sorted(train['period'].unique())[-1:])]\n",
    "print(train['period'].unique())\n",
    "\n",
    "train = train[['ADMIN0', 'ADMIN1', 'ADMIN2', 'CS_score', 'period']]\n",
    "train.rename(columns={'CS_score':'predicted'}, inplace=True)\n",
    "test = pd.merge(test, train, on = ['ADMIN0', 'ADMIN1', 'ADMIN2'], how='left')\n",
    "test['predicted'] = test['predicted'].fillna(0)\n",
    "print(len(test.index))\n",
    "\n",
    "accuracies.append(accuracy_score(test['CS_score'], test['predicted']))\n",
    "precisions.append(precision_score(test['CS_score'], test['predicted'], average='weighted'))\n",
    "recalls.append(recall_score(test['CS_score'], test['predicted'], average='weighted'))\n",
    "f1s.append(f1_score(test['CS_score'], test['predicted'], average='weighted'))\n",
    "\n",
    "# Second naive model: score is the same as the score in the same period last year (SPLY)\n",
    "train = df[df['period'] != max(df['period'])]\n",
    "test = df[df['period'] == max(df['period'])]\n",
    "\n",
    "train = train[train['period'].isin(sorted(train['period'].unique())[-3:-2])]\n",
    "print(train['period'].unique())\n",
    "\n",
    "train = train[['ADMIN0', 'ADMIN1' , 'ADMIN2', 'CS_score', 'period']]\n",
    "train.rename(columns={'CS_score':'predicted'}, inplace=True)\n",
    "test = pd.merge(test, train, on = ['ADMIN0', 'ADMIN1', 'ADMIN2'], how='left')\n",
    "test['predicted'] = test['predicted'].fillna(0)\n",
    "print(len(test.index))\n",
    "\n",
    "accuracies.append(accuracy_score(test['CS_score'], test['predicted']))\n",
    "precisions.append(precision_score(test['CS_score'], test['predicted'], average='weighted'))\n",
    "recalls.append(recall_score(test['CS_score'], test['predicted'], average='weighted'))\n",
    "f1s.append(f1_score(test['CS_score'], test['predicted'], average='weighted'))\n",
    "\n",
    "# Third naive model: take max score of the previous 2 periods (Max-2PP)\n",
    "train = df[df['period'] != max(df['period'])]\n",
    "test = df[df['period'] == max(df['period'])]\n",
    "\n",
    "train =  train[train['period'].isin(sorted(train['period'].unique())[-2:])]\n",
    "print(train['period'].unique())\n",
    "\n",
    "train = train[['ADMIN0', 'ADMIN1' , 'ADMIN2', 'CS_score', 'period']]\n",
    "train = pd.DataFrame(train.groupby(['ADMIN0', 'ADMIN1', 'ADMIN2'])['CS_score'].max()).reset_index()\n",
    "train.rename(columns={'CS_score':'predicted'}, inplace=True)\n",
    "test = pd.merge(test, train, on = ['ADMIN0', 'ADMIN1', 'ADMIN2'], how='left')\n",
    "test['predicted'] = test['predicted'].fillna(0)\n",
    "print(len(test.index))\n",
    "\n",
    "accuracies.append(accuracy_score(test['CS_score'], test['predicted']))\n",
    "precisions.append(precision_score(test['CS_score'], test['predicted'], average='weighted'))\n",
    "recalls.append(recall_score(test['CS_score'], test['predicted'], average='weighted'))\n",
    "f1s.append(f1_score(test['CS_score'], test['predicted'], average='weighted'))\n",
    "\n",
    "base_summary['Model'] = base_models\n",
    "base_summary['Test Accuracy'] = accuracies\n",
    "base_summary['Test Precision'] = precisions\n",
    "base_summary['Test Recall'] = recalls\n",
    "base_summary['F1'] = f1s\n",
    "base_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## Model only on historical FS data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\"../data_2023/acled_data/processed/fews_with_conflicts_admin2.parquet\")\n",
    "df = df[df['CS_score'].isin([1.0, 2.0, 3.0, 4.0, 5.0])]\n",
    "\n",
    "df['region'] = df['ADMIN0'] + '-' + df['ADMIN1'] + '-' + df['ADMIN2']\n",
    "df = df.drop(['ADMIN1', 'ADMIN2'], axis=1)\n",
    "df['CS_score_class'] = np.ceil(df['CS_score'])\n",
    "\n",
    "## New features\n",
    "df['transitions'] = df.groupby('region')['region'].transform(lambda x: (x != x.shift()).sum())\n",
    "df['previous_CS'] = df.groupby('region')['CS_score_class'].shift(1)\n",
    "\n",
    "df = df[df['period'].isin(sorted(df['period'].unique())[-7:])]\n",
    "\n",
    "use_features = ['region', 'period', 'CS_score_class', 'ADMIN0', 'previous_CS', 'transitions']\n",
    "\n",
    "df = df[use_features]\n",
    "df = pd.get_dummies(df, columns=['region', 'ADMIN0'], dtype=int)\n",
    "\n",
    "cols = list(df.columns)\n",
    "cols.remove('CS_score_class')\n",
    "cols.remove('period')\n",
    "\n",
    "train = df[df['period'] != max(df['period'])]\n",
    "train = train[train['previous_CS'].isin([1.0, 2.0, 3.0, 4.0, 5.0])]\n",
    "train = train[train['period'].isin(sorted(train['period'].unique())[-3:])]\n",
    "\n",
    "test = df[df['period'] == max(df['period'])]\n",
    "test = test[test['previous_CS'].isin([1.0, 2.0, 3.0, 4.0, 5.0])]\n",
    "\n",
    "X = train[cols]\n",
    "y = train['CS_score_class']\n",
    "X_test = test[cols]\n",
    "y_test = test['CS_score_class']\n",
    "\n",
    "LR_model = LogisticRegression(class_weight = 'balanced').fit(X, y)  \n",
    "print(\"LR DONE\")\n",
    "\n",
    "RF_model = RandomForestClassifier(n_estimators = 50, min_samples_split=30, class_weight = 'balanced', n_jobs = -1, random_state=5).fit(X, y)\n",
    "#Best Parameters: {'bootstrap': True, 'class_weight': 'balanced', 'max_depth': 35, 'min_samples_leaf': 1, 'min_samples_split': 40, 'n_estimators': 50}\n",
    "print(\"RF DONE\")\n",
    "\n",
    "classes_weights = class_weight.compute_sample_weight(\n",
    "    class_weight='balanced',\n",
    "    y=train['CS_score_class']\n",
    ")\n",
    "GB_model = XGBClassifier(n_estimators=10, max_depth=30,  n_jobs = -1, random_state=5).fit(X, (np.array(y, dtype=int) - 1), sample_weight=classes_weights)\n",
    "#print(\"GB DONE\")\n",
    "\n",
    "print(len(y_test))\n",
    "\n",
    "models = [LR_model, RF_model]#, GB_model]\n",
    "\n",
    "summary = pd.DataFrame(columns = ['Model', \n",
    "                                  #'Train Accuracy', 'Train Precision','Train Recall', \n",
    "                            'Test Accuracy', 'Test Precision','Test Recall', 'F1'])\n",
    "\n",
    "for m in models:\n",
    "    print(len(m.predict(X_test)))\n",
    "    row = list()\n",
    "    row.append(str(m))\n",
    "    if m != GB_model:\n",
    "        row.append(accuracy_score(y_test, m.predict(X_test)))\n",
    "        row.append(precision_score(y_test, m.predict(X_test), average='weighted'))\n",
    "        row.append(recall_score(y_test, m.predict(X_test), average='weighted'))\n",
    "        row.append(f1_score(y_test, m.predict(X_test), average='weighted'))\n",
    "    else:\n",
    "        row.append(accuracy_score((np.array(y_test, dtype=int) - 1), m.predict(X_test)))\n",
    "        row.append(precision_score((np.array(y_test, dtype=int) - 1), m.predict(X_test), average='weighted'))\n",
    "        row.append(recall_score((np.array(y_test, dtype=int) - 1), m.predict(X_test), average='weighted'))\n",
    "        row.append(f1_score((np.array(y_test, dtype=int) - 1), m.predict(X_test), average='weighted'))\n",
    "    summary_length = len(summary)\n",
    "    summary.loc[summary_length] = row\n",
    "\n",
    "summary = pd.concat([base_summary, summary]).reset_index()\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "0.777 - 0.744"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create larger figure size\n",
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "mcm = confusion_matrix(y_test, LR_model.predict(X_test))\n",
    "ax = sns.heatmap(mcm, annot=True, cmap='Blues', fmt='g', annot_kws={'size': 14})\n",
    "\n",
    "ax.set_title('Confusion Matrix\\n', fontsize=16, pad=20)\n",
    "ax.set_xlabel('\\nPredicted Values', fontsize=14)\n",
    "ax.set_ylabel('Actual Values', fontsize=14)\n",
    "\n",
    "# Ticket labels\n",
    "ax.xaxis.set_ticklabels(['1','2', '3', '4'], fontsize=12)\n",
    "ax.yaxis.set_ticklabels(['1','2', '3', '4'], fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameter grid for Random Forest\n",
    "parameters = {\n",
    "    'n_estimators': [10, 20, 30, 50, 100],\n",
    "    'max_depth': [5, 10, 35,],\n",
    "    'min_samples_leaf': [1, 5, 10, 40],\n",
    "    'min_samples_split': [2, 10, 40],\n",
    "    'bootstrap': [True, False],\n",
    "    'class_weight' : ['balanced']\n",
    "}\n",
    "\n",
    "# Create the Random Forest model\n",
    "rf_model = RandomForestClassifier()\n",
    "\n",
    "# Create GridSearchCV\n",
    "grid_search = GridSearchCV(rf_model, parameters, scoring='accuracy', cv=5, n_jobs=-1, verbose=10)\n",
    "\n",
    "# Fit the model\n",
    "#grid_search.fit(X, y)\n",
    "\n",
    "# Display the best parameters\n",
    "#print(\"Best Parameters:\", grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "## Model that includes conflicts data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\"../data_2023/acled_data/processed/fews_with_conflicts_admin2.parquet\")\n",
    "df = df[df['CS_score'].isin([1.0, 2.0, 3.0, 4.0, 5.0])]\n",
    "\n",
    "df['region'] = df['ADMIN0'] + '-' + df['ADMIN1'] + '-' + df['ADMIN2']\n",
    "df = df.drop(['ADMIN1', 'ADMIN2'], axis=1)\n",
    "df['CS_score_class'] = np.ceil(df['CS_score'])\n",
    "\n",
    "## New features\n",
    "# ADD IF CONFLICTS ARE SIGNIFICANT OR NOT FOR THE REGION BASED ON PREVIOUS WORK\n",
    "df['transitions'] = df.groupby('region')['CS_score_class'].transform(lambda x: (x != x.shift()).sum())\n",
    "df['total_conflicts'] = df.groupby('region')['conflicts'].transform(pd.Series.cumsum)\n",
    "df['total_fatalities'] = df.groupby('region')['fatalities'].transform(pd.Series.cumsum)\n",
    "df['previous_CS'] = df.groupby('region')['CS_score_class'].shift(1)\n",
    "\n",
    "df = df[df['period'].isin(sorted(df['period'].unique())[-7:])]\n",
    "\n",
    "use_features = ['region', 'period', 'CS_score_class', 'ADMIN0', 'previous_CS',\n",
    "                'conflicts', \n",
    "                'transitions', \n",
    "                'fatalities',\n",
    "                'total_conflicts', \n",
    "                'total_fatalities', \n",
    "                ]\n",
    "\n",
    "## Scaling non binary features\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "df[['conflicts', 'total_conflicts', 'fatalities', 'total_fatalities']] = min_max_scaler.fit_transform(df[['conflicts', 'total_conflicts', 'fatalities', 'total_fatalities']])\n",
    "\n",
    "df = df[use_features]\n",
    "df = pd.get_dummies(df, columns=['region', 'ADMIN0'], dtype=int)\n",
    "\n",
    "cols = list(df.columns)\n",
    "cols.remove('CS_score_class')\n",
    "cols.remove('period')\n",
    "\n",
    "train = df[df['period'] != max(df['period'])]\n",
    "train = train[train['previous_CS'].isin([1.0, 2.0, 3.0, 4.0, 5.0])]\n",
    "train = train[train['period'].isin(sorted(train['period'].unique())[-3:])]\n",
    "\n",
    "test = df[df['period'] == max(df['period'])]\n",
    "test = test[test['previous_CS'].isin([1.0, 2.0, 3.0, 4.0, 5.0])]\n",
    "\n",
    "X = train[cols]\n",
    "y = train['CS_score_class']\n",
    "X_test = test[cols]\n",
    "y_test = test['CS_score_class']\n",
    "oversample = ADASYN(sampling_strategy='minority')\n",
    "#X, y = oversample.fit_resample(X, y)\n",
    "\n",
    "LR_model = LogisticRegression(class_weight = 'balanced').fit(X, y)  \n",
    "print(\"LR DONE\")\n",
    "\n",
    "RF_model = RandomForestClassifier(n_estimators = 50, min_samples_split=30, class_weight = 'balanced', n_jobs = -1, random_state=5).fit(X, y)\n",
    "#Best Parameters: {'bootstrap': True, 'class_weight': 'balanced', 'max_depth': 35, 'min_samples_leaf': 1, 'min_samples_split': 40, 'n_estimators': 50}\n",
    "print(\"RF DONE\")\n",
    "\n",
    "classes_weights = class_weight.compute_sample_weight(\n",
    "    class_weight='balanced',\n",
    "    y=train['CS_score_class']\n",
    ")\n",
    "#GB_model = XGBClassifier(n_estimators=100, max_depth=20,  n_jobs = -1, random_state=5).fit(X, (np.array(y, dtype=int) - 1), sample_weight=classes_weights)\n",
    "#print(\"GB DONE\")\n",
    "\n",
    "print(len(y_test))\n",
    "\n",
    "models = [LR_model, RF_model]\n",
    "\n",
    "summary = pd.DataFrame(columns = ['Model', \n",
    "                                  #'Train Accuracy', 'Train Precision','Train Recall', \n",
    "                            'Test Accuracy', 'Test Precision','Test Recall', 'F1'])\n",
    "\n",
    "for m in models:\n",
    "    print(len(m.predict(X_test)))\n",
    "    row = list()\n",
    "    row.append(str(m))\n",
    "    if m != GB_model:\n",
    "        row.append(accuracy_score(y_test, m.predict(X_test)))\n",
    "        row.append(precision_score(y_test, m.predict(X_test), average='weighted'))\n",
    "        row.append(recall_score(y_test, m.predict(X_test), average='weighted'))\n",
    "        row.append(f1_score(y_test, m.predict(X_test), average='weighted'))\n",
    "    else:\n",
    "        row.append(accuracy_score((np.array(y_test, dtype=int) - 1), m.predict(X_test)))\n",
    "        row.append(precision_score((np.array(y_test, dtype=int) - 1), m.predict(X_test), average='weighted'))\n",
    "        row.append(recall_score((np.array(y_test, dtype=int) - 1), m.predict(X_test), average='weighted'))\n",
    "        row.append(f1_score((np.array(y_test, dtype=int) - 1), m.predict(X_test), average='weighted'))\n",
    "    summary_length = len(summary)\n",
    "    summary.loc[summary_length] = row\n",
    "\n",
    "summary = pd.concat([base_summary, summary]).reset_index()\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = pd.DataFrame(list(zip(X.columns, RF_model.feature_importances_)), columns =['Features', 'importance'])\n",
    "features = features[features['importance'] >= 0.005]\n",
    "features = features.sort_values('importance', ascending=True)\n",
    "fig = px.bar(features, x=\"importance\", y=\"Features\", orientation='h')\n",
    "fig.update_layout(title_text=\"Feature importance\",\n",
    "                  title_x=0.5)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create larger figure size\n",
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "mcm = confusion_matrix(y_test, LR_model.predict(X_test))\n",
    "ax = sns.heatmap(mcm, annot=True, cmap='Blues', fmt='g', annot_kws={'size': 14})\n",
    "\n",
    "ax.set_title('Confusion Matrix\\n', fontsize=20, pad=20)\n",
    "ax.set_xlabel('\\nPredicted Values', fontsize=14)\n",
    "ax.set_ylabel('Actual Values', fontsize=14)\n",
    "\n",
    "# Ticket labels\n",
    "ax.xaxis.set_ticklabels(['1','2', '3', '4'], fontsize=12)\n",
    "ax.yaxis.set_ticklabels(['1','2', '3', '4'], fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Compute confusion matrix\n",
    "mcm = confusion_matrix(y_test, LR_model.predict(X_test))\n",
    "\n",
    "# Convert to percentage (relative to total observations)\n",
    "mcm_percent = mcm / mcm.sum()\n",
    "\n",
    "# Create larger figure\n",
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "# Plot heatmap, fmt='.2%' displays values as percentages\n",
    "ax = sns.heatmap(\n",
    "    mcm_percent, \n",
    "    annot=True, \n",
    "    cmap='Blues', \n",
    "    fmt='.2%',   # Format as percentage with 2 decimals\n",
    "    annot_kws={'size': 14}\n",
    ")\n",
    "\n",
    "ax.set_title('Confusion Matrix (Percentage)\\n', fontsize=20, pad=20)\n",
    "ax.set_xlabel('\\nPredicted Values', fontsize=14)\n",
    "ax.set_ylabel('Actual Values', fontsize=14)\n",
    "\n",
    "ax.xaxis.set_ticklabels(['1','2','3','4'], fontsize=15)\n",
    "ax.yaxis.set_ticklabels(['1','2','3','4'], fontsize=15)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mcm_percent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\"../data/acled_data/processed/fews_with_conflicts_admin2.parquet\")\n",
    "\n",
    "df['region'] = df['ADMIN0'] + '-' + df['ADMIN1'] + '-' + df['ADMIN2']\n",
    "df = df.drop(['ADMIN1', 'ADMIN2'], axis=1)\n",
    "df = df[df['CS_score'].notna() & df['CS_score'] != 0.0]\n",
    "df['CS_score_class'] = np.ceil(df['CS_score'])\n",
    "\n",
    "## New features\n",
    "df['transitions'] = df.groupby('region')['CS_score_class'].transform(pd.Series.cumsum)\n",
    "df['total_conflicts'] = df.groupby('region')['conflicts'].transform(pd.Series.cumsum)\n",
    "df['total_fatalities'] = df.groupby('region')['fatalities'].transform(pd.Series.cumsum)\n",
    "df['previous_CS'] = df.groupby('region')['CS_score_class'].shift(1)\n",
    "\n",
    "use_features = ['region', 'period', 'CS_score_class', 'ADMIN0', \n",
    "                'conflicts', 'transitions', 'total_conflicts', 'total_fatalities', 'previous_CS']\n",
    "\n",
    "df = df[use_features]\n",
    "df = df[df['period'] >= '202106']\n",
    "df = pd.get_dummies(df, columns=['region', 'ADMIN0'])\n",
    "\n",
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "19e238ca122764f831b3091dd8ff35b1b7683cbf7688be73b57ef77bc8403891"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
