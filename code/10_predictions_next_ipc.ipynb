{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix, f1_score, multilabel_confusion_matrix\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "events = pd.read_parquet(\"../data/gdelt/events/6_final/events_dataset.parquet\")\n",
    "gkg = pd.read_parquet(\"../data/gdelt/gkg/6_final/gkg_dataset.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(events.index))\n",
    "print(events.columns)\n",
    "events.tail(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forecasting Configuration\n",
    "# ============================\n",
    "# Set the forecast horizon: predict CS_score N periods ahead\n",
    "# Options: 1 (nowcasting), 2, 3, or 4 (forecasting)\n",
    "# Literature shows GDELT features are more valuable for forecasting (2-4 periods ahead)\n",
    "# because baseline (previous_CS) becomes less powerful with longer horizons\n",
    "\n",
    "FORECAST_HORIZON = 3  # Predict 3 periods ahead (can be changed to 2 or 4)\n",
    "\n",
    "print(f\"=== Forecasting Configuration ===\")\n",
    "print(f\"Forecast Horizon: {FORECAST_HORIZON} period(s) ahead\")\n",
    "if FORECAST_HORIZON == 1:\n",
    "    print(\"Mode: NOWCASTING (predicting next period)\")\n",
    "else:\n",
    "    print(f\"Mode: FORECASTING (predicting {FORECAST_HORIZON} periods ahead)\")\n",
    "print(f\"\\nWhy forecasting helps GDELT features:\")\n",
    "print(f\"  - Baseline 'previous_CS' becomes less predictive ({FORECAST_HORIZON} periods old)\")\n",
    "print(f\"  - GDELT temporal patterns can capture early warning signals\")\n",
    "print(f\"  - More realistic for early warning systems\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(events, gkg, on=[\"ADMIN0\", \"ADMIN1\", \"ADMIN2\", \"period\"], how=\"outer\", suffixes=(\"_events\", \"_gkg\"))\n",
    "print(len(df.index))\n",
    "print(max(df['period']))\n",
    "\n",
    "# Fix ADMIN0=None issue from outer merge\n",
    "# When merging with outer join, if one dataset has ADMIN1/ADMIN2 but missing ADMIN0,\n",
    "# the merged ADMIN0 becomes None. We need to fill it from lookup.\n",
    "print(f\"\\nRows with ADMIN0=None or NaN: {(df['ADMIN0'].isna() | (df['ADMIN0'] == 'None') | (df['ADMIN0'] == None)).sum()}\")\n",
    "\n",
    "# Create lookup from events: (ADMIN1, ADMIN2) -> ADMIN0\n",
    "events_lookup = {}\n",
    "for _, row in events[['ADMIN0', 'ADMIN1', 'ADMIN2']].drop_duplicates().iterrows():\n",
    "    if pd.notna(row['ADMIN0']) and row['ADMIN0'] != 'None' and pd.notna(row['ADMIN1']) and pd.notna(row['ADMIN2']):\n",
    "        key = (row['ADMIN1'], row['ADMIN2'])\n",
    "        if key not in events_lookup:\n",
    "            events_lookup[key] = row['ADMIN0']\n",
    "\n",
    "# Create lookup from gkg: (ADMIN1, ADMIN2) -> ADMIN0  \n",
    "gkg_lookup = {}\n",
    "for _, row in gkg[['ADMIN0', 'ADMIN1', 'ADMIN2']].drop_duplicates().iterrows():\n",
    "    if pd.notna(row['ADMIN0']) and row['ADMIN0'] != 'None' and pd.notna(row['ADMIN1']) and pd.notna(row['ADMIN2']):\n",
    "        key = (row['ADMIN1'], row['ADMIN2'])\n",
    "        if key not in gkg_lookup:\n",
    "            gkg_lookup[key] = row['ADMIN0']\n",
    "\n",
    "# Fill missing ADMIN0 using lookups\n",
    "mask_missing = (df['ADMIN0'].isna()) | (df['ADMIN0'] == 'None') | (df['ADMIN0'] == None)\n",
    "if mask_missing.sum() > 0:\n",
    "    # Try events lookup first\n",
    "    df.loc[mask_missing, 'ADMIN0'] = df.loc[mask_missing].apply(\n",
    "        lambda row: events_lookup.get((row['ADMIN1'], row['ADMIN2']), None), axis=1\n",
    "    )\n",
    "    # Then try gkg lookup for any still missing\n",
    "    mask_still_missing = (df['ADMIN0'].isna()) | (df['ADMIN0'] == 'None') | (df['ADMIN0'] == None)\n",
    "    if mask_still_missing.sum() > 0:\n",
    "        df.loc[mask_still_missing, 'ADMIN0'] = df.loc[mask_still_missing].apply(\n",
    "            lambda row: gkg_lookup.get((row['ADMIN1'], row['ADMIN2']), None), axis=1\n",
    "        )\n",
    "\n",
    "# Convert string 'None' to NaN for consistency\n",
    "df.loc[df['ADMIN0'] == 'None', 'ADMIN0'] = None\n",
    "\n",
    "print(f\"Rows with ADMIN0=None after fixing: {df['ADMIN0'].isna().sum()}\")\n",
    "\n",
    "# Drop rows where ADMIN0 is still None (can't be grouped for y_next creation)\n",
    "if df['ADMIN0'].isna().sum() > 0:\n",
    "    print(f\"   Dropping {df['ADMIN0'].isna().sum()} rows with ADMIN0=None (cannot be grouped)\")\n",
    "    df = df[df['ADMIN0'].notna()].copy()\n",
    "\n",
    "print(f\"Final rows: {len(df)}\")\n",
    "print(f\"Period range: {df['period'].min()} to {df['period'].max()}\")\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_cols = [\"ADMIN0\", \"ADMIN1\", \"ADMIN2\", \"period\"]\n",
    "\n",
    "def check_duplicates(df, name):\n",
    "    dupes = (\n",
    "        df\n",
    "        .groupby(key_cols)\n",
    "        .size()\n",
    "        .reset_index(name=\"n\")\n",
    "        .query(\"n > 1\")\n",
    "    )\n",
    "    print(f\"{name}: {len(dupes)} duplicated keys\")\n",
    "    return dupes\n",
    "\n",
    "dupes_events = check_duplicates(events, \"events\")\n",
    "dupes_gkg = check_duplicates(gkg, \"gkg\")\n",
    "dupes_merged = check_duplicates(df, \"merged\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BETWEEN-ASSESSMENT FORECASTING SETUP\n",
    "# ====================================\n",
    "# CRITICAL: This implements the correct \"between-assessment forecasting\" setup\n",
    "# \n",
    "# Key principle: Observed IPC defines targets; unobserved months are prediction times, not missing labels.\n",
    "#\n",
    "# Target: y_next = next observed IPC assessment (forward-lagged label)\n",
    "# Feature: IPC_last = last observed IPC before this row (forward-filled, constant between assessments)\n",
    "#\n",
    "# Rules:\n",
    "# - Only use IPC_last as IPC feature (no rolling, no multiple lags, no trends)\n",
    "# - CS_score_events is NEVER the target anymore\n",
    "# - Keep ALL rows (including last periods without future assessments)\n",
    "# - Filter to rows with valid y_next only when training models\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"BETWEEN-ASSESSMENT FORECASTING SETUP\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nKey principle: Observed IPC defines targets; unobserved months are prediction times.\\n\")\n",
    "\n",
    "# Create region identifiers\n",
    "df['region'] = df['ADMIN0'] + '-' + df['ADMIN1']\n",
    "df['district'] = df['ADMIN0'] + '-' + df['ADMIN1'] + '-' + df['ADMIN2']\n",
    "\n",
    "# Sort by district (ADMIN2) and period to ensure deterministic operations\n",
    "df = df.sort_values(['ADMIN0', 'ADMIN1', 'ADMIN2', 'period']).reset_index(drop=True)\n",
    "\n",
    "# Get CS_score (IPC) from events or gkg - this is the observed IPC\n",
    "df['CS_score'] = df['CS_score_events'].fillna(df['CS_score_gkg'])\n",
    "df['CS_score'] = pd.to_numeric(df['CS_score'], errors='coerce')\n",
    "\n",
    "# Create IPC_last: last observed IPC before this row (forward-filled)\n",
    "# This is the ONLY IPC feature allowed - it's stale and constant between assessments\n",
    "print(\"1. Creating IPC_last (last observed IPC, forward-filled)...\")\n",
    "df['IPC_last'] = df.groupby(['ADMIN0', 'ADMIN1', 'ADMIN2'], sort=False)['CS_score'].transform(\n",
    "    lambda x: x.ffill()\n",
    ")\n",
    "\n",
    "# Create y_next: next observed IPC assessment (forward-lagged label)\n",
    "# This is the target variable - we predict the next assessment outcome\n",
    "print(\"2. Creating y_next (next observed IPC assessment - THE TARGET)...\")\n",
    "\n",
    "def get_next_assessment(group):\n",
    "    \"\"\"Get the next observed IPC assessment for each row\"\"\"\n",
    "    cs_values = group['CS_score'].values\n",
    "    result = []\n",
    "    \n",
    "    for i in range(len(cs_values)):\n",
    "        # Look forward to find next non-null value\n",
    "        found = False\n",
    "        for j in range(i + 1, len(cs_values)):\n",
    "            if pd.notna(cs_values[j]) and 1 <= cs_values[j] <= 5:\n",
    "                result.append(cs_values[j])\n",
    "                found = True\n",
    "                break\n",
    "        if not found:\n",
    "            result.append(np.nan)\n",
    "    \n",
    "    return pd.Series(result, index=group.index)\n",
    "\n",
    "df['y_next'] = df.groupby(['ADMIN0', 'ADMIN1', 'ADMIN2'], sort=False, group_keys=False).apply(\n",
    "    get_next_assessment\n",
    ")\n",
    "\n",
    "# Keep ALL rows - don't drop periods without future assessments\n",
    "# Rows without y_next (like 202402) can still be used for feature engineering\n",
    "# Filter to rows with valid y_next only when training models\n",
    "rows_with_y_next = df['y_next'].notna().sum()\n",
    "rows_without_y_next = df['y_next'].isna().sum()\n",
    "print(f\"   Rows with y_next (can be used for training): {rows_with_y_next}\")\n",
    "print(f\"   Rows without y_next (last periods, feature engineering only): {rows_without_y_next}\")\n",
    "print(f\"   Total rows kept: {len(df)} (no periods dropped)\")\n",
    "\n",
    "# Clean y_next where it exists: ensure valid range (1-5) and convert to int\n",
    "# Only clean where y_next is not NaN\n",
    "mask_valid_y_next = df['y_next'].notna()\n",
    "df.loc[mask_valid_y_next, 'y_next'] = df.loc[mask_valid_y_next, 'y_next'].round()\n",
    "df.loc[mask_valid_y_next, 'y_next'] = df.loc[mask_valid_y_next, 'y_next'].clip(1, 5)\n",
    "df.loc[mask_valid_y_next, 'y_next'] = df.loc[mask_valid_y_next, 'y_next'].astype(int)\n",
    "\n",
    "# Summary statistics\n",
    "print(f\"\\n=== Summary ===\")\n",
    "print(f\"IPC_last: last observed IPC (forward-filled, constant between assessments)\")\n",
    "print(f\"y_next: next observed IPC assessment (THE TARGET)\")\n",
    "print(f\"\\nIPC_last distribution:\")\n",
    "print(df['IPC_last'].value_counts().sort_index())\n",
    "print(f\"\\ny_next distribution (TARGET):\")\n",
    "print(df['y_next'].value_counts().sort_index())\n",
    "print(f\"\\nRows with valid y_next: {df['y_next'].notna().sum()}\")\n",
    "print(f\"Rows without y_next (last periods): {df['y_next'].isna().sum()}\")\n",
    "print(f\"Current DataFrame shape: {df.shape}\")\n",
    "print(f\"Period range: {df['period'].min()} to {df['period'].max()}\")\n",
    "\n",
    "# Show example for verification\n",
    "print(f\"\\n=== Example (first ADMIN2) ===\")\n",
    "if len(df) > 0:\n",
    "    example_admin2 = df['ADMIN2'].iloc[0]\n",
    "    example = df[df['ADMIN2'] == example_admin2].head(12)[['ADMIN2', 'period', 'CS_score', 'IPC_last', 'y_next']]\n",
    "    print(example.to_string(index=False))\n",
    "    print(f\"\\nNote: IPC_last is constant between assessments, y_next is the next assessment value\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(min(df['period']))\n",
    "print(max(df['period']))\n",
    "df = df.sort_values(['ADMIN0', 'ADMIN1', 'ADMIN2', 'period'])\n",
    "df[['ADMIN0', 'ADMIN1', 'ADMIN2', 'CS_score_events', 'period', 'y_next']].tail(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Availability Analysis - RIGHT AFTER MERGE\n",
    "# ================================================\n",
    "# This must be done BEFORE any feature engineering or filtering\n",
    "# to accurately assess data coverage using original SQLDATE and DATE arrays\n",
    "\n",
    "print(\"=== Data Coverage Analysis (After Merge) ===\\n\")\n",
    "\n",
    "# Total rows after merge\n",
    "total_rows = len(df)\n",
    "print(f\"1. Total rows after merge: {total_rows:,}\")\n",
    "\n",
    "# Function to check if arrays are empty\n",
    "def is_empty_array(x):\n",
    "    \"\"\"Check if an array/list is empty\"\"\"\n",
    "    if x is None:\n",
    "        return True\n",
    "    if isinstance(x, np.ndarray):\n",
    "        return x.size == 0\n",
    "    try:\n",
    "        if pd.isna(x):\n",
    "            return True\n",
    "    except (ValueError, TypeError):\n",
    "        pass\n",
    "    if isinstance(x, (list, tuple)):\n",
    "        return len(x) == 0\n",
    "    return False\n",
    "\n",
    "# Check for events data - SQLDATE being empty means no events\n",
    "if 'SQLDATE' in df.columns:\n",
    "    df['has_events_data'] = (~df['SQLDATE'].apply(is_empty_array)).astype(int)\n",
    "    events_count = df['has_events_data'].sum()\n",
    "    print(f\"2. Rows with events data (non-empty SQLDATE): {events_count:,} ({events_count/total_rows*100:.1f}%)\")\n",
    "else:\n",
    "    df['has_events_data'] = 0\n",
    "    print(\"2. SQLDATE column not found - cannot check events data\")\n",
    "\n",
    "# Check for GKG data - DATE being empty means no GKG data  \n",
    "if 'DATE' in df.columns:\n",
    "    df['has_gkg_data'] = (~df['DATE'].apply(is_empty_array)).astype(int)\n",
    "    gkg_count = df['has_gkg_data'].sum()\n",
    "    print(f\"3. Rows with GKG data (non-empty DATE): {gkg_count:,} ({gkg_count/total_rows*100:.1f}%)\")\n",
    "else:\n",
    "    df['has_gkg_data'] = 0\n",
    "    print(\"3. DATE column not found - cannot check GKG data\")\n",
    "\n",
    "# Overall feature availability\n",
    "df['has_any_features'] = (df['has_events_data'] | df['has_gkg_data']).astype(int)\n",
    "features_count = df['has_any_features'].sum()\n",
    "print(f\"4. Rows with ANY GDELT features: {features_count:,} ({features_count/total_rows*100:.1f}%)\")\n",
    "print(f\"   Rows WITHOUT GDELT features: {total_rows - features_count:,} ({(total_rows - features_count)/total_rows*100:.1f}%)\")\n",
    "\n",
    "# Check CS_score availability\n",
    "valid_cs_count = 0\n",
    "valid_cs_with_features_count = 0\n",
    "\n",
    "if 'CS_score_events' in df.columns and 'CS_score_gkg' in df.columns:\n",
    "    df['CS_score'] = df['CS_score_events'].fillna(df['CS_score_gkg'])\n",
    "    df['CS_score'] = pd.to_numeric(df['CS_score'], errors='coerce')\n",
    "    valid_cs = df[(df['CS_score'] >= 1) & (df['CS_score'] <= 5) & (df['CS_score'].notna())]\n",
    "    valid_cs_count = len(valid_cs)\n",
    "    print(f\"\\n5. Rows with valid CS_score (1-5): {valid_cs_count:,} ({valid_cs_count/total_rows*100:.1f}%)\")\n",
    "    \n",
    "    # Rows with valid CS_score AND features\n",
    "    valid_cs_with_features = valid_cs[valid_cs['has_any_features'] == 1]\n",
    "    valid_cs_with_features_count = len(valid_cs_with_features)\n",
    "    print(f\"6. Rows with valid CS_score AND GDELT features: {valid_cs_with_features_count:,}\")\n",
    "    print(f\"   - Coverage: {valid_cs_with_features_count/valid_cs_count*100:.1f}% of valid CS_score rows\")\n",
    "    print(f\"   - Coverage: {valid_cs_with_features_count/total_rows*100:.1f}% of total rows\")\n",
    "else:\n",
    "    print(\"\\n5. CS_score columns not found\")\n",
    "\n",
    "print(f\"\\n=== Summary ===\")\n",
    "print(f\"Total rows: {total_rows:,}\")\n",
    "if valid_cs_count > 0:\n",
    "    print(f\"Valid CS_score rows: {valid_cs_count:,}\")\n",
    "    print(f\"Valid CS_score + Features: {valid_cs_with_features_count:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering for CS_score Prediction\n",
    "# ============================================\n",
    "\n",
    "def safe_list_agg(lst, func):\n",
    "    \"\"\"Safely aggregate a list, handling None, empty lists, and non-numeric values\"\"\"\n",
    "    # Handle None first\n",
    "    if lst is None:\n",
    "        return np.nan\n",
    "    \n",
    "    # Handle numpy arrays and lists\n",
    "    if isinstance(lst, np.ndarray):\n",
    "        if lst.size == 0:\n",
    "            return np.nan\n",
    "        # Convert to list for processing\n",
    "        lst = lst.tolist()\n",
    "    \n",
    "    # Check for pandas NA/NaN (must check after None and array checks)\n",
    "    try:\n",
    "        if pd.isna(lst):\n",
    "            return np.nan\n",
    "    except (ValueError, TypeError):\n",
    "        # pd.isna() failed, might be array-like, continue processing\n",
    "        pass\n",
    "    \n",
    "    # Handle scalar numeric values\n",
    "    if isinstance(lst, (int, float)):\n",
    "        return float(lst)\n",
    "    \n",
    "    # Handle strings\n",
    "    if isinstance(lst, str):\n",
    "        try:\n",
    "            # Try to evaluate if it's a string representation of a list\n",
    "            if lst.startswith('[') or lst.startswith('('):\n",
    "                lst = eval(lst)\n",
    "            else:\n",
    "                # Try to convert single value\n",
    "                return float(lst)\n",
    "        except:\n",
    "            return np.nan\n",
    "    \n",
    "    # Check if it's a list-like structure\n",
    "    if not isinstance(lst, (list, tuple)):\n",
    "        return np.nan\n",
    "    \n",
    "    # Handle empty lists\n",
    "    if len(lst) == 0:\n",
    "        return np.nan\n",
    "    \n",
    "    # Process list elements\n",
    "    try:\n",
    "        # Convert to numeric, filtering out non-numeric values\n",
    "        numeric_lst = []\n",
    "        for x in lst:\n",
    "            # Check for NaN/None values\n",
    "            try:\n",
    "                if pd.isna(x) or x is None:\n",
    "                    continue\n",
    "            except (ValueError, TypeError):\n",
    "                # pd.isna() might fail for some types, try to convert anyway\n",
    "                pass\n",
    "            \n",
    "            try:\n",
    "                val = float(x)\n",
    "                if not np.isinf(val) and not np.isnan(val):\n",
    "                    numeric_lst.append(val)\n",
    "            except (ValueError, TypeError):\n",
    "                continue\n",
    "        \n",
    "        if len(numeric_lst) == 0:\n",
    "            return np.nan\n",
    "        \n",
    "        result = func(numeric_lst)\n",
    "        return float(result) if not np.isnan(result) and not np.isinf(result) else np.nan\n",
    "    except Exception as e:\n",
    "        return np.nan\n",
    "\n",
    "def safe_list_count(x):\n",
    "    \"\"\"Safely count elements in a list/array, handling various data types\"\"\"\n",
    "    if x is None:\n",
    "        return 0\n",
    "    if isinstance(x, np.ndarray):\n",
    "        return x.size if x.size > 0 else 0\n",
    "    if isinstance(x, (list, tuple)):\n",
    "        return len(x)\n",
    "    # For scalar values, check if it's not NaN\n",
    "    try:\n",
    "        if pd.isna(x):\n",
    "            return 0\n",
    "        return 1\n",
    "    except (ValueError, TypeError):\n",
    "        # If pd.isna fails, assume it's a valid value\n",
    "        return 1\n",
    "\n",
    "def aggregate_list_features(df, list_cols, prefix=\"\"):\n",
    "    \"\"\"Aggregate list columns into multiple statistical features\"\"\"\n",
    "    # Collect all new columns in a dictionary to avoid fragmentation\n",
    "    new_cols = {}\n",
    "    cols_to_drop = []\n",
    "    \n",
    "    for col in list_cols:\n",
    "        if col not in df.columns:\n",
    "            continue\n",
    "        \n",
    "        base_name = col.replace('_list', '').replace('_events', '').replace('_gkg', '')\n",
    "        if prefix:\n",
    "            base_name = f\"{prefix}_{base_name}\"\n",
    "        \n",
    "        # Compute all aggregations for this column\n",
    "        new_cols[f\"{base_name}_mean\"] = df[col].apply(lambda x: safe_list_agg(x, np.mean))\n",
    "        new_cols[f\"{base_name}_max\"] = df[col].apply(lambda x: safe_list_agg(x, np.max))\n",
    "        new_cols[f\"{base_name}_sum\"] = df[col].apply(lambda x: safe_list_agg(x, np.sum))\n",
    "        new_cols[f\"{base_name}_count\"] = df[col].apply(safe_list_count)\n",
    "        new_cols[f\"{base_name}_std\"] = df[col].apply(lambda x: safe_list_agg(x, np.std))\n",
    "        new_cols[f\"{base_name}_min\"] = df[col].apply(lambda x: safe_list_agg(x, np.min))\n",
    "        \n",
    "        # Track columns to drop\n",
    "        cols_to_drop.append(col)\n",
    "    \n",
    "    # Add all new columns at once using pd.concat to avoid fragmentation\n",
    "    if new_cols:\n",
    "        new_df = pd.DataFrame(new_cols, index=df.index)\n",
    "        df = pd.concat([df, new_df], axis=1)\n",
    "    \n",
    "    # Drop original list columns\n",
    "    if cols_to_drop:\n",
    "        df = df.drop(columns=cols_to_drop)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Step 1: Determine target variable\n",
    "# ===================================\n",
    "# IMPORTANT: CS_score from FEWSNET is only available roughly every 4 months.\n",
    "# Intermediary months can be used for feature engineering (lags, moving averages)\n",
    "# but will be filtered out before model training (only CS_score between 1-5 are valid).\n",
    "\n",
    "# Option 1: Use events CS_score as primary, fill with gkg if missing\n",
    "df['CS_score'] = df['CS_score_events'].fillna(df['CS_score_gkg'])\n",
    "\n",
    "# Option 2: Average if both exist (uncomment if preferred)\n",
    "# df['CS_score'] = df[['CS_score_events', 'CS_score_gkg']].mean(axis=1)\n",
    "\n",
    "# Option 3: Use maximum (uncomment if preferred)\n",
    "# df['CS_score'] = df[['CS_score_events', 'CS_score_gkg']].max(axis=1)\n",
    "\n",
    "print(f\"CS_score distribution (before filtering):\")\n",
    "print(df['CS_score'].value_counts().sort_index())\n",
    "print(f\"\\nMissing CS_score: {df['CS_score'].isna().sum()}\")\n",
    "print(f\"\\nNote: Intermediary months (with missing CS_score) will be used for\")\n",
    "print(f\"feature engineering but filtered out before model training.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(min(df['period']))\n",
    "print(max(df['period']))\n",
    "df = df.sort_values(['ADMIN0', 'ADMIN1', 'ADMIN2', 'period'])\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in df.columns:\n",
    "    print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMPREHENSIVE FEATURE ENGINEERING\n",
    "# ===================================\n",
    "# This cell performs ALL feature engineering for both IPC and GDELT features\n",
    "# At this point: lists are NOT aggregated yet, y_next and IPC_last exist\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"COMPREHENSIVE FEATURE ENGINEERING\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nStarting with {len(df)} rows\")\n",
    "print(f\"Columns before feature engineering: {len(df.columns)}\\n\")\n",
    "\n",
    "# ============================================================================\n",
    "# 1. AGGREGATE GDELT LIST FEATURES\n",
    "# ============================================================================\n",
    "print(\"1. Aggregating GDELT list features...\")\n",
    "\n",
    "# Helper functions (already defined in previous cell, but ensure they exist)\n",
    "def safe_list_agg(lst, func):\n",
    "    \"\"\"Safely aggregate a list, handling None, empty lists, and non-numeric values\"\"\"\n",
    "    if lst is None:\n",
    "        return np.nan\n",
    "    if isinstance(lst, np.ndarray):\n",
    "        if lst.size == 0:\n",
    "            return np.nan\n",
    "        lst = lst.tolist()\n",
    "    try:\n",
    "        if pd.isna(lst):\n",
    "            return np.nan\n",
    "    except (ValueError, TypeError):\n",
    "        pass\n",
    "    if isinstance(lst, (int, float)):\n",
    "        return float(lst)\n",
    "    if isinstance(lst, str):\n",
    "        try:\n",
    "            if lst.startswith('[') or lst.startswith('('):\n",
    "                lst = eval(lst)\n",
    "            else:\n",
    "                return float(lst)\n",
    "        except:\n",
    "            return np.nan\n",
    "    if not isinstance(lst, (list, tuple)):\n",
    "        return np.nan\n",
    "    if len(lst) == 0:\n",
    "        return np.nan\n",
    "    try:\n",
    "        numeric_lst = []\n",
    "        for x in lst:\n",
    "            try:\n",
    "                if pd.isna(x) or x is None:\n",
    "                    continue\n",
    "            except (ValueError, TypeError):\n",
    "                pass\n",
    "            try:\n",
    "                val = float(x)\n",
    "                if not np.isinf(val) and not np.isnan(val):\n",
    "                    numeric_lst.append(val)\n",
    "            except (ValueError, TypeError):\n",
    "                continue\n",
    "        if len(numeric_lst) == 0:\n",
    "            return np.nan\n",
    "        result = func(numeric_lst)\n",
    "        return float(result) if not np.isnan(result) and not np.isinf(result) else np.nan\n",
    "    except Exception as e:\n",
    "        return np.nan\n",
    "\n",
    "def safe_list_count(x):\n",
    "    \"\"\"Safely count elements in a list/array\"\"\"\n",
    "    if x is None:\n",
    "        return 0\n",
    "    if isinstance(x, np.ndarray):\n",
    "        return x.size if x.size > 0 else 0\n",
    "    if isinstance(x, (list, tuple)):\n",
    "        return len(x)\n",
    "    try:\n",
    "        if pd.isna(x):\n",
    "            return 0\n",
    "        return 1\n",
    "    except (ValueError, TypeError):\n",
    "        return 1\n",
    "\n",
    "def aggregate_list_features(df, list_cols, prefix=\"\"):\n",
    "    \"\"\"Aggregate list columns into meaningful statistical features for NLP-derived data\n",
    "    \n",
    "    For noisy NLP features, we keep only:\n",
    "    - mean: central tendency\n",
    "    - count: volume indicator\n",
    "    - sum: total (especially useful for frequencies/counts)\n",
    "    - max: peak intensity (only for crisis indicators)\n",
    "    \n",
    "    We skip: min (usually 0), std (variance not meaningful for noisy signals)\n",
    "    \"\"\"\n",
    "    new_cols = {}\n",
    "    cols_to_drop = []\n",
    "    \n",
    "    for col in list_cols:\n",
    "        if col not in df.columns:\n",
    "            continue\n",
    "        \n",
    "        base_name = col.replace('_list', '').replace('_events', '').replace('_gkg', '')\n",
    "        if prefix:\n",
    "            base_name = f\"{prefix}_{base_name}\"\n",
    "        \n",
    "        # For text columns (NER, clean_text), only create count\n",
    "        if 'NER' in col or 'clean_text' in col:\n",
    "            new_cols[f\"{base_name}_count\"] = df[col].apply(safe_list_count)\n",
    "        else:\n",
    "            # For numeric columns, create meaningful aggregations only\n",
    "            # Mean: central tendency (most important for noisy signals)\n",
    "            new_cols[f\"{base_name}_mean\"] = df[col].apply(lambda x: safe_list_agg(x, np.mean))\n",
    "            \n",
    "            # Count: volume indicator (how many articles/events)\n",
    "            new_cols[f\"{base_name}_count\"] = df[col].apply(safe_list_count)\n",
    "            \n",
    "            # Sum: total (especially useful for frequencies and counts)\n",
    "            new_cols[f\"{base_name}_sum\"] = df[col].apply(lambda x: safe_list_agg(x, np.sum))\n",
    "            \n",
    "            # Max: peak intensity (only for crisis indicators where peaks matter)\n",
    "            # Check if this is a crisis-related feature\n",
    "            is_crisis_feature = any(x in col.lower() for x in ['fatalities', 'displaced', 'injured', \n",
    "                                                                 'violence', 'torture', 'crisis'])\n",
    "            if is_crisis_feature:\n",
    "                new_cols[f\"{base_name}_max\"] = df[col].apply(lambda x: safe_list_agg(x, np.max))\n",
    "            \n",
    "            # Skip: min (usually 0), std (variance not meaningful for noisy NLP signals)\n",
    "        \n",
    "        cols_to_drop.append(col)\n",
    "    \n",
    "    if new_cols:\n",
    "        new_df = pd.DataFrame(new_cols, index=df.index)\n",
    "        df = pd.concat([df, new_df], axis=1)\n",
    "    \n",
    "    if cols_to_drop:\n",
    "        df = df.drop(columns=cols_to_drop)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Find and aggregate list columns\n",
    "list_cols_events = [c for c in df.columns if c.endswith('_list_events') or (c.endswith('_events') and '_list' in c)]\n",
    "list_cols_gkg = [c for c in df.columns if c.endswith('_list_gkg') or (c.endswith('_gkg') and '_list' in c)]\n",
    "\n",
    "print(f\"   Found {len(list_cols_events)} events list columns\")\n",
    "print(f\"   Found {len(list_cols_gkg)} gkg list columns\")\n",
    "\n",
    "if list_cols_events:\n",
    "    print(\"   Aggregating events features...\")\n",
    "    df = aggregate_list_features(df, list_cols_events, prefix=\"evt\")\n",
    "\n",
    "if list_cols_gkg:\n",
    "    print(\"   Aggregating gkg features...\")\n",
    "    df = aggregate_list_features(df, list_cols_gkg, prefix=\"gkg\")\n",
    "\n",
    "# ============================================================================\n",
    "# 2. CLEAN AND PREPARE IPC FEATURES\n",
    "# ============================================================================\n",
    "print(\"\\n2. Cleaning IPC features...\")\n",
    "\n",
    "# Clean IPC_last: round to nearest integer and ensure valid range\n",
    "df['IPC_last'] = df['IPC_last'].round()\n",
    "# Keep IPC_last as float for now (can filter invalid values later if needed)\n",
    "# Don't drop rows here - keep all periods\n",
    "\n",
    "print(f\"   IPC_last range: {df['IPC_last'].min():.1f} to {df['IPC_last'].max():.1f}\")\n",
    "print(f\"   Valid IPC_last (1-5): {(df['IPC_last'].between(1, 5, inclusive='both')).sum()} rows\")\n",
    "\n",
    "# ============================================================================\n",
    "# 3. CREATE TEMPORAL FEATURES FOR GDELT\n",
    "# ============================================================================\n",
    "print(\"\\n3. Creating temporal features for GDELT...\")\n",
    "\n",
    "# Focus on most reliable features for temporal patterns\n",
    "# Prioritize: sentiment, crisis indicators, food security\n",
    "key_gdelt_features = [\n",
    "    # Sentiment (most reliable NLP signals)\n",
    "    'evt_compound_score_mean', 'evt_sentiment.compound_mean', 'gkg_compound_mean',\n",
    "    \n",
    "    # Crisis indicators (high signal-to-noise)\n",
    "    'evt_fatalities_freq_mean', 'evt_displaced_freq_mean',\n",
    "    'gkg_fatalities_freq_mean', 'gkg_displaced_freq_mean',\n",
    "    \n",
    "    # Food security (directly relevant to IPC)\n",
    "    'evt_food_insecurity_freq_mean', 'gkg_food_insecurity_freq_mean',\n",
    "    \n",
    "    # Economic/agricultural (relevant but less reliable)\n",
    "    'evt_economic_shocks_freq_mean', 'evt_agriculture_freq_mean',\n",
    "]\n",
    "\n",
    "# Filter to features that actually exist\n",
    "key_gdelt_features = [f for f in key_gdelt_features if f in df.columns]\n",
    "\n",
    "print(f\"   Creating temporal features for {len(key_gdelt_features)} key GDELT features\")\n",
    "\n",
    "# Ensure sorted for lag operations\n",
    "df = df.sort_values(['ADMIN0', 'ADMIN1', 'ADMIN2', 'period']).reset_index(drop=True)\n",
    "\n",
    "# Create rolling windows (smoothing noise - most useful for NLP features)\n",
    "# Use 3 and 6 periods to capture short and medium-term trends\n",
    "for window in [3, 6]:\n",
    "    for feat in key_gdelt_features:\n",
    "        df[f\"{feat}_rolling_{window}\"] = df.groupby(['ADMIN0', 'ADMIN1', 'ADMIN2'], sort=False)[feat].transform(\n",
    "            lambda x: x.rolling(window=window, min_periods=1).mean()\n",
    "        )\n",
    "\n",
    "# Create lags (1 and 2 periods only - reduce noise)\n",
    "# Lags can capture trends but too many lags add noise\n",
    "for lag in [1, 2]:\n",
    "    for feat in key_gdelt_features:\n",
    "        df[f\"{feat}_lag{lag}\"] = df.groupby(['ADMIN0', 'ADMIN1', 'ADMIN2'], sort=False)[feat].shift(lag)\n",
    "\n",
    "# Create escalation indicators (binary - less noisy than continuous change)\n",
    "# Use rolling mean comparison instead of raw change to reduce noise\n",
    "for feat in key_gdelt_features:\n",
    "    rolling_3 = df.groupby(['ADMIN0', 'ADMIN1', 'ADMIN2'], sort=False)[feat].transform(\n",
    "        lambda x: x.rolling(window=3, min_periods=1).mean()\n",
    "    )\n",
    "    rolling_6 = df.groupby(['ADMIN0', 'ADMIN1', 'ADMIN2'], sort=False)[feat].transform(\n",
    "        lambda x: x.rolling(window=6, min_periods=2).mean()\n",
    "    )\n",
    "    # Escalation: current 3-period average > 6-period average\n",
    "    df[f\"{feat}_escalation\"] = (rolling_3 > rolling_6).astype(int)\n",
    "\n",
    "# Skip: change, pct_change, anomaly (too noisy for NLP-derived features)\n",
    "\n",
    "print(f\"   Created temporal features: rolling windows (3, 6 periods), lags (1, 2 periods), escalation indicators\")\n",
    "\n",
    "# ============================================================================\n",
    "# 4. CREATE FEATURE INDICATORS AND INTERACTIONS\n",
    "# ============================================================================\n",
    "print(\"\\n4. Creating feature indicators and interactions...\")\n",
    "\n",
    "# GDELT feature availability indicator\n",
    "gdelt_cols = [c for c in df.columns if c.startswith('evt_') or c.startswith('gkg_')]\n",
    "if gdelt_cols:\n",
    "    has_gdelt = df[gdelt_cols].notna().any(axis=1)\n",
    "    df['has_gdelt'] = has_gdelt.astype(int)\n",
    "else:\n",
    "    df['has_gdelt'] = 0\n",
    "\n",
    "# Combine sentiment features from events and gkg\n",
    "if 'evt_compound_score_mean' in df.columns and 'gkg_compound_mean' in df.columns:\n",
    "    df['sentiment_combined'] = (df['evt_compound_score_mean'].fillna(0) + df['gkg_compound_mean'].fillna(0)) / 2\n",
    "    df['sentiment_agreement'] = ((df['evt_compound_score_mean'] * df['gkg_compound_mean']) > 0).astype(int)\n",
    "\n",
    "# Combine crisis indicators\n",
    "crisis_features = [c for c in df.columns if any(x in c for x in ['fatalities', 'displaced', 'injured']) \n",
    "                   and c.endswith('_mean') and (c.startswith('evt_') or c.startswith('gkg_'))]\n",
    "if crisis_features:\n",
    "    df['crisis_intensity'] = df[crisis_features].fillna(0).sum(axis=1)\n",
    "\n",
    "# Food security combined\n",
    "food_features = [c for c in df.columns if 'food_insecurity' in c and c.endswith('_mean') \n",
    "                 and (c.startswith('evt_') or c.startswith('gkg_'))]\n",
    "if food_features:\n",
    "    df['food_security_combined'] = df[food_features].fillna(0).mean(axis=1)\n",
    "\n",
    "print(f\"   Created interaction and combined features\")\n",
    "\n",
    "# ============================================================================\n",
    "# 5. CREATE GEOGRAPHIC DUMMY VARIABLES\n",
    "# ============================================================================\n",
    "print(\"\\n5. Creating geographic dummy variables...\")\n",
    "\n",
    "# Only create dummies for base geographic identifiers (ADMIN0, ADMIN1, ADMIN2)\n",
    "# Note: 'region' and 'district' are redundant (derived from ADMIN0+ADMIN1 and ADMIN0+ADMIN1+ADMIN2)\n",
    "geographic_cols = []\n",
    "for col in ['ADMIN0', 'ADMIN1', 'ADMIN2']:\n",
    "    if col in df.columns:\n",
    "        # Create dummy variables (one-hot encoding)\n",
    "        dummies = pd.get_dummies(df[col], prefix=col, drop_first=False)\n",
    "        df = pd.concat([df, dummies], axis=1)\n",
    "        geographic_cols.extend(dummies.columns.tolist())\n",
    "        print(f\"   Created {len(dummies.columns)} dummy variables for {col}\")\n",
    "\n",
    "geographic_features = geographic_cols\n",
    "print(f\"   Total geographic features: {len(geographic_features)}\")\n",
    "print(f\"   Note: 'region' and 'district' are kept as categorical identifiers but not converted to dummies (redundant with ADMIN0+ADMIN1+ADMIN2)\")\n",
    "\n",
    "# ============================================================================\n",
    "# 6. SUMMARY\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FEATURE ENGINEERING COMPLETE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nFinal dataset: {len(df)} rows\")\n",
    "print(f\"Total columns: {len(df.columns)}\")\n",
    "\n",
    "# Feature counts\n",
    "ipc_features = ['IPC_last']\n",
    "\n",
    "# GDELT base features: evt_/gkg_ prefixed features + raw GKG numeric columns\n",
    "gdelt_base = [c for c in df.columns if (c.startswith('evt_') or c.startswith('gkg_')) \n",
    "              and not any(x in c for x in ['_lag', '_rolling', '_escalation'])]\n",
    "\n",
    "# Add raw GKG numeric columns (these are GDELT features but don't have gkg_ prefix)\n",
    "gkg_raw = [c for c in ['n_displaced', 'n_killed', 'n_injured', 'n_missing', 'usd_aid',\n",
    "                       'n_food_related', 'n_water_related', 'n_price_related', \n",
    "                       'n_conflict_related', 'n_disease_related', 'n_weather_related',\n",
    "                       'n_market_related', 'n_policy_related', 'tone', 'tone_abs', 'is_negative'] \n",
    "           if c in df.columns]\n",
    "gdelt_base.extend(gkg_raw)\n",
    "\n",
    "# GDELT temporal features: evt_/gkg_ prefixed features with temporal patterns\n",
    "gdelt_temporal = [c for c in df.columns if (c.startswith('evt_') or c.startswith('gkg_')) \n",
    "                   and any(x in c for x in ['_lag', '_rolling', '_escalation'])]\n",
    "\n",
    "# Geographic features will be created as dummies later, initialize as empty for now\n",
    "geographic_features = []\n",
    "\n",
    "# Geographic features: only ADMIN0, ADMIN1, ADMIN2 are converted to dummies\n",
    "# 'region' and 'district' are kept as categorical identifiers for grouping but not converted to dummies (redundant)\n",
    "# Exclude original categorical columns from feature lists (but keep them in dataframe for grouping purposes)\n",
    "geographic_categorical_cols = ['ADMIN0', 'ADMIN1', 'ADMIN2', 'region', 'district']\n",
    "\n",
    "# Exclude all non-feature columns (geographic categoricals, metadata, scores, indicators, target, list columns)\n",
    "excluded_cols = (ipc_features + gdelt_base + gdelt_temporal + geographic_features +\n",
    "                 ['y_next', 'has_gdelt', 'has_events_data', 'has_gkg_data', 'has_any_features',\n",
    "                  'CS_score', 'CS_score_events', 'CS_score_gkg',\n",
    "                  'period', 'SQLDATE', 'EventCode', 'SOURCEURL', 'NumMentions', 'NumSources', \n",
    "                  'NumArticles', 'valid_SOURCEURL', 'DATE', 'V2Themes', 'DocumentIdentifier', \n",
    "                  'Amounts', 'valid_DocumentIdentifier'] +\n",
    "                 geographic_categorical_cols +\n",
    "                 # Also exclude any remaining list columns that weren't aggregated\n",
    "                 [c for c in df.columns if '_list' in c])\n",
    "other_features = [c for c in df.columns if c not in excluded_cols]\n",
    "\n",
    "print(f\"\\nFeature breakdown:\")\n",
    "print(f\"  - IPC features: {len(ipc_features)}\")\n",
    "print(f\"  - GDELT base features: {len(gdelt_base)} (mean, count, sum, max for crisis)\")\n",
    "print(f\"  - GDELT temporal features: {len(gdelt_temporal)} (rolling windows, lags, escalation)\")\n",
    "print(f\"  - Geographic features: {len(geographic_features)} (dummy variables)\")\n",
    "print(f\"  - Other features: {len(other_features)}\")\n",
    "print(f\"  - Total features: {len(ipc_features) + len(gdelt_base) + len(gdelt_temporal) + len(geographic_features) + len(other_features)}\")\n",
    "print(f\"\\nNote: Simplified feature set for noisy NLP-derived signals:\")\n",
    "print(f\"  - Base: mean, count, sum (all features) + max (crisis features only)\")\n",
    "print(f\"  - Temporal: rolling windows (3, 6 periods), lags (1, 2 periods), escalation indicators\")\n",
    "print(f\"  - Removed: min, std, change, pct_change, anomaly (too noisy for NLP features)\")\n",
    "\n",
    "print(f\"\\nRows with y_next: {df['y_next'].notna().sum()} ({df['y_next'].notna().mean()*100:.1f}%)\")\n",
    "print(f\"Rows with IPC_last: {df['IPC_last'].notna().sum()} ({df['IPC_last'].notna().mean()*100:.1f}%)\")\n",
    "print(f\"Rows with GDELT features: {df['has_gdelt'].sum()} ({df['has_gdelt'].mean()*100:.1f}%)\")\n",
    "\n",
    "# ============================================================================\n",
    "# 6. DROP PROCESSED/METADATA COLUMNS\n",
    "# ============================================================================\n",
    "print(\"\\n6. Dropping processed metadata columns...\")\n",
    "\n",
    "cols_to_drop = ['SQLDATE', 'EventCode', 'SOURCEURL', 'NumMentions', \n",
    "                'NumSources', 'NumArticles', 'valid_SOURCEURL', 'CS_score_gkg', \n",
    "                'DATE', 'V2Themes', 'DocumentIdentifier', 'Amounts', 'valid_DocumentIdentifier',\n",
    "                'CS_score_events']\n",
    "# Only drop columns that exist\n",
    "cols_to_drop = [c for c in cols_to_drop if c in df.columns]\n",
    "if cols_to_drop:\n",
    "    df.drop(cols_to_drop, axis=1, inplace=True)\n",
    "    print(f\"   Dropped {len(cols_to_drop)} metadata columns\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"READY FOR MODELING\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nFeature sets available:\")\n",
    "print(f\"  - IPC only: {len(ipc_features)} IPC features + {len(geographic_features)} geographic features\")\n",
    "print(f\"  - IPC + GDELT base: {len(ipc_features)} IPC + {len(gdelt_base)} GDELT base + {len(geographic_features)} geographic\")\n",
    "print(f\"  - IPC + GDELT + temporal: {len(ipc_features)} IPC + {len(gdelt_base)} GDELT base + {len(gdelt_temporal)} GDELT temporal + {len(geographic_features)} geographic\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in df.columns:\n",
    "    print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(min(df['period']))\n",
    "print(max(df['period']))\n",
    "df = df.sort_values(['ADMIN0', 'ADMIN1', 'ADMIN2', 'period'])\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure train and test, and loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_training_periods = 24  # Number of periods to use for training\n",
    "n_eval_periods = 1       # Number of rolling evaluations to run (going backwards from latest period)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# ROLLING WINDOW EVALUATION CONFIGURATION\n",
    "# ============================================================================\n",
    "# Configure how many periods to use for training and how many rolling evaluations to run\n",
    "# Get all unique periods and sort them\n",
    "all_periods = sorted(df['period'].unique())\n",
    "latest_period = all_periods[-1]\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"ROLLING WINDOW EVALUATION SETUP\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nTotal periods available: {len(all_periods)}\")\n",
    "print(f\"Period range: {all_periods[0]} to {all_periods[-1]}\")\n",
    "print(f\"\\nConfiguration:\")\n",
    "print(f\"  - Training periods per evaluation: {n_training_periods}\")\n",
    "print(f\"  - Number of evaluations: {n_eval_periods}\")\n",
    "print(f\"\\nEvaluation schedule (going backwards from latest period):\")\n",
    "\n",
    "# Create evaluation splits\n",
    "evaluation_splits = []\n",
    "\n",
    "for i in range(n_eval_periods):\n",
    "    # Test period: latest period minus i\n",
    "    test_period_idx = len(all_periods) - 1 - i\n",
    "    test_period = all_periods[test_period_idx]\n",
    "    \n",
    "    # Training periods: n_training_periods periods before test period\n",
    "    train_start_idx = test_period_idx - n_training_periods\n",
    "    train_end_idx = test_period_idx - 1\n",
    "    \n",
    "    if train_start_idx < 0:\n",
    "        print(f\"\\n  ⚠️  Evaluation {i+1}: Cannot create split - not enough periods\")\n",
    "        print(f\"     Need {n_training_periods} training periods before {test_period}\")\n",
    "        continue\n",
    "    \n",
    "    train_periods = all_periods[train_start_idx:test_period_idx]\n",
    "    \n",
    "    evaluation_splits.append({\n",
    "        'eval_id': i + 1,\n",
    "        'test_period': test_period,\n",
    "        'train_periods': train_periods,\n",
    "        'train_start': train_periods[0],\n",
    "        'train_end': train_periods[-1],\n",
    "        'n_train_periods': len(train_periods)\n",
    "    })\n",
    "    \n",
    "    print(f\"\\n  Evaluation {i+1}:\")\n",
    "    print(f\"    Test period: {test_period}\")\n",
    "    print(f\"    Training periods: {train_periods[0]} to {train_periods[-1]} ({len(train_periods)} periods)\")\n",
    "\n",
    "print(f\"\\n\" + \"=\"*70)\n",
    "print(f\"Created {len(evaluation_splits)} evaluation splits\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Store splits for use in modeling\n",
    "# Each split can be accessed as: evaluation_splits[i]['test_period'], evaluation_splits[i]['train_periods']\n",
    "\n",
    "# ============================================================================\n",
    "# TRAIN AND EVALUATE LOGISTIC REGRESSION MODEL\n",
    "# ============================================================================\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TRAINING AND EVALUATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ============================================================================\n",
    "# DEFINE FEATURE SETS\n",
    "# ============================================================================\n",
    "# Get feature lists (check if they exist from feature engineering cell)\n",
    "if 'ipc_features' in globals():\n",
    "    ipc_feat = ipc_features\n",
    "else:\n",
    "    ipc_feat = ['IPC_last']\n",
    "\n",
    "if 'gdelt_base' in globals():\n",
    "    gdelt_base_feat = gdelt_base\n",
    "else:\n",
    "    gdelt_base_feat = [c for c in df.columns if (c.startswith('evt_') or c.startswith('gkg_')) \n",
    "                       and not any(x in c for x in ['_lag', '_rolling', '_escalation'])]\n",
    "    # Add raw GKG numeric columns\n",
    "    gkg_raw = [c for c in ['n_displaced', 'n_killed', 'n_injured', 'n_missing', 'usd_aid',\n",
    "                           'n_food_related', 'n_water_related', 'n_price_related', \n",
    "                           'n_conflict_related', 'n_disease_related', 'n_weather_related',\n",
    "                           'n_market_related', 'n_policy_related', 'tone', 'tone_abs', 'is_negative'] \n",
    "               if c in df.columns]\n",
    "    gdelt_base_feat.extend(gkg_raw)\n",
    "\n",
    "if 'gdelt_temporal' in globals():\n",
    "    gdelt_temporal_feat = gdelt_temporal\n",
    "else:\n",
    "    gdelt_temporal_feat = [c for c in df.columns if (c.startswith('evt_') or c.startswith('gkg_')) \n",
    "                            and any(x in c for x in ['_lag', '_rolling', '_escalation'])]\n",
    "\n",
    "# Find geographic dummy variables\n",
    "geographic_feat = [c for c in df.columns if c.startswith('ADMIN0_') or c.startswith('ADMIN1_') or c.startswith('ADMIN2_')]\n",
    "\n",
    "# Remove features that don't exist\n",
    "ipc_feat = [c for c in ipc_feat if c in df.columns]\n",
    "gdelt_base_feat = [c for c in gdelt_base_feat if c in df.columns]\n",
    "gdelt_temporal_feat = [c for c in gdelt_temporal_feat if c in df.columns]\n",
    "geographic_feat = [c for c in geographic_feat if c in df.columns]\n",
    "\n",
    "# Define feature combinations\n",
    "feature_sets = {\n",
    "    'IPC only': ipc_feat + geographic_feat,\n",
    "    'IPC + GDELT base': ipc_feat + gdelt_base_feat + geographic_feat,\n",
    "    'IPC + GDELT + temporal': ipc_feat + gdelt_base_feat + gdelt_temporal_feat + geographic_feat\n",
    "}\n",
    "\n",
    "print(f\"\\nFeature sets available:\")\n",
    "for name, features in feature_sets.items():\n",
    "    n_ipc = len([c for c in features if c in ipc_feat])\n",
    "    n_gdelt_base = len([c for c in features if c in gdelt_base_feat])\n",
    "    n_gdelt_temp = len([c for c in features if c in gdelt_temporal_feat])\n",
    "    n_geo = len([c for c in features if c in geographic_feat])\n",
    "    print(f\"  - {name}: {n_ipc} IPC + {n_gdelt_base} GDELT base + {n_gdelt_temp} GDELT temporal + {n_geo} geographic = {len(features)} total\")\n",
    "\n",
    "# Store results for each evaluation and feature set\n",
    "all_results = {}\n",
    "\n",
    "# Evaluate each feature set\n",
    "for feature_set_name, feature_cols in feature_sets.items():\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"FEATURE SET: {feature_set_name}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Using {len(feature_cols)} features\")\n",
    "    \n",
    "    feature_results = []\n",
    "    \n",
    "    for split in evaluation_splits:\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"Evaluation {split['eval_id']}: Test period {split['test_period']}\")\n",
    "        print(f\"{'='*70}\")\n",
    "    \n",
    "        # Create train/test splits\n",
    "        train_df = df[\n",
    "            (df['period'].isin(split['train_periods'])) & \n",
    "            (df['y_next'].notna())  # Only use rows with valid targets for training\n",
    "        ].copy()\n",
    "        \n",
    "        test_df = df[df['period'] == split['test_period']].copy()\n",
    "        \n",
    "        # For test data: use y_next if available, otherwise use CS_score (for latest period)\n",
    "        # Create target column: prefer y_next, fallback to CS_score (rounded to integer)\n",
    "        test_df['target'] = test_df['y_next'].fillna(test_df['CS_score'].round().clip(1, 5))\n",
    "        \n",
    "        # Filter test to only rows with valid target (y_next or CS_score in valid range 1-5)\n",
    "        test_df_eval = test_df[\n",
    "            (test_df['target'].notna()) & \n",
    "            (test_df['target'] >= 1) & \n",
    "            (test_df['target'] <= 5)\n",
    "        ].copy()\n",
    "        \n",
    "        print(f\"  Training samples: {len(train_df)}\")\n",
    "        print(f\"  Test samples (total): {len(test_df)}\")\n",
    "        print(f\"  Test samples (with y_next): {(test_df['y_next'].notna()).sum()}\")\n",
    "        print(f\"  Test samples (with CS_score): {(test_df['CS_score'].notna() & (test_df['CS_score'] >= 1) & (test_df['CS_score'] <= 5)).sum()}\")\n",
    "        print(f\"  Test samples (with valid target): {len(test_df_eval)}\")\n",
    "        \n",
    "        if len(train_df) == 0:\n",
    "            print(f\"  ⚠️  Skipping - no training data\")\n",
    "            continue\n",
    "        \n",
    "        if len(test_df_eval) == 0:\n",
    "            print(f\"  ⚠️  Skipping - no test data with valid target (y_next or CS_score)\")\n",
    "            continue\n",
    "        \n",
    "        # Prepare features and target\n",
    "        X_train = train_df[feature_cols].fillna(0)\n",
    "        y_train = train_df['y_next'].astype(int)\n",
    "        \n",
    "        X_test = test_df_eval[feature_cols].fillna(0)\n",
    "        y_test = test_df_eval['target'].astype(int)\n",
    "        \n",
    "        # Check for missing values\n",
    "        if X_train.isna().any().any() or X_test.isna().any().any():\n",
    "            print(f\"  ⚠️  Warning: Missing values in features, filling with 0\")\n",
    "            X_train = X_train.fillna(0)\n",
    "            X_test = X_test.fillna(0)\n",
    "        \n",
    "        # Scale features\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "        \n",
    "        # Train logistic regression\n",
    "        print(f\"  Training logistic regression...\")\n",
    "        lr_model = LogisticRegression(\n",
    "            max_iter=1000,\n",
    "            random_state=42,\n",
    "            solver='lbfgs'\n",
    "        )\n",
    "        lr_model.fit(X_train_scaled, y_train)\n",
    "        \n",
    "        # Predictions\n",
    "        y_pred = lr_model.predict(X_test_scaled)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "        recall = recall_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "        f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "        # Store results\n",
    "        feature_results.append({\n",
    "            'eval_id': split['eval_id'],\n",
    "            'test_period': split['test_period'],\n",
    "            'n_train': len(train_df),\n",
    "            'n_test': len(test_df_eval),\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1': f1,\n",
    "            'confusion_matrix': cm\n",
    "        })\n",
    "        \n",
    "        print(f\"\\n  Results:\")\n",
    "        print(f\"    Accuracy:  {accuracy:.4f}\")\n",
    "        print(f\"    Precision: {precision:.4f}\")\n",
    "        print(f\"    Recall:    {recall:.4f}\")\n",
    "        print(f\"    F1 Score:  {f1:.4f}\")\n",
    "        print(f\"\\n  Confusion Matrix:\")\n",
    "        print(f\"    {cm}\")\n",
    "    \n",
    "    # Store results for this feature set\n",
    "    all_results[feature_set_name] = feature_results\n",
    "\n",
    "# ============================================================================\n",
    "# AVERAGE RESULTS ACROSS ALL EVALUATIONS AND FEATURE SETS\n",
    "# ============================================================================\n",
    "if len(all_results) > 0:\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"SUMMARY: AVERAGE RESULTS BY FEATURE SET\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    for feature_set_name, results in all_results.items():\n",
    "        if len(results) == 0:\n",
    "            continue\n",
    "            \n",
    "        print(f\"\\n{feature_set_name}:\")\n",
    "        print(\"-\" * 70)\n",
    "        \n",
    "        avg_accuracy = np.mean([r['accuracy'] for r in results])\n",
    "        avg_precision = np.mean([r['precision'] for r in results])\n",
    "        avg_recall = np.mean([r['recall'] for r in results])\n",
    "        avg_f1 = np.mean([r['f1'] for r in results])\n",
    "        \n",
    "        # Average confusion matrix\n",
    "        avg_cm = np.mean([r['confusion_matrix'] for r in results], axis=0).astype(int)\n",
    "        \n",
    "        print(f\"  Average Metrics (across {len(results)} evaluations):\")\n",
    "        print(f\"    Accuracy:  {avg_accuracy:.4f}\")\n",
    "        print(f\"    Precision: {avg_precision:.4f}\")\n",
    "        print(f\"    Recall:    {avg_recall:.4f}\")\n",
    "        print(f\"    F1 Score:  {avg_f1:.4f}\")\n",
    "        \n",
    "        print(f\"\\n  Average Confusion Matrix:\")\n",
    "        print(f\"    {avg_cm}\")\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Total feature sets evaluated: {len([k for k, v in all_results.items() if len(v) > 0])}\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Create summary dataframe\n",
    "    summary_data = []\n",
    "    for feature_set_name, results in all_results.items():\n",
    "        if len(results) == 0:\n",
    "            continue\n",
    "        \n",
    "        avg_accuracy = np.mean([r['accuracy'] for r in results])\n",
    "        avg_precision = np.mean([r['precision'] for r in results])\n",
    "        avg_recall = np.mean([r['recall'] for r in results])\n",
    "        avg_f1 = np.mean([r['f1'] for r in results])\n",
    "        \n",
    "        # Count features\n",
    "        n_features = len(feature_sets[feature_set_name])\n",
    "        n_ipc = len([c for c in feature_sets[feature_set_name] if c in ipc_feat])\n",
    "        n_gdelt_base = len([c for c in feature_sets[feature_set_name] if c in gdelt_base_feat])\n",
    "        n_gdelt_temp = len([c for c in feature_sets[feature_set_name] if c in gdelt_temporal_feat])\n",
    "        n_geo = len([c for c in feature_sets[feature_set_name] if c in geographic_feat])\n",
    "        \n",
    "        summary_data.append({\n",
    "            'Feature Set': feature_set_name,\n",
    "            'N Features': n_features,\n",
    "            'IPC': n_ipc,\n",
    "            'GDELT Base': n_gdelt_base,\n",
    "            'GDELT Temporal': n_gdelt_temp,\n",
    "            'Geographic': n_geo,\n",
    "            'Accuracy': avg_accuracy,\n",
    "            'Precision': avg_precision,\n",
    "            'Recall': avg_recall,\n",
    "            'F1 Score': avg_f1,\n",
    "            'N Evaluations': len(results)\n",
    "        })\n",
    "    \n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    summary_df = summary_df.sort_values('Accuracy', ascending=False)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"SUMMARY TABLE\")\n",
    "    print(\"=\"*70)\n",
    "    print(summary_df.to_string(index=False))\n",
    "    \n",
    "else:\n",
    "    print(\"\\n⚠️  No evaluations completed - check data availability\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why Literature Shows Better Results: Key Differences\n",
    "\n",
    "## Potential Reasons for Better Performance in Literature:\n",
    "\n",
    "1. **Different Prediction Tasks**:\n",
    "   - **Transitions/Changes**: Predicting when CS_score will worsen (crisis onset) rather than exact level\n",
    "   - **Early Warning**: Predicting 2-4 periods ahead (forecasting) rather than next period (nowcasting)\n",
    "   - **Binary Classification**: Predicting crisis (CS≥3) vs non-crisis (CS<3) rather than 5-class classification\n",
    "\n",
    "2. **Feature Engineering**:\n",
    "   - **Temporal Patterns**: Using lagged and rolling features (as we just added) rather than only current-period values\n",
    "   - **Anomaly Detection**: Focusing on deviations from historical patterns\n",
    "   - **Interaction Features**: Combining GDELT with other data sources (weather, prices, etc.)\n",
    "\n",
    "3. **Data Coverage**:\n",
    "   - **Spatial Aggregation**: Some studies aggregate to ADMIN0 or ADMIN1 level where coverage is better\n",
    "   - **Temporal Aggregation**: Using quarterly rather than monthly data\n",
    "   - **Filtering**: Focusing on regions/periods with good GDELT coverage\n",
    "\n",
    "4. **Evaluation Metrics**:\n",
    "   - **Recall for Rare Events**: Focusing on detecting crises (high recall for CS≥3) rather than overall accuracy\n",
    "   - **Early Detection**: Measuring how early they detect transitions, not just accuracy\n",
    "\n",
    "5. **Baseline Comparison**:\n",
    "   - **Weaker Baselines**: Some studies don't include `previous_CS` in baseline, making improvement easier to show\n",
    "   - **Different Baselines**: Comparing against simpler models or using different evaluation windows\n",
    "\n",
    "## Our Current Approach:\n",
    "- ✅ Now includes temporal GDELT features (lags, rolling windows, anomalies, escalation)\n",
    "- ✅ Uses same framework as previous work (allowing fair comparison)\n",
    "- ⚠️ Still predicting exact CS_score level (highly autocorrelated)\n",
    "- ⚠️ Using monthly data with CS_score available every 4 months\n",
    "\n",
    "## Potential Improvements:\n",
    "1. **Predict Transitions**: Predict if CS_score will worsen (CS_t+1 > CS_t) rather than exact value\n",
    "2. **Forecasting Horizon**: Predict 2-4 periods ahead instead of next period\n",
    "3. **Crisis Detection**: Binary classification (crisis vs non-crisis) with focus on recall\n",
    "4. **Feature Selection**: Use only temporal GDELT features, filter out low-importance current-period features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering Summary\n",
    "\n",
    "### Features Created:\n",
    "\n",
    "1. **List Aggregations** (from both events and gkg):\n",
    "   - Mean, Max, Min, Sum, Count, Std for all list features\n",
    "   - Separated by `evt_` and `gkg_` prefixes\n",
    "\n",
    "2. **Combined Features**:\n",
    "   - CS_score combinations (combined, diff, has_both)\n",
    "   - Casualty rates and totals\n",
    "   - Theme aggregations (conflict intensity, crisis severity)\n",
    "   - Sentiment combinations\n",
    "   - Coverage intensity metrics\n",
    "\n",
    "3. **Temporal Features**:\n",
    "   - Lag features (CS_score_lag1, lag2, lag3)\n",
    "   - Moving averages (ma2, ma3)\n",
    "   - Change and percentage change features\n",
    "   - Cyclical period encoding (sin/cos)\n",
    "\n",
    "4. **Geographic Features**:\n",
    "   - Aggregated CS_score by ADMIN0 and ADMIN1 levels\n",
    "   - Standard deviations by geographic level\n",
    "   - Count of regions with same score\n",
    "\n",
    "5. **Interaction Features**:\n",
    "   - Ratios (casualty_rate, aid_per_casualty)\n",
    "   - Normalized features (tone_abs_normalized)\n",
    "   - Coverage metrics (mentions_per_source, articles_per_source)\n",
    "\n",
    "### Next Steps for ML:\n",
    "\n",
    "1. **Feature Selection**: Consider using:\n",
    "   - Correlation-based selection\n",
    "   - Mutual information\n",
    "   - Recursive feature elimination\n",
    "   - L1 regularization (Lasso)\n",
    "\n",
    "2. **Categorical Encoding**: If you have categorical features:\n",
    "   - One-hot encoding for low cardinality\n",
    "   - Target encoding for high cardinality\n",
    "   - Embedding for very high cardinality\n",
    "\n",
    "3. **Scaling**: Consider:\n",
    "   - StandardScaler or MinMaxScaler for numeric features\n",
    "   - Especially important for distance-based algorithms\n",
    "\n",
    "4. **Model Suggestions**:\n",
    "   - Random Forest (handles non-linear relationships well)\n",
    "   - Gradient Boosting (XGBoost, LightGBM, CatBoost)\n",
    "   - Neural Networks (if you have enough data)\n",
    "   - Consider class weights if classes are imbalanced\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
