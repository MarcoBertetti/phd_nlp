{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix, f1_score, multilabel_confusion_matrix\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "df = pd.read_parquet(\"../data/gdelt/final_gdelt_dataset/data.parquet\")\n",
    "print(len(df.index))\n",
    "print(df.columns)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# --- Helpers ---\n",
    "\n",
    "def array_to_stats(x):\n",
    "    \"\"\"Convert list/array/string into (mean, std, max).\"\"\"\n",
    "    if x is None or (isinstance(x, float) and np.isnan(x)):\n",
    "        return (np.nan, np.nan, np.nan)\n",
    "    if isinstance(x, (list, np.ndarray)):\n",
    "        vals = [float(v) for v in x if str(v).lower() not in [\"nan\", \"none\"]]\n",
    "    else:\n",
    "        s = str(x).replace(\"[\", \"\").replace(\"]\", \"\").replace(\"'\", \"\").strip()\n",
    "        if s == \"\" or s.lower() == \"nan\":\n",
    "            return (np.nan, np.nan, np.nan)\n",
    "        vals = []\n",
    "        for v in s.split():\n",
    "            try:\n",
    "                vals.append(float(v))\n",
    "            except:\n",
    "                continue\n",
    "    if not vals:\n",
    "        return (np.nan, np.nan, np.nan)\n",
    "    return (np.mean(vals), np.std(vals), np.max(vals))\n",
    "\n",
    "\n",
    "def clean_numeric_array(x):\n",
    "    \"\"\"Convert array/string of numbers into mean (for sentiment, tone, etc.).\"\"\"\n",
    "    if x is None or (isinstance(x, float) and np.isnan(x)):\n",
    "        return np.nan\n",
    "    if isinstance(x, (list, np.ndarray)):\n",
    "        vals = [float(v) for v in x if str(v).lower() not in [\"nan\", \"none\"]]\n",
    "        return np.mean(vals) if vals else np.nan\n",
    "    # if string representation like \"[1.23 2.34]\"\n",
    "    s = str(x).replace(\"[\", \"\").replace(\"]\", \"\").replace(\"'\", \"\").split()\n",
    "    try:\n",
    "        vals = [float(v) for v in s]\n",
    "        return np.mean(vals) if vals else np.nan\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "\n",
    "def clean_listlike(x):\n",
    "    \"\"\"Convert pred_impact_type/pred_urgency into list of strings.\"\"\"\n",
    "    if x is None or (isinstance(x, float) and np.isnan(x)):\n",
    "        return []\n",
    "    if isinstance(x, (list, np.ndarray)):\n",
    "        return [str(v).strip() for v in x if v not in [None, \"nan\", \"NaN\"]]\n",
    "    # fallback: parse string\n",
    "    s = str(x).replace(\"[\", \"\").replace(\"]\", \"\").replace(\"'\", \"\").replace('\"', \"\").strip()\n",
    "    if s == \"\" or s.lower() == \"nan\":\n",
    "        return []\n",
    "    return [v.strip() for v in s.split() if v.strip()]\n",
    "\n",
    "\n",
    "def clean_sentiment(x):\n",
    "    \"\"\"Reduce sentiment arrays/lists/strings to mean value.\"\"\"\n",
    "    if x is None or (isinstance(x, float) and np.isnan(x)):\n",
    "        return np.nan\n",
    "    if isinstance(x, (list, np.ndarray)):\n",
    "        try:\n",
    "            return float(np.mean(x))\n",
    "        except:\n",
    "            return np.nan\n",
    "    s = str(x).replace(\"[\", \"\").replace(\"]\", \"\").replace(\"'\", \"\").strip()\n",
    "    if s == \"\" or s.lower() == \"nan\":\n",
    "        return np.nan\n",
    "    try:\n",
    "        values = [float(v) for v in s.split()]\n",
    "        return float(np.mean(values)) if values else np.nan\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "\n",
    "# --- Main preprocessing ---\n",
    "def preprocess(df):\n",
    "    # 1. Fix sentiment arrays -> numeric mean\n",
    "    sentiment_cols = [\n",
    "        \"compound_score_events\", \"neg_score_events\", \"neu_score_events\", \"pos_score_events\",\n",
    "        \"compound_score_gkg\", \"neg_score_gkg\", \"neu_score_gkg\", \"pos_score_gkg\"\n",
    "    ]\n",
    "    for col in sentiment_cols:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].apply(clean_sentiment)\n",
    "    \n",
    "    # 2. Handle categorical predictions -> one-hot\n",
    "    for col in [\"pred_impact_type_events\", \"pred_impact_type_gkg\",\n",
    "                \"pred_urgency_events\", \"pred_urgency_gkg\"]:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].apply(clean_listlike)\n",
    "            dummies = df[col].explode().str.get_dummies().groupby(level=0).sum()\n",
    "            dummies = dummies.add_prefix(f\"{col}_\")\n",
    "            df = pd.concat([df.drop(columns=[col]), dummies], axis=1)\n",
    "\n",
    "    # 3. Tone arrays -> mean/std/max\n",
    "    for col in [\"tone\", \"tone_abs\"]:\n",
    "        if col in df.columns:\n",
    "            stats = df[col].apply(array_to_stats)\n",
    "            df[f\"{col}_mean\"] = stats.apply(lambda x: x[0])\n",
    "            df[f\"{col}_std\"]  = stats.apply(lambda x: x[1])\n",
    "            df[f\"{col}_max\"]  = stats.apply(lambda x: x[2])\n",
    "            df = df.drop(columns=[col])\n",
    "\n",
    "    # 4. Normalize counts by NumArticles (create _per_article cols)\n",
    "    if \"NumArticles\" in df.columns:\n",
    "        count_cols = [c for c in df.columns if c.endswith(\"_count_events\") or c.endswith(\"_count_gkg\")]\n",
    "        for col in count_cols:\n",
    "            df[f\"{col}_per_article\"] = df[col] / (df[\"NumArticles\"] + 1e-6)\n",
    "\n",
    "    # 5. *_per_article arrays -> mean/std/max\n",
    "    per_article_cols = [c for c in df.columns if c.endswith(\"_per_article\")]\n",
    "    for col in per_article_cols:\n",
    "        stats = df[col].apply(array_to_stats)\n",
    "        df[f\"{col}_mean\"] = stats.apply(lambda x: x[0])\n",
    "        df[f\"{col}_std\"]  = stats.apply(lambda x: x[1])\n",
    "        df[f\"{col}_max\"]  = stats.apply(lambda x: x[2])\n",
    "        df = df.drop(columns=[col])\n",
    "\n",
    "    # 6. Period -> year/month\n",
    "    if \"period\" in df.columns:\n",
    "        df[\"period\"] = pd.to_numeric(df[\"period\"], errors=\"coerce\")\n",
    "        df[\"year\"] = (df[\"period\"] // 100).astype(\"Int64\")\n",
    "        df[\"month\"] = (df[\"period\"] % 100).astype(\"Int64\")\n",
    "\n",
    "    # 7. Drop unused text/ID fields\n",
    "    drop_cols = [\n",
    "        \"SQLDATE\", \"EventCode\", \"SOURCEURL\", \"NumMentions\", \"NumSources\", \"NumArticles\",\n",
    "        \"V2Themes\", \"DocumentIdentifier\", \"clean_text\"\n",
    "    ]\n",
    "    df = df.drop(columns=[c for c in drop_cols if c in df.columns])\n",
    "\n",
    "    return df\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = preprocess(df)\n",
    "df.fillna(0.0, inplace=True)\n",
    "num_cols = df.select_dtypes(include=[np.number]).columns\n",
    "df[num_cols] = df[num_cols].round(3)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output verification: Check shape after merging\n",
    "print(f\"df shape after merging: {df.shape}\")\n",
    "df['CS_score'] = df['CS_score'].astype(float)\n",
    "\n",
    "# Filter CS_score\n",
    "df = df[df['CS_score'].isin([1.0, 2.0, 3.0, 4.0, 5.0])]\n",
    "\n",
    "# Output verification: Check shape after filtering CS_score\n",
    "print(f\"df shape after filtering CS_score: {df.shape}\")\n",
    "\n",
    "# Round CS_score\n",
    "df['CS_score'] = np.ceil(df['CS_score'])\n",
    "\n",
    "# Split data into train and test\n",
    "train = df[df['period'] != max(df['period'])]\n",
    "test = df[df['period'] == max(df['period'])]\n",
    "\n",
    "# Output verification: Check shapes of train and test sets\n",
    "print(f\"train shape: {train.shape}\")\n",
    "print(f\"test shape: {test.shape}\")\n",
    "\n",
    "# Initialize summary DataFrame\n",
    "base_summary = pd.DataFrame(columns=['Model', 'Test Accuracy', 'Test Precision', 'Test Recall', 'F1'])\n",
    "base_models = ['PPS', 'SPLY', 'Max-2PP']\n",
    "accuracies = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "f1s = []\n",
    "\n",
    "# First naive model: PPS\n",
    "train = train[train['period'].isin(sorted(train['period'].unique())[-1:])]\n",
    "print(f\"train shape for PPS: {train.shape}\")\n",
    "\n",
    "train = train[['ADMIN0', 'ADMIN1', 'ADMIN2', 'CS_score', 'period']]\n",
    "train.rename(columns={'CS_score': 'predicted'}, inplace=True)\n",
    "test = pd.merge(test, train, on=['ADMIN0', 'ADMIN1', 'ADMIN2'], how='left')\n",
    "test['predicted'] = test['predicted'].fillna(0)\n",
    "print(f\"test shape after PPS merge: {test.shape}\")\n",
    "\n",
    "accuracies.append(accuracy_score(test['CS_score'], test['predicted']))\n",
    "precisions.append(precision_score(test['CS_score'], test['predicted'], average='weighted'))\n",
    "recalls.append(recall_score(test['CS_score'], test['predicted'], average='weighted'))\n",
    "f1s.append(f1_score(test['CS_score'], test['predicted'], average='weighted'))\n",
    "\n",
    "# Second naive model: SPLY\n",
    "train = df[df['period'] != max(df['period'])]\n",
    "test = df[df['period'] == max(df['period'])]\n",
    "\n",
    "train = train[train['period'].isin(sorted(train['period'].unique())[-3:-2])]\n",
    "print(f\"train shape for SPLY: {train.shape}\")\n",
    "\n",
    "train = train[['ADMIN0', 'ADMIN1', 'ADMIN2', 'CS_score', 'period']]\n",
    "train.rename(columns={'CS_score': 'predicted'}, inplace=True)\n",
    "test = pd.merge(test, train, on=['ADMIN0', 'ADMIN1', 'ADMIN2'], how='left')\n",
    "test['predicted'] = test['predicted'].fillna(0)\n",
    "print(f\"test shape after SPLY merge: {test.shape}\")\n",
    "\n",
    "accuracies.append(accuracy_score(test['CS_score'], test['predicted']))\n",
    "precisions.append(precision_score(test['CS_score'], test['predicted'], average='weighted'))\n",
    "recalls.append(recall_score(test['CS_score'], test['predicted'], average='weighted'))\n",
    "f1s.append(f1_score(test['CS_score'], test['predicted'], average='weighted'))\n",
    "\n",
    "# Third naive model: Max-2PP\n",
    "train = df[df['period'] != max(df['period'])]\n",
    "test = df[df['period'] == max(df['period'])]\n",
    "\n",
    "train = train[train['period'].isin(sorted(train['period'].unique())[-2:])]\n",
    "print(f\"train shape for Max-2PP: {train.shape}\")\n",
    "\n",
    "train = train[['ADMIN0', 'ADMIN1', 'ADMIN2', 'CS_score', 'period']]\n",
    "train = pd.DataFrame(train.groupby(['ADMIN0', 'ADMIN1', 'ADMIN2'])['CS_score'].max()).reset_index()\n",
    "train.rename(columns={'CS_score': 'predicted'}, inplace=True)\n",
    "test = pd.merge(test, train, on=['ADMIN0', 'ADMIN1', 'ADMIN2'], how='left')\n",
    "test['predicted'] = test['predicted'].fillna(0)\n",
    "print(f\"test shape after Max-2PP merge: {test.shape}\")\n",
    "\n",
    "accuracies.append(accuracy_score(test['CS_score'], test['predicted']))\n",
    "precisions.append(precision_score(test['CS_score'], test['predicted'], average='weighted'))\n",
    "recalls.append(recall_score(test['CS_score'], test['predicted'], average='weighted'))\n",
    "f1s.append(f1_score(test['CS_score'], test['predicted'], average='weighted'))\n",
    "\n",
    "# Compile results\n",
    "base_summary['Model'] = base_models\n",
    "base_summary['Test Accuracy'] = accuracies\n",
    "base_summary['Test Precision'] = precisions\n",
    "base_summary['Test Recall'] = recalls\n",
    "base_summary['F1'] = f1s\n",
    "\n",
    "# Display summary\n",
    "print(base_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actual ML model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Sort by region + period\n",
    "df = df.sort_values([\"ADMIN0\", \"ADMIN1\", \"ADMIN2\", \"period\"])\n",
    "\n",
    "# Create lags safely (these are past values only)\n",
    "df[\"CS_score_lag1\"] = df.groupby([\"ADMIN0\",\"ADMIN1\",\"ADMIN2\"])[\"CS_score\"].shift(1)\n",
    "df[\"CS_score_lag2\"] = df.groupby([\"ADMIN0\",\"ADMIN1\",\"ADMIN2\"])[\"CS_score\"].shift(2)\n",
    "\n",
    "# Drop rows where lag values are NaN (first periods)\n",
    "df = df.dropna(subset=[\"CS_score_lag1\", \"CS_score_lag2\"])\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "print(len(df.index))\n",
    "\n",
    "# Then split train/test by period\n",
    "max_period = df[\"period\"].max()\n",
    "train = df[df[\"period\"] != max_period]\n",
    "train.reset_index(drop=True, inplace=True)\n",
    "print(len(train.index))\n",
    "\n",
    "test = df[df[\"period\"] == max_period]\n",
    "test.reset_index(drop=True, inplace=True)\n",
    "print(len(test.index))\n",
    "\n",
    "# Prepare features\n",
    "target = \"CS_score\"\n",
    "drop_cols = [\"ADMIN0\", \"ADMIN1\", \"ADMIN2\", \"period\", target]\n",
    "X_train = train.drop(columns=[c for c in drop_cols if c in train.columns])\n",
    "y_train = train[target]\n",
    "\n",
    "X_test = test.drop(columns=[c for c in drop_cols if c in test.columns])\n",
    "y_test = test[target]\n",
    "\n",
    "# --- Random Forest ---\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=300,\n",
    "    max_depth=20,\n",
    "    random_state=42,\n",
    "    class_weight=\"balanced\",\n",
    "    n_jobs=-1\n",
    ")\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "\n",
    "rf_acc = accuracy_score(y_test, y_pred_rf)\n",
    "rf_prec = precision_score(y_test, y_pred_rf, average=\"weighted\")\n",
    "rf_rec = recall_score(y_test, y_pred_rf, average=\"weighted\")\n",
    "rf_f1 = f1_score(y_test, y_pred_rf, average=\"weighted\")\n",
    "\n",
    "base_summary.loc[len(base_summary)] = [\"RandomForest\", rf_acc, rf_prec, rf_rec, rf_f1]\n",
    "\n",
    "# --- CatBoost ---\n",
    "classes = np.unique(y_train)\n",
    "weights = compute_class_weight(class_weight=\"balanced\", classes=classes, y=y_train)\n",
    "class_weights = dict(zip(classes, weights))\n",
    "\n",
    "cat = CatBoostClassifier(\n",
    "    iterations=500,\n",
    "    depth=8,\n",
    "    learning_rate=0.05,\n",
    "    random_seed=42,\n",
    "    verbose=False,\n",
    "    class_weights=class_weights\n",
    ")\n",
    "\n",
    "cat.fit(X_train, y_train)\n",
    "\n",
    "y_pred_cat = cat.predict(X_test)\n",
    "\n",
    "cat_acc = accuracy_score(y_test, y_pred_cat)\n",
    "cat_prec = precision_score(y_test, y_pred_cat, average=\"weighted\")\n",
    "cat_rec = recall_score(y_test, y_pred_cat, average=\"weighted\")\n",
    "cat_f1 = f1_score(y_test, y_pred_cat, average=\"weighted\")\n",
    "\n",
    "base_summary.loc[len(base_summary)] = [\"CatBoost\", cat_acc, cat_prec, cat_rec, cat_f1]\n",
    "\n",
    "print(base_summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "\n",
    "# Extract feature importances\n",
    "importances = rf.feature_importances_\n",
    "feature_names = X_train.columns\n",
    "\n",
    "# Put into DataFrame\n",
    "feat_imp = pd.DataFrame({\n",
    "    \"feature\": feature_names,\n",
    "    \"importance\": importances\n",
    "}).sort_values(by=\"importance\", ascending=False)\n",
    "\n",
    "# Select top N features\n",
    "top_n = 20  # show more features if you want\n",
    "feat_top = feat_imp.head(top_n)\n",
    "\n",
    "fig = px.bar(\n",
    "    feat_top,\n",
    "    x=\"importance\",\n",
    "    y=\"feature\",\n",
    "    orientation=\"h\",\n",
    "    title=f\"Random Forest - Top {top_n} Feature Importances\",\n",
    "    labels={\"importance\": \"Importance\", \"feature\": \"Feature\"},\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    yaxis=dict(autorange=\"reversed\"),\n",
    "    plot_bgcolor=\"white\",\n",
    "    xaxis=dict(showgrid=True, gridcolor=\"lightgray\"),\n",
    "    height=40 * top_n  # 40px per bar (adjust to taste)\n",
    ")\n",
    "\n",
    "fig.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
